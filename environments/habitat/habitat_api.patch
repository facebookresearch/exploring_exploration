# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

diff --git a/demos/configs/exploration_mp3d.yaml b/demos/configs/exploration_mp3d.yaml
new file mode 100644
index 0000000..e1a5120
--- /dev/null
+++ b/demos/configs/exploration_mp3d.yaml
@@ -0,0 +1,47 @@
+ENVIRONMENT:
+  MAX_EPISODE_STEPS: 501
+SIMULATOR:
+  TYPE: "Sim-v1"
+  AGENT_0:
+    SENSORS: ['RGB_SENSOR', 'DEPTH_SENSOR', 'FINE_OCC_SENSOR', 'COARSE_OCC_SENSOR']
+  ACTION_SPACE_CONFIG: "v2"
+  HABITAT_SIM_V0:
+    GPU_DEVICE_ID: 0
+  RGB_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  DEPTH_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  OCCUPANCY_MAPS:
+    MAP_SCALE: 0.05
+    MAP_SIZE: 800
+    MAX_DEPTH: 3
+    SMALL_MAP_RANGE: 40
+    LARGE_MAP_RANGE: 200
+    HEIGHT_LOWER: 0.5
+    HEIGHT_UPPER: 2.0
+    USE_GT_OCC_MAP: True
+  FINE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  COARSE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+TASK:
+  TYPE: Pose-v0
+  SENSORS: ['ORACLE_ACTION_SENSOR']
+  ORACLE_ACTION_SENSOR:
+    ORACLE_TYPE: 'random'
+  MEASUREMENTS: ['TOP_DOWN_MAP_POSE']
+  TOP_DOWN_MAP_POSE:
+    TYPE: TopDownMapPose
+    FOG_OF_WAR:
+        DRAW: True
+        FOV: 90
+        VISIBILITY_DIST: 4.0
+DATASET:
+  TYPE: PoseEstimation-v1
+  SPLIT: test
+  DATA_PATH: <HABITAT_ROOT>/habitat-api/data/datasets/pose_estimation/mp3d/v1/{split}/{split}.json.gz
+  SHUFFLE_DATASET: False
diff --git a/demos/configs/pointnav_mp3d.yaml b/demos/configs/pointnav_mp3d.yaml
new file mode 100644
index 0000000..5b3d659
--- /dev/null
+++ b/demos/configs/pointnav_mp3d.yaml
@@ -0,0 +1,57 @@
+ENVIRONMENT:
+  MAX_EPISODE_STEPS: 700
+  T_EXP: 500
+  T_NAV: 200
+SIMULATOR:
+  TYPE: "Sim-v1"
+  AGENT_0:
+    SENSORS: ['RGB_SENSOR', 'DEPTH_SENSOR', 'FINE_OCC_SENSOR', 'COARSE_OCC_SENSOR', 'HIGHRES_COARSE_OCC_SENSOR']
+  ACTION_SPACE_CONFIG: "v2"
+  HABITAT_SIM_V0:
+    GPU_DEVICE_ID: 0
+  RGB_SENSOR:
+    WIDTH: 84
+    HEIGHT: 84
+  DEPTH_SENSOR:
+    WIDTH: 84
+    HEIGHT: 84
+  OCCUPANCY_MAPS:
+    MAP_SCALE: 0.05
+    MAP_SIZE: 800
+    MAX_DEPTH: 3
+    SMALL_MAP_RANGE: 40
+    LARGE_MAP_RANGE: 200
+    HEIGHT_LOWER: 0.5
+    HEIGHT_UPPER: 2.0
+  FINE_OCC_SENSOR:
+    WIDTH: 84
+    HEIGHT: 84
+  COARSE_OCC_SENSOR:
+    WIDTH: 84
+    HEIGHT: 84
+  HIGHRES_COARSE_OCC_SENSOR:
+    WIDTH: 800
+    HEIGHT: 800
+TASK:
+  TYPE: ExpNav-v0
+  SUCCESS_DISTANCE: 0.2
+  SENSORS: ['ORACLE_ACTION_SENSOR', 'GRID_GOAL_SENSOR', 'SP_ACTION_SENSOR']
+  ORACLE_ACTION_SENSOR:
+    ORACLE_TYPE: 'random'
+  GRID_GOAL_SENSOR:
+    TYPE: GridGoalSensorExploreNavigation
+    T_EXP: 500
+    T_NAV: 200
+  SP_ACTION_SENSOR:
+    TYPE: SPActionSensorExploreNavigation
+    T_EXP: 500
+    T_NAV: 200
+  MEASUREMENTS: ['TOP_DOWN_MAP_EXP_NAV']
+  TOP_DOWN_MAP_EXP_NAV:
+    T_EXP: 500
+    T_NAV: 200
+DATASET:
+  TYPE: ExpNav-v1
+  SPLIT: test
+  DATA_PATH: <HABITAT_ROOT>/habitat-api/data/datasets/exp_nav/mp3d/v1/{split}/{split}.json.gz
+  SHUFFLE_DATASET: False
diff --git a/demos/configs/pose_estimation_mp3d.yaml b/demos/configs/pose_estimation_mp3d.yaml
new file mode 100644
index 0000000..400a6a6
--- /dev/null
+++ b/demos/configs/pose_estimation_mp3d.yaml
@@ -0,0 +1,51 @@
+ENVIRONMENT:
+  MAX_EPISODE_STEPS: 501
+SIMULATOR:
+  TYPE: "Sim-v1"
+  AGENT_0:
+    SENSORS: ['RGB_SENSOR', 'DEPTH_SENSOR', 'FINE_OCC_SENSOR', 'COARSE_OCC_SENSOR']
+  ACTION_SPACE_CONFIG: "v2"
+  HABITAT_SIM_V0:
+    GPU_DEVICE_ID: 0
+  RGB_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  DEPTH_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  OCCUPANCY_MAPS:
+    MAP_SCALE: 0.05
+    MAP_SIZE: 800
+    MAX_DEPTH: 3
+    SMALL_MAP_RANGE: 40
+    LARGE_MAP_RANGE: 200
+    HEIGHT_LOWER: 0.5
+    HEIGHT_UPPER: 2.0
+    USE_GT_OCC_MAP: True
+  FINE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  COARSE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+TASK:
+  TYPE: Pose-v0
+  SENSORS: ['POSE_RGB_SENSOR', 'POSE_REGRESS_SENSOR', 'ORACLE_ACTION_SENSOR']
+  POSE_RGB_SENSOR:
+    NREF: 100
+  POSE_REGRESS_SENSOR:
+    NREF: 100
+  ORACLE_ACTION_SENSOR:
+    ORACLE_TYPE: 'random'
+  MEASUREMENTS: ['TOP_DOWN_MAP_POSE']
+  TOP_DOWN_MAP_POSE:
+    TYPE: TopDownMapPose
+    FOG_OF_WAR:
+        DRAW: True
+        FOV: 90
+        VISIBILITY_DIST: 4.0
+DATASET:
+  TYPE: PoseEstimation-v1
+  SPLIT: test
+  DATA_PATH: <HABITAT_ROOT>/habitat-api/data/datasets/pose_estimation/mp3d/v1/{split}/{split}.json.gz
+  SHUFFLE_DATASET: False
diff --git a/demos/configs/reconstruction_mp3d.yaml b/demos/configs/reconstruction_mp3d.yaml
new file mode 100644
index 0000000..b06f69c
--- /dev/null
+++ b/demos/configs/reconstruction_mp3d.yaml
@@ -0,0 +1,49 @@
+ENVIRONMENT:
+  MAX_EPISODE_STEPS: 501
+SIMULATOR:
+  TYPE: "Sim-v1"
+  AGENT_0:
+    SENSORS: ['RGB_SENSOR', 'DEPTH_SENSOR', 'FINE_OCC_SENSOR', 'COARSE_OCC_SENSOR']
+  ACTION_SPACE_CONFIG: "v2"
+  HABITAT_SIM_V0:
+    GPU_DEVICE_ID: 0
+  RGB_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  DEPTH_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  OCCUPANCY_MAPS:
+    MAP_SCALE: 0.05
+    MAP_SIZE: 800
+    MAX_DEPTH: 3
+    SMALL_MAP_RANGE: 40
+    LARGE_MAP_RANGE: 200
+    HEIGHT_LOWER: 0.5
+    HEIGHT_UPPER: 2.0
+    USE_GT_OCC_MAP: True
+  FINE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  COARSE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+TASK:
+  TYPE: Pose-v0
+  SENSORS: ['POSE_RGB_SENSOR', 'POSE_REGRESS_SENSOR', 'ORACLE_ACTION_SENSOR']
+  POSE_RGB_SENSOR:
+    NREF: 100
+  POSE_REGRESS_SENSOR:
+    NREF: 100
+  MEASUREMENTS: ['TOP_DOWN_MAP_POSE']
+  TOP_DOWN_MAP_POSE:
+    TYPE: TopDownMapPose
+    FOG_OF_WAR:
+        DRAW: True
+        FOV: 90
+        VISIBILITY_DIST: 4.0
+DATASET:
+  TYPE: PoseEstimation-v1
+  SPLIT: test
+  DATA_PATH: <HABITAT_ROOT>/habitat-api/data/datasets/uniform_pose_estimation/mp3d/v1/{split}/{split}.json.gz
+  SHUFFLE_DATASET: False
diff --git a/demos/exploration_demo.py b/demos/exploration_demo.py
new file mode 100644
index 0000000..b0e100f
--- /dev/null
+++ b/demos/exploration_demo.py
@@ -0,0 +1,59 @@
+import cv2
+import pdb
+import math
+import numpy as np
+from utils import *
+
+import habitat
+
+class DummyRLEnv(habitat.RLEnv):
+    def __init__(self, config, dataset=None, env_ind=0):
+        super(DummyRLEnv, self).__init__(config, dataset)
+        self._env_ind = env_ind
+
+    def get_reward_range(self):
+        return -1.0, 1.0
+
+    def get_reward(self, observations):
+        return 0.0
+
+    def get_done(self, observations):
+        done = False
+        if self._env.episode_over:
+            done = True
+        return done
+
+    def get_info(self, observations):
+        return self.habitat_env.get_metrics()
+
+    def get_env_ind(self):
+        return self._env_ind
+
+config = habitat.get_config_pose('demos/configs/exploration_mp3d.yaml')
+
+env = DummyRLEnv(config=config)
+env.seed(1234)
+
+obs = env.reset()
+
+"""
+Action space:
+    MOVE_FORWARD = 0
+    TURN_LEFT = 1
+    TURN_RIGHT = 2
+    STOP = 3
+"""
+
+while True:
+    action = obs['oracle_action_sensor'][0].item()
+    obs, reward, done, info = env.step(action)
+    if done:
+        obs = env.reset()
+
+    rgb_im = proc_rgb(obs['rgb'])
+    fine_occ_im = proc_rgb(obs['fine_occupancy'])
+    coarse_occ_im = proc_rgb(obs['coarse_occupancy'])
+    topdown_im = proc_rgb(info['top_down_map_pose'])
+
+    cv2.imshow('Exploration demo', np.concatenate([rgb_im, fine_occ_im, coarse_occ_im, topdown_im], axis=1))
+    cv2.waitKey(30)
diff --git a/demos/pointnav_demo.py b/demos/pointnav_demo.py
new file mode 100644
index 0000000..e0733a8
--- /dev/null
+++ b/demos/pointnav_demo.py
@@ -0,0 +1,99 @@
+import cv2
+import pdb
+import math
+import numpy as np
+from utils import *
+
+import habitat
+
+class DummyRLEnv(habitat.RLEnv):
+    def __init__(self, config, dataset=None, env_ind=0):
+        super(DummyRLEnv, self).__init__(config, dataset)
+        self.T_exp = config.ENVIRONMENT.T_EXP
+        self.T_nav = config.ENVIRONMENT.T_NAV
+        assert(self.T_exp + self.T_nav == config.ENVIRONMENT.MAX_EPISODE_STEPS)
+        self._env_ind = env_ind
+
+    def step(self, action):
+        observations, reward, done, info = super().step(action)
+        if self._env._elapsed_steps == self.T_exp:
+            observations = self._respawn_agent()
+
+        return observations, reward, done, info
+
+    def _respawn_agent(self):
+        position = self.habitat_env.current_episode.start_nav_position
+        rotation = self.habitat_env.current_episode.start_nav_rotation
+        observations = self.habitat_env._sim.get_observations_at(
+                          position,
+                          rotation,
+                          keep_agent_at_new_pose=True
+                       )
+
+        observations.update(
+            self.habitat_env.task.sensor_suite.get_observations(
+                observations=observations, episode=self.current_episode
+            )
+        )
+
+        return observations
+
+    def get_reward_range(self):
+        return -1.0, 1.0
+
+    def get_reward(self, observations):
+        return 0.0
+
+    def get_done(self, observations):
+        done = False
+        if self._env.episode_over:
+            done = True
+        return done
+
+    def get_info(self, observations):
+        return self.habitat_env.get_metrics()
+
+    def get_env_ind(self):
+        return self._env_ind
+
+config = habitat.get_config_exp_nav('demos/configs/pointnav_mp3d.yaml')
+goal_radius = config.SIMULATOR.FORWARD_STEP_SIZE
+
+env = DummyRLEnv(config=config)
+env.seed(1234)
+
+"""
+Action space:
+    MOVE_FORWARD = 0
+    TURN_LEFT = 1
+    TURN_RIGHT = 2
+    STOP = 3
+"""
+
+for i in range(10):
+    obs = env.reset()
+    for t in range(config.ENVIRONMENT.MAX_EPISODE_STEPS):
+        if t < config.ENVIRONMENT.T_EXP:
+            action = obs['oracle_action_sensor'][0].item()
+        else:
+            action = obs['sp_action_sensor_exp_nav'][0].item()
+
+        obs, reward, done, info = env.step(action)
+
+        if done:
+            cv2.destroyWindow('PointNav: navigation phase')
+            break
+
+        rgb_im = proc_rgb(obs['rgb'])
+        fine_occ_im = proc_rgb(obs['fine_occupancy'])
+        coarse_occ_im = proc_rgb(obs['highres_coarse_occupancy'])
+        topdown_im = proc_rgb(info['top_down_map_exp_nav'])
+
+        if t < config.ENVIRONMENT.T_EXP:
+            cv2.imshow('PointNav: exploration phase', np.concatenate([rgb_im, fine_occ_im, coarse_occ_im, topdown_im], axis=1))
+        else:
+            if t == config.ENVIRONMENT.T_EXP:
+                cv2.destroyWindow('PointNav: exploration phase')
+            cv2.imshow('PointNav: navigation phase', np.concatenate([rgb_im, fine_occ_im, coarse_occ_im, topdown_im], axis=1))
+    
+        cv2.waitKey(150)
diff --git a/demos/pose_estimation_demo.py b/demos/pose_estimation_demo.py
new file mode 100644
index 0000000..e3fd5de
--- /dev/null
+++ b/demos/pose_estimation_demo.py
@@ -0,0 +1,86 @@
+import cv2
+import pdb
+import math
+import numpy as np
+
+from utils import *
+
+import habitat
+
+class DummyRLEnv(habitat.RLEnv):
+    def __init__(self, config, dataset=None, env_ind=0):
+        super(DummyRLEnv, self).__init__(config, dataset)
+        self._env_ind = env_ind
+
+    def get_reward_range(self):
+        return -1.0, 1.0
+
+    def get_reward(self, observations):
+        return 0.0
+
+    def get_done(self, observations):
+        done = False
+        if self._env.episode_over:
+            done = True
+        return done
+
+    def get_info(self, observations):
+        return self.habitat_env.get_metrics()
+
+    def get_env_ind(self):
+        return self._env_ind
+
+def create_reference_grid(refs_uint8):
+    """
+    Inputs:
+        refs_uint8 - (nRef, H, W, C) numpy array
+    """
+    refs_uint8 = np.copy(refs_uint8)
+    nRef, H, W, C = refs_uint8.shape
+
+    nrow = int(math.sqrt(nRef))
+
+    ncol = nRef // nrow # (number of images per column)
+    if nrow * ncol < nRef:
+        ncol += 1
+    final_grid = np.zeros((nrow * ncol, *refs_uint8.shape[1:]), dtype=np.uint8)
+    font = cv2.FONT_HERSHEY_SIMPLEX
+
+    final_grid[:nRef] = refs_uint8
+    final_grid = final_grid.reshape(ncol, nrow, *final_grid.shape[1:]) # (ncol, nrow, H, W, C)
+    final_grid = final_grid.transpose(0, 2, 1, 3, 4)
+    final_grid = final_grid.reshape(ncol * H, nrow * W, C)
+    return final_grid
+
+config = habitat.get_config_pose('demos/configs/pose_estimation_mp3d.yaml')
+goal_radius = config.SIMULATOR.FORWARD_STEP_SIZE
+
+env = DummyRLEnv(config=config)
+env.seed(1234)
+
+obs = env.reset()
+
+"""
+Action space:
+    MOVE_FORWARD = 0
+    TURN_LEFT = 1
+    TURN_RIGHT = 2
+    STOP = 3
+"""
+
+pose_refs_rgb = proc_rgb(create_reference_grid(obs['pose_estimation_rgb']))
+
+while True:
+    action = obs['oracle_action_sensor'][0].item()
+    obs, reward, done, info = env.step(action)
+
+    if done:
+        obs = env.reset()
+        pose_refs_rgb = proc_rgb(create_reference_grid(obs['pose_estimation_rgb']))
+
+    rgb_im = proc_rgb(obs['rgb'])
+    topdown_im = proc_rgb(info['top_down_map_pose'])
+
+    cv2.imshow('Pose estimation demo', np.concatenate([rgb_im, topdown_im, pose_refs_rgb], axis=1))
+    cv2.waitKey(60)
+
diff --git a/demos/reconstruction_demo.py b/demos/reconstruction_demo.py
new file mode 100644
index 0000000..702c1e9
--- /dev/null
+++ b/demos/reconstruction_demo.py
@@ -0,0 +1,86 @@
+import cv2
+import pdb
+import math
+import numpy as np
+
+from utils import *
+
+import habitat
+
+class DummyRLEnv(habitat.RLEnv):
+    def __init__(self, config, dataset=None, env_ind=0):
+        super(DummyRLEnv, self).__init__(config, dataset)
+        self._env_ind = env_ind
+
+    def get_reward_range(self):
+        return -1.0, 1.0
+
+    def get_reward(self, observations):
+        return 0.0
+
+    def get_done(self, observations):
+        done = False
+        if self._env.episode_over:
+            done = True
+        return done
+
+    def get_info(self, observations):
+        return self.habitat_env.get_metrics()
+
+    def get_env_ind(self):
+        return self._env_ind
+
+def create_reference_grid(refs_uint8):
+    """
+    Inputs:
+        refs_uint8 - (nRef, H, W, C) numpy array
+    """
+    refs_uint8 = np.copy(refs_uint8)
+    nRef, H, W, C = refs_uint8.shape
+
+    nrow = int(math.sqrt(nRef))
+
+    ncol = nRef // nrow # (number of images per column)
+    if nrow * ncol < nRef:
+        ncol += 1
+    final_grid = np.zeros((nrow * ncol, *refs_uint8.shape[1:]), dtype=np.uint8)
+    font = cv2.FONT_HERSHEY_SIMPLEX
+
+    final_grid[:nRef] = refs_uint8
+    final_grid = final_grid.reshape(ncol, nrow, *final_grid.shape[1:]) # (ncol, nrow, H, W, C)
+    final_grid = final_grid.transpose(0, 2, 1, 3, 4)
+    final_grid = final_grid.reshape(ncol * H, nrow * W, C)
+    return final_grid
+
+config = habitat.get_config_pose('demos/configs/reconstruction_mp3d.yaml')
+goal_radius = config.SIMULATOR.FORWARD_STEP_SIZE
+
+env = DummyRLEnv(config=config)
+env.seed(1234)
+
+obs = env.reset()
+
+"""
+Action space:
+    MOVE_FORWARD = 0
+    TURN_LEFT = 1
+    TURN_RIGHT = 2
+    STOP = 3
+"""
+
+pose_refs_rgb = proc_rgb(create_reference_grid(obs['pose_estimation_rgb']))
+
+while True:
+    action = obs['oracle_action_sensor'][0].item()
+    obs, reward, done, info = env.step(action)
+
+    if done:
+        obs = env.reset()
+        pose_refs_rgb = proc_rgb(create_reference_grid(obs['pose_estimation_rgb']))
+
+    rgb_im = proc_rgb(obs['rgb'])
+    topdown_im = proc_rgb(info['top_down_map_pose'])
+
+    cv2.imshow('Reconstruction demo', np.concatenate([rgb_im, topdown_im, pose_refs_rgb], axis=1))
+    cv2.waitKey(60)
+
diff --git a/demos/utils.py b/demos/utils.py
new file mode 100644
index 0000000..e2ebec2
--- /dev/null
+++ b/demos/utils.py
@@ -0,0 +1,7 @@
+import cv2
+import pdb
+import math
+import numpy as np
+
+def proc_rgb(rgb):
+    return cv2.resize(np.flip(rgb, axis=2), (300, 300))
diff --git a/habitat/config/default.py b/habitat/config/default.py
index 8fd60e7..b33d40f 100644
--- a/habitat/config/default.py
+++ b/habitat/config/default.py
@@ -8,7 +8,7 @@ from typing import List, Optional, Union
 
 from habitat.config import Config as CN  # type: ignore
 
-DEFAULT_CONFIG_DIR = "configs/"
+DEFAULT_CONFIG_DIR = "configs"
 CONFIG_FILE_SEPARATOR = ","
 
 # -----------------------------------------------------------------------------
@@ -55,6 +55,30 @@ _C.TASK.PROXIMITY_SENSOR = CN()
 _C.TASK.PROXIMITY_SENSOR.TYPE = "ProximitySensor"
 _C.TASK.PROXIMITY_SENSOR.MAX_DETECTION_RADIUS = 2.0
 # -----------------------------------------------------------------------------
+# # LOCAL TOP DOWN SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.LOCAL_TOP_DOWN_SENSOR = CN()
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.TYPE = "LocalTopDownSensor"
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.WIDTH = 480
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.HEIGHT = 640
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.NUM_TOPDOWN_MAP_SAMPLE_POINTS = 20000
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.MAP_SCALE = 0.1
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.MAP_RANGE = 100
+# -----------------------------------------------------------------------------
+# # ORACLE ACTION SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.ORACLE_ACTION_SENSOR = CN()
+_C.TASK.ORACLE_ACTION_SENSOR.TYPE = "OracleActionSensor"
+_C.TASK.ORACLE_ACTION_SENSOR.GOAL_RADIUS = 0.25
+_C.TASK.ORACLE_ACTION_SENSOR.ORACLE_TYPE = "random"
+_C.TASK.ORACLE_ACTION_SENSOR.NUM_TARGETS = 500
+# -----------------------------------------------------------------------------
+# # SHORTEST PATH ACTION SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.SP_ACTION_SENSOR = CN()
+_C.TASK.SP_ACTION_SENSOR.TYPE = "SPActionSensor"
+_C.TASK.SP_ACTION_SENSOR.GOAL_RADIUS = 0.25
+# -----------------------------------------------------------------------------
 # # SPL MEASUREMENT
 # -----------------------------------------------------------------------------
 _C.TASK.SPL = CN()
@@ -95,6 +119,8 @@ _C.SIMULATOR.SEED = _C.SEED
 _C.SIMULATOR.TURN_ANGLE = 10  # angle to rotate left or right in degrees
 _C.SIMULATOR.TILT_ANGLE = 15  # angle to tilt the camera up or down in degrees
 _C.SIMULATOR.DEFAULT_AGENT_ID = 0
+_C.SIMULATOR.ENABLE_ODOMETRY_NOISE = False
+_C.SIMULATOR.ODOMETER_NOISE_SCALING = 0.0
 # -----------------------------------------------------------------------------
 # # SENSORS
 # -----------------------------------------------------------------------------
@@ -122,6 +148,49 @@ _C.SIMULATOR.DEPTH_SENSOR.NORMALIZE_DEPTH = True
 _C.SIMULATOR.SEMANTIC_SENSOR = SENSOR.clone()
 _C.SIMULATOR.SEMANTIC_SENSOR.TYPE = "HabitatSimSemanticSensor"
 # -----------------------------------------------------------------------------
+# # FINE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.FINE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.FINE_OCC_SENSOR.TYPE = "HabitatSimFineOccSensor"
+# -----------------------------------------------------------------------------
+# # COARSE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.COARSE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.COARSE_OCC_SENSOR.TYPE = "HabitatSimCoarseOccSensor"
+# -----------------------------------------------------------------------------
+# # PROJ-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.PROJ_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.PROJ_OCC_SENSOR.TYPE = "HabitatSimProjOccSensor"
+# -----------------------------------------------------------------------------
+# # HIGHRES-COARSE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.HIGHRES_COARSE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.HIGHRES_COARSE_OCC_SENSOR.TYPE = \
+        "HabitatSimHighResCoarseOccSensor"
+# -----------------------------------------------------------------------------
+# # OCCUPANCY MAPS
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.OCCUPANCY_MAPS = CN()
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT = SENSOR.HEIGHT
+_C.SIMULATOR.OCCUPANCY_MAPS.WIDTH = SENSOR.WIDTH
+_C.SIMULATOR.OCCUPANCY_MAPS.MAP_SCALE = 0.05
+_C.SIMULATOR.OCCUPANCY_MAPS.MAP_SIZE = 800
+_C.SIMULATOR.OCCUPANCY_MAPS.MAX_DEPTH = 3
+_C.SIMULATOR.OCCUPANCY_MAPS.SMALL_MAP_RANGE = 80
+_C.SIMULATOR.OCCUPANCY_MAPS.LARGE_MAP_RANGE = 400
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT_LOWER = 0.1
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT_UPPER = 2.0
+_C.SIMULATOR.OCCUPANCY_MAPS.GET_PROJ_LOC_MAP = False
+_C.SIMULATOR.OCCUPANCY_MAPS.GET_HIGHRES_LOC_MAP = False
+_C.SIMULATOR.OCCUPANCY_MAPS.USE_GT_OCC_MAP = False
+# -----------------------------------------------------------------------------
+# # OBJECT ANNOTATIONS
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.OBJECT_ANNOTATIONS = CN()
+_C.SIMULATOR.OBJECT_ANNOTATIONS.IS_AVAILABLE = False
+_C.SIMULATOR.OBJECT_ANNOTATIONS.PATH = './'
+# -----------------------------------------------------------------------------
 # AGENT
 # -----------------------------------------------------------------------------
 _C.SIMULATOR.AGENT_0 = CN()
@@ -155,6 +224,7 @@ _C.DATASET.CONTENT_SCENES = ["*"]
 _C.DATASET.DATA_PATH = (
     "data/datasets/pointnav/habitat-test-scenes/v1/{split}/{split}.json.gz"
 )
+_C.DATASET.SHUFFLE_DATASET = True
 
 
 # -----------------------------------------------------------------------------
diff --git a/habitat/config/default_exp_nav.py b/habitat/config/default_exp_nav.py
new file mode 100644
index 0000000..de7d0ef
--- /dev/null
+++ b/habitat/config/default_exp_nav.py
@@ -0,0 +1,298 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List, Optional, Union
+
+from habitat.config import Config as CN  # type: ignore
+
+DEFAULT_CONFIG_DIR = "configs"
+CONFIG_FILE_SEPARATOR = ","
+
+# -----------------------------------------------------------------------------
+# Config definition
+# -----------------------------------------------------------------------------
+_C = CN()
+_C.SEED = 100
+# -----------------------------------------------------------------------------
+# ENVIRONMENT
+# -----------------------------------------------------------------------------
+_C.ENVIRONMENT = CN()
+_C.ENVIRONMENT.MAX_EPISODE_STEPS = 1000
+_C.ENVIRONMENT.T_EXP = 500
+_C.ENVIRONMENT.T_NAV = 500
+_C.ENVIRONMENT.MAX_EPISODE_SECONDS = 10000000
+# -----------------------------------------------------------------------------
+# TASK
+# -----------------------------------------------------------------------------
+_C.TASK = CN()
+_C.TASK.TYPE = "ExpNav-v0"
+_C.TASK.SUCCESS_DISTANCE = 0.2
+_C.TASK.SENSORS = []
+_C.TASK.MEASUREMENTS = []
+# -----------------------------------------------------------------------------
+# # HEADING SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.HEADING_SENSOR = CN()
+_C.TASK.HEADING_SENSOR.TYPE = "HeadingSensor"
+# -----------------------------------------------------------------------------
+# # PROXIMITY SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.PROXIMITY_SENSOR = CN()
+_C.TASK.PROXIMITY_SENSOR.TYPE = "ProximitySensor"
+_C.TASK.PROXIMITY_SENSOR.MAX_DETECTION_RADIUS = 2.0
+# -----------------------------------------------------------------------------
+# # LOCAL TOP DOWN SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.LOCAL_TOP_DOWN_SENSOR = CN()
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.TYPE = "LocalTopDownSensor"
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.WIDTH = 480
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.HEIGHT = 640
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.NUM_TOPDOWN_MAP_SAMPLE_POINTS = 20000
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.MAP_SCALE = 0.1
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.MAP_RANGE = 100
+# -----------------------------------------------------------------------------
+# # DELTA SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.DELTA_SENSOR = CN()
+_C.TASK.DELTA_SENSOR.TYPE = "DeltaSensor"
+# -----------------------------------------------------------------------------
+# # ORACLE ACTION SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.ORACLE_ACTION_SENSOR = CN()
+_C.TASK.ORACLE_ACTION_SENSOR.TYPE = "OracleActionSensor"
+_C.TASK.ORACLE_ACTION_SENSOR.GOAL_RADIUS = 0.25
+_C.TASK.ORACLE_ACTION_SENSOR.ORACLE_TYPE = "random"
+_C.TASK.ORACLE_ACTION_SENSOR.NUM_TARGETS = 500
+# -----------------------------------------------------------------------------
+# # COLLISION SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.COLLISION_SENSOR = CN()
+_C.TASK.COLLISION_SENSOR.TYPE = "CollisionSensor"
+# -----------------------------------------------------------------------------
+# # Grid Goal Sensor
+# -----------------------------------------------------------------------------
+_C.TASK.GRID_GOAL_SENSOR = CN()
+_C.TASK.GRID_GOAL_SENSOR.TYPE = "GridGoalSensorExploreNavigation"
+_C.TASK.GRID_GOAL_SENSOR.T_EXP = _C.ENVIRONMENT.T_EXP
+_C.TASK.GRID_GOAL_SENSOR.T_NAV = _C.ENVIRONMENT.T_NAV
+# -----------------------------------------------------------------------------
+# # Shortest Path Action Sensor
+# -----------------------------------------------------------------------------
+_C.TASK.SP_ACTION_SENSOR = CN()
+_C.TASK.SP_ACTION_SENSOR.TYPE = "SPActionSensorExploreNavigation"
+_C.TASK.SP_ACTION_SENSOR.GOAL_RADIUS = 0.25
+_C.TASK.SP_ACTION_SENSOR.T_EXP = _C.ENVIRONMENT.T_EXP
+_C.TASK.SP_ACTION_SENSOR.T_NAV = _C.ENVIRONMENT.T_NAV
+# -----------------------------------------------------------------------------
+# # TopDownMapPose MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.TOP_DOWN_MAP_EXP_NAV = CN()
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.TYPE = "TopDownMapExpNav"
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.MAX_EPISODE_STEPS = _C.ENVIRONMENT.MAX_EPISODE_STEPS
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.T_EXP = _C.ENVIRONMENT.T_EXP
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.T_NAV = _C.ENVIRONMENT.T_NAV
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.MAP_PADDING = 3
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.NUM_TOPDOWN_MAP_SAMPLE_POINTS = 20000
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.MAP_RESOLUTION = 1250
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.DRAW_SOURCE_AND_TARGET = True
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.DRAW_BORDER = True
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.DRAW_SHORTEST_PATH = True
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.FOG_OF_WAR = CN()
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.FOG_OF_WAR.DRAW = True
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.FOG_OF_WAR.VISIBILITY_DIST = 5.0
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.FOG_OF_WAR.FOV = 90
+# -----------------------------------------------------------------------------
+# # COLLISIONS MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.COLLISIONS = CN()
+_C.TASK.COLLISIONS.TYPE = "Collisions"
+# -----------------------------------------------------------------------------
+# SIMULATOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR = CN()
+_C.SIMULATOR.TYPE = "Sim-v0"
+_C.SIMULATOR.ACTION_SPACE_CONFIG = "v0"
+_C.SIMULATOR.FORWARD_STEP_SIZE = 0.25  # in metres
+_C.SIMULATOR.SCENE = (
+    "data/scene_datasets/habitat-test-scenes/" "van-gogh-room.glb"
+)
+_C.SIMULATOR.SEED = _C.SEED
+_C.SIMULATOR.TURN_ANGLE = 10  # angle to rotate left or right in degrees
+_C.SIMULATOR.TILT_ANGLE = 15  # angle to tilt the camera up or down in degrees
+_C.SIMULATOR.DEFAULT_AGENT_ID = 0
+# -----------------------------------------------------------------------------
+# # SENSORS
+# -----------------------------------------------------------------------------
+SENSOR = CN()
+SENSOR.HEIGHT = 480
+SENSOR.WIDTH = 640
+SENSOR.HFOV = 90  # horizontal field of view in degrees
+SENSOR.POSITION = [0, 1.25, 0]
+# -----------------------------------------------------------------------------
+# # RGB SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.RGB_SENSOR = SENSOR.clone()
+_C.SIMULATOR.RGB_SENSOR.TYPE = "HabitatSimRGBSensor"
+# -----------------------------------------------------------------------------
+# DEPTH SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.DEPTH_SENSOR = SENSOR.clone()
+_C.SIMULATOR.DEPTH_SENSOR.TYPE = "HabitatSimDepthSensor"
+_C.SIMULATOR.DEPTH_SENSOR.MIN_DEPTH = 0
+_C.SIMULATOR.DEPTH_SENSOR.MAX_DEPTH = 10
+_C.SIMULATOR.DEPTH_SENSOR.NORMALIZE_DEPTH = True
+# -----------------------------------------------------------------------------
+# SEMANTIC SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.SEMANTIC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.SEMANTIC_SENSOR.TYPE = "HabitatSimSemanticSensor"
+# -----------------------------------------------------------------------------
+# # FINE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.FINE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.FINE_OCC_SENSOR.TYPE = "HabitatSimFineOccSensor"
+# -----------------------------------------------------------------------------
+# # COARSE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.COARSE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.COARSE_OCC_SENSOR.TYPE = "HabitatSimCoarseOccSensor"
+# -----------------------------------------------------------------------------
+# # HIGHRES-COARSE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.HIGHRES_COARSE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.HIGHRES_COARSE_OCC_SENSOR.TYPE = "HabitatSimHighResCoarseOccSensor"
+# -----------------------------------------------------------------------------
+# # OCCUPANCY MAPS
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.OCCUPANCY_MAPS = CN()
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT = SENSOR.HEIGHT
+_C.SIMULATOR.OCCUPANCY_MAPS.WIDTH = SENSOR.WIDTH
+_C.SIMULATOR.OCCUPANCY_MAPS.MAP_SCALE = 0.05
+_C.SIMULATOR.OCCUPANCY_MAPS.MAP_SIZE = 800
+_C.SIMULATOR.OCCUPANCY_MAPS.MAX_DEPTH = 3
+_C.SIMULATOR.OCCUPANCY_MAPS.SMALL_MAP_RANGE = 80
+_C.SIMULATOR.OCCUPANCY_MAPS.LARGE_MAP_RANGE = 400
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT_LOWER = 0.1
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT_UPPER = 2.0
+_C.SIMULATOR.OCCUPANCY_MAPS.GET_PROJ_LOC_MAP = False
+_C.SIMULATOR.OCCUPANCY_MAPS.GET_HIGHRES_LOC_MAP = True
+_C.SIMULATOR.OCCUPANCY_MAPS.USE_GT_OCC_MAP = False
+# -----------------------------------------------------------------------------
+# # OBJECT ANNOTATIONS
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.OBJECT_ANNOTATIONS = CN()
+_C.SIMULATOR.OBJECT_ANNOTATIONS.IS_AVAILABLE = False
+_C.SIMULATOR.OBJECT_ANNOTATIONS.PATH = './'
+# -----------------------------------------------------------------------------
+# # IMAGE GOAL SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.IMAGE_GOAL_SENSOR = SENSOR.clone()
+_C.TASK.IMAGE_GOAL_SENSOR.TYPE = "ImageGoalSensorExploreNavigation"
+_C.TASK.IMAGE_GOAL_SENSOR.T_EXP = _C.ENVIRONMENT.T_EXP
+_C.TASK.IMAGE_GOAL_SENSOR.T_NAV = _C.ENVIRONMENT.T_NAV
+# -----------------------------------------------------------------------------
+# # SPL MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.SPL_EXP_NAV = CN()
+_C.TASK.SPL_EXP_NAV.TYPE = "SPLExpNav"
+_C.TASK.SPL_EXP_NAV.SUCCESS_DISTANCE = 0.2
+_C.TASK.SPL_EXP_NAV.T_EXP = _C.ENVIRONMENT.T_EXP
+_C.TASK.SPL_EXP_NAV.T_NAV = _C.ENVIRONMENT.T_NAV
+# -----------------------------------------------------------------------------
+# # SUCCESS MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.SUCCESS_EXP_NAV = CN()
+_C.TASK.SUCCESS_EXP_NAV.TYPE = "SuccessExpNav"
+_C.TASK.SUCCESS_EXP_NAV.SUCCESS_DISTANCE = 0.2
+_C.TASK.SUCCESS_EXP_NAV.T_EXP = _C.ENVIRONMENT.T_EXP
+_C.TASK.SUCCESS_EXP_NAV.T_NAV = _C.ENVIRONMENT.T_NAV
+# -----------------------------------------------------------------------------
+# # NAVIGATION ERROR MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.NAVIGATION_ERROR_EXP_NAV = CN()
+_C.TASK.NAVIGATION_ERROR_EXP_NAV.TYPE = "NavigationErrorExpNav"
+_C.TASK.NAVIGATION_ERROR_EXP_NAV.T_EXP = _C.ENVIRONMENT.T_EXP
+_C.TASK.NAVIGATION_ERROR_EXP_NAV.T_NAV = _C.ENVIRONMENT.T_NAV
+# -----------------------------------------------------------------------------
+# # AREA COVERED MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.AREA_COVERED = CN()
+_C.TASK.AREA_COVERED.TYPE = "AreaCovered"
+# -----------------------------------------------------------------------------
+# # COLLISIONS MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.COLLISIONS = CN()
+_C.TASK.COLLISIONS.TYPE = "Collisions"
+# -----------------------------------------------------------------------------
+# AGENT
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.AGENT_0 = CN()
+_C.SIMULATOR.AGENT_0.HEIGHT = 1.5
+_C.SIMULATOR.AGENT_0.RADIUS = 0.1
+_C.SIMULATOR.AGENT_0.MASS = 32.0
+_C.SIMULATOR.AGENT_0.LINEAR_ACCELERATION = 20.0
+_C.SIMULATOR.AGENT_0.ANGULAR_ACCELERATION = 4 * 3.14
+_C.SIMULATOR.AGENT_0.LINEAR_FRICTION = 0.5
+_C.SIMULATOR.AGENT_0.ANGULAR_FRICTION = 1.0
+_C.SIMULATOR.AGENT_0.COEFFICIENT_OF_RESTITUTION = 0.0
+_C.SIMULATOR.AGENT_0.SENSORS = ["RGB_SENSOR"]
+_C.SIMULATOR.AGENT_0.IS_SET_START_STATE = False
+_C.SIMULATOR.AGENT_0.START_POSITION = [0, 0, 0]
+_C.SIMULATOR.AGENT_0.START_ROTATION = [0, 0, 0, 1]
+_C.SIMULATOR.AGENTS = ["AGENT_0"]
+# -----------------------------------------------------------------------------
+# SIMULATOR HABITAT_SIM_V0
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.HABITAT_SIM_V0 = CN()
+_C.SIMULATOR.HABITAT_SIM_V0.GPU_DEVICE_ID = 0
+# -----------------------------------------------------------------------------
+# DATASET
+# -----------------------------------------------------------------------------
+_C.DATASET = CN()
+_C.DATASET.TYPE = "ExpNav-v1"
+_C.DATASET.SPLIT = "train"
+_C.DATASET.SCENES_DIR = "data/scene_datasets"
+_C.DATASET.NUM_EPISODE_SAMPLE = -1
+_C.DATASET.CONTENT_SCENES = ["*"]
+_C.DATASET.DATA_PATH = (
+    "data/datasets/exp_nav/habitat-test-scenes/v1/{split}/{split}.json.gz"
+)
+_C.DATASET.SHUFFLE_DATASET = True
+
+
+# -----------------------------------------------------------------------------
+
+
+def get_config_exp_nav(
+    config_paths: Optional[Union[List[str], str]] = None,
+    opts: Optional[list] = None,
+) -> CN:
+    r"""Create a unified config with default values overwritten by values from
+    `config_paths` and overwritten by options from `opts`.
+    Args:
+        config_paths: List of config paths or string that contains comma
+        separated list of config paths.
+        opts: Config options (keys, values) in a list (e.g., passed from
+        command line into the config. For example, `opts = ['FOO.BAR',
+        0.5]`. Argument can be used for parameter sweeping or quick tests.
+    """
+    config = _C.clone()
+    if config_paths:
+        if isinstance(config_paths, str):
+            if CONFIG_FILE_SEPARATOR in config_paths:
+                config_paths = config_paths.split(CONFIG_FILE_SEPARATOR)
+            else:
+                config_paths = [config_paths]
+
+        for config_path in config_paths:
+            config.merge_from_file(config_path)
+
+    if opts:
+        config.merge_from_list(opts)
+
+    config.freeze()
+    return config
+
diff --git a/habitat/config/default_pose.py b/habitat/config/default_pose.py
new file mode 100644
index 0000000..c51a731
--- /dev/null
+++ b/habitat/config/default_pose.py
@@ -0,0 +1,286 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List, Optional, Union
+
+from habitat.config import Config as CN  # type: ignore
+
+DEFAULT_CONFIG_DIR = "configs"
+CONFIG_FILE_SEPARATOR = ","
+
+# -----------------------------------------------------------------------------
+# Config definition
+# -----------------------------------------------------------------------------
+_C = CN()
+_C.SEED = 100
+# -----------------------------------------------------------------------------
+# ENVIRONMENT
+# -----------------------------------------------------------------------------
+_C.ENVIRONMENT = CN()
+_C.ENVIRONMENT.MAX_EPISODE_STEPS = 1000
+_C.ENVIRONMENT.MAX_EPISODE_SECONDS = 10000000
+# -----------------------------------------------------------------------------
+# TASK
+# -----------------------------------------------------------------------------
+_C.TASK = CN()
+_C.TASK.TYPE = "Pose-v0"
+_C.TASK.SENSORS = []
+_C.TASK.MEASUREMENTS = []
+# -----------------------------------------------------------------------------
+# # HEADING SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.HEADING_SENSOR = CN()
+_C.TASK.HEADING_SENSOR.TYPE = "HeadingSensor"
+# -----------------------------------------------------------------------------
+# # PROXIMITY SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.PROXIMITY_SENSOR = CN()
+_C.TASK.PROXIMITY_SENSOR.TYPE = "ProximitySensor"
+_C.TASK.PROXIMITY_SENSOR.MAX_DETECTION_RADIUS = 2.0
+# -----------------------------------------------------------------------------
+# # POSE REGRESS SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.POSE_REGRESS_SENSOR = CN()
+_C.TASK.POSE_REGRESS_SENSOR.TYPE = "PoseEstimationRegressSensor"
+_C.TASK.POSE_REGRESS_SENSOR.NREF = 20
+# -----------------------------------------------------------------------------
+# # DELTA SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.DELTA_SENSOR = CN()
+_C.TASK.DELTA_SENSOR.TYPE = "DeltaSensor"
+# -----------------------------------------------------------------------------
+# # ORACLE ACTION SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.ORACLE_ACTION_SENSOR = CN()
+_C.TASK.ORACLE_ACTION_SENSOR.TYPE = "OracleActionSensor"
+_C.TASK.ORACLE_ACTION_SENSOR.GOAL_RADIUS = 0.25
+_C.TASK.ORACLE_ACTION_SENSOR.ORACLE_TYPE = "random"
+_C.TASK.ORACLE_ACTION_SENSOR.NUM_TARGETS = 500
+# -----------------------------------------------------------------------------
+# # COLLISION SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.COLLISION_SENSOR = CN()
+_C.TASK.COLLISION_SENSOR.TYPE = "CollisionSensor"
+# -----------------------------------------------------------------------------
+# # OPSR MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.OPSR = CN()
+_C.TASK.OPSR.TYPE = "OPSR"
+_C.TASK.OPSR.GEODESIC_DIST_THRESH = 2.0 # Meters
+_C.TASK.OPSR.ANGULAR_DIST_THRESH = 20.0 # Degrees
+# -----------------------------------------------------------------------------
+# # AREA COVERED MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.AREA_COVERED = CN()
+_C.TASK.AREA_COVERED.TYPE = "AreaCovered"
+# -----------------------------------------------------------------------------
+# # INC AREA COVERED MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.INC_AREA_COVERED = CN()
+_C.TASK.INC_AREA_COVERED.TYPE = "IncAreaCovered"
+# -----------------------------------------------------------------------------
+# # FRAC AREA COVERED MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.FRAC_AREA_COVERED = CN()
+_C.TASK.FRAC_AREA_COVERED.TYPE = "FracAreaCovered"
+# -----------------------------------------------------------------------------
+# # TopDownMapPose MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.TOP_DOWN_MAP_POSE = CN()
+_C.TASK.TOP_DOWN_MAP_POSE.TYPE = "TopDownMapPose"
+_C.TASK.TOP_DOWN_MAP_POSE.MAX_EPISODE_STEPS = _C.ENVIRONMENT.MAX_EPISODE_STEPS
+_C.TASK.TOP_DOWN_MAP_POSE.MAP_PADDING = 3
+_C.TASK.TOP_DOWN_MAP_POSE.NUM_TOPDOWN_MAP_SAMPLE_POINTS = 20000
+_C.TASK.TOP_DOWN_MAP_POSE.MAP_RESOLUTION = 1250
+_C.TASK.TOP_DOWN_MAP_POSE.DRAW_SOURCE_AND_REFERENCES = True
+_C.TASK.TOP_DOWN_MAP_POSE.DRAW_BORDER = True
+_C.TASK.TOP_DOWN_MAP_POSE.DRAW_SHORTEST_PATH = True
+_C.TASK.TOP_DOWN_MAP_POSE.FOG_OF_WAR = CN()
+_C.TASK.TOP_DOWN_MAP_POSE.FOG_OF_WAR.DRAW = True
+_C.TASK.TOP_DOWN_MAP_POSE.FOG_OF_WAR.VISIBILITY_DIST = 5.0
+_C.TASK.TOP_DOWN_MAP_POSE.FOG_OF_WAR.FOV = 90
+# -----------------------------------------------------------------------------
+# # COLLISIONS MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.COLLISIONS = CN()
+_C.TASK.COLLISIONS.TYPE = "Collisions"
+# -----------------------------------------------------------------------------
+# # OBJECTS COVERED GEOMETRIC MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.OBJECTS_COVERED_GEOMETRIC = CN()
+_C.TASK.OBJECTS_COVERED_GEOMETRIC.TYPE = "ObjectsCoveredGeometric"
+# -----------------------------------------------------------------------------
+# # NOVELTY_REWARD MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.NOVELTY_REWARD = CN()
+_C.TASK.NOVELTY_REWARD.TYPE = "NoveltyReward"
+_C.TASK.NOVELTY_REWARD.GRID_SIZE = 0.5 # In meters
+# -----------------------------------------------------------------------------
+# SIMULATOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR = CN()
+_C.SIMULATOR.TYPE = "Sim-v0"
+_C.SIMULATOR.ACTION_SPACE_CONFIG = "v0"
+_C.SIMULATOR.FORWARD_STEP_SIZE = 0.25  # in metres
+_C.SIMULATOR.SCENE = (
+    "data/scene_datasets/habitat-test-scenes/" "van-gogh-room.glb"
+)
+_C.SIMULATOR.SEED = _C.SEED
+_C.SIMULATOR.TURN_ANGLE = 10  # angle to rotate left or right in degrees
+_C.SIMULATOR.TILT_ANGLE = 15  # angle to tilt the camera up or down in degrees
+_C.SIMULATOR.DEFAULT_AGENT_ID = 0
+# -----------------------------------------------------------------------------
+# # SENSORS
+# -----------------------------------------------------------------------------
+SENSOR = CN()
+SENSOR.HEIGHT = 480
+SENSOR.WIDTH = 640
+SENSOR.HFOV = 90  # horizontal field of view in degrees
+SENSOR.POSITION = [0, 1.25, 0]
+# -----------------------------------------------------------------------------
+# # RGB SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.RGB_SENSOR = SENSOR.clone()
+_C.SIMULATOR.RGB_SENSOR.TYPE = "HabitatSimRGBSensor"
+# -----------------------------------------------------------------------------
+# DEPTH SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.DEPTH_SENSOR = SENSOR.clone()
+_C.SIMULATOR.DEPTH_SENSOR.TYPE = "HabitatSimDepthSensor"
+_C.SIMULATOR.DEPTH_SENSOR.MIN_DEPTH = 0
+_C.SIMULATOR.DEPTH_SENSOR.MAX_DEPTH = 10
+_C.SIMULATOR.DEPTH_SENSOR.NORMALIZE_DEPTH = True
+# -----------------------------------------------------------------------------
+# SEMANTIC SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.SEMANTIC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.SEMANTIC_SENSOR.TYPE = "HabitatSimSemanticSensor"
+# -----------------------------------------------------------------------------
+# # FINE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.FINE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.FINE_OCC_SENSOR.TYPE = "HabitatSimFineOccSensor"
+# -----------------------------------------------------------------------------
+# # COARSE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.COARSE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.COARSE_OCC_SENSOR.TYPE = "HabitatSimCoarseOccSensor"
+# -----------------------------------------------------------------------------
+# # HIGHRES COARSE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.HIGHRES_COARSE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.HIGHRES_COARSE_OCC_SENSOR.TYPE = "HabitatSimHighResCoarseOccSensor"
+# -----------------------------------------------------------------------------
+# # OCCUPANCY MAPS
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.OCCUPANCY_MAPS = CN()
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT = SENSOR.HEIGHT
+_C.SIMULATOR.OCCUPANCY_MAPS.WIDTH = SENSOR.WIDTH
+_C.SIMULATOR.OCCUPANCY_MAPS.MAP_SCALE = 0.05
+_C.SIMULATOR.OCCUPANCY_MAPS.MAP_SIZE = 800
+_C.SIMULATOR.OCCUPANCY_MAPS.MAX_DEPTH = 3
+_C.SIMULATOR.OCCUPANCY_MAPS.SMALL_MAP_RANGE = 80
+_C.SIMULATOR.OCCUPANCY_MAPS.LARGE_MAP_RANGE = 400
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT_LOWER = 0.1
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT_UPPER = 2.0
+_C.SIMULATOR.OCCUPANCY_MAPS.GET_PROJ_LOC_MAP = False
+_C.SIMULATOR.OCCUPANCY_MAPS.GET_HIGHRES_LOC_MAP = False
+_C.SIMULATOR.OCCUPANCY_MAPS.USE_GT_OCC_MAP = False
+# -----------------------------------------------------------------------------
+# # OBJECT ANNOTATIONS
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.OBJECT_ANNOTATIONS = CN()
+_C.SIMULATOR.OBJECT_ANNOTATIONS.IS_AVAILABLE = False
+_C.SIMULATOR.OBJECT_ANNOTATIONS.PATH = './'
+# -----------------------------------------------------------------------------
+# # POSE RGB SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.POSE_RGB_SENSOR = SENSOR.clone()
+_C.TASK.POSE_RGB_SENSOR.TYPE = "PoseEstimationRGBSensor"
+_C.TASK.POSE_RGB_SENSOR.NREF = 20
+# -----------------------------------------------------------------------------
+# # POSE DEPTH SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.POSE_DEPTH_SENSOR = _C.SIMULATOR.DEPTH_SENSOR.clone()
+_C.TASK.POSE_DEPTH_SENSOR.TYPE = "PoseEstimationDepthSensor"
+_C.TASK.POSE_DEPTH_SENSOR.NREF = 20
+# -----------------------------------------------------------------------------
+# # POSE MASK SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.POSE_MASK_SENSOR = CN()
+_C.TASK.POSE_MASK_SENSOR.TYPE = "PoseEstimationMaskSensor"
+_C.TASK.POSE_MASK_SENSOR.NREF = 20
+
+# -----------------------------------------------------------------------------
+# AGENT
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.AGENT_0 = CN()
+_C.SIMULATOR.AGENT_0.HEIGHT = 1.5
+_C.SIMULATOR.AGENT_0.RADIUS = 0.1
+_C.SIMULATOR.AGENT_0.MASS = 32.0
+_C.SIMULATOR.AGENT_0.LINEAR_ACCELERATION = 20.0
+_C.SIMULATOR.AGENT_0.ANGULAR_ACCELERATION = 4 * 3.14
+_C.SIMULATOR.AGENT_0.LINEAR_FRICTION = 0.5
+_C.SIMULATOR.AGENT_0.ANGULAR_FRICTION = 1.0
+_C.SIMULATOR.AGENT_0.COEFFICIENT_OF_RESTITUTION = 0.0
+_C.SIMULATOR.AGENT_0.SENSORS = ["RGB_SENSOR"]
+_C.SIMULATOR.AGENT_0.IS_SET_START_STATE = False
+_C.SIMULATOR.AGENT_0.START_POSITION = [0, 0, 0]
+_C.SIMULATOR.AGENT_0.START_ROTATION = [0, 0, 0, 1]
+_C.SIMULATOR.AGENTS = ["AGENT_0"]
+# -----------------------------------------------------------------------------
+# SIMULATOR HABITAT_SIM_V0
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.HABITAT_SIM_V0 = CN()
+_C.SIMULATOR.HABITAT_SIM_V0.GPU_DEVICE_ID = 0
+# -----------------------------------------------------------------------------
+# DATASET
+# -----------------------------------------------------------------------------
+_C.DATASET = CN()
+_C.DATASET.TYPE = "PoseEstimation-v1"
+_C.DATASET.SPLIT = "train"
+_C.DATASET.SCENES_DIR = "data/scene_datasets"
+_C.DATASET.NUM_EPISODE_SAMPLE = -1
+_C.DATASET.CONTENT_SCENES = ["*"]
+_C.DATASET.DATA_PATH = (
+    "pose_estimation/habitat-test-scenes/v1/{split}/{split}.json.gz"
+)
+_C.DATASET.SHUFFLE_DATASET = True
+
+
+# -----------------------------------------------------------------------------
+
+
+def get_config_pose(
+    config_paths: Optional[Union[List[str], str]] = None,
+    opts: Optional[list] = None,
+) -> CN:
+    r"""Create a unified config with default values overwritten by values from
+    `config_paths` and overwritten by options from `opts`.
+    Args:
+        config_paths: List of config paths or string that contains comma
+        separated list of config paths.
+        opts: Config options (keys, values) in a list (e.g., passed from
+        command line into the config. For example, `opts = ['FOO.BAR',
+        0.5]`. Argument can be used for parameter sweeping or quick tests.
+    """
+    config = _C.clone()
+    if config_paths:
+        if isinstance(config_paths, str):
+            if CONFIG_FILE_SEPARATOR in config_paths:
+                config_paths = config_paths.split(CONFIG_FILE_SEPARATOR)
+            else:
+                config_paths = [config_paths]
+
+        for config_path in config_paths:
+            config.merge_from_file(config_path)
+
+    if opts:
+        config.merge_from_list(opts)
+
+    config.freeze()
+    return config
+
diff --git a/habitat/core/env.py b/habitat/core/env.py
index 7141050..78ddbf2 100644
--- a/habitat/core/env.py
+++ b/habitat/core/env.py
@@ -190,6 +190,9 @@ class Env:
         assert len(self.episodes) > 0, "Episodes list is empty"
 
         self.current_episode = next(self._episode_iterator)
+        episode_id = self.current_episode.episode_id
+        scene_id = self.current_episode.scene_id
+        print(f'=====> Env: Sampled episode: {episode_id}, scene: {scene_id}')
         self.reconfigure(self._config)
 
         observations = self._sim.reset()
diff --git a/habitat/core/simulator.py b/habitat/core/simulator.py
index 7effea5..1ab4273 100644
--- a/habitat/core/simulator.py
+++ b/habitat/core/simulator.py
@@ -27,10 +27,10 @@ class ActionSpaceConfiguration(abc.ABC):
 
 
 class _DefaultSimulatorActions(Enum):
-    STOP = 0
-    MOVE_FORWARD = 1
-    TURN_LEFT = 2
-    TURN_RIGHT = 3
+    MOVE_FORWARD = 0
+    TURN_LEFT = 1
+    TURN_RIGHT = 2
+    STOP = 3
     LOOK_UP = 4
     LOOK_DOWN = 5
 
@@ -232,6 +232,74 @@ class SemanticSensor(Sensor):
         raise NotImplementedError
 
 
+class FineOccSensor(Sensor):
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
+        super().__init__(*args, **kwargs)
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "fine_occupancy"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any) -> SensorTypes:
+        return SensorTypes.COLOR
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any) -> Space:
+        raise NotImplementedError
+
+    def get_observation(self, *args: Any, **kwargs: Any) -> Any:
+        raise NotImplementedError
+
+
+class CoarseOccSensor(Sensor):
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
+        super().__init__(*args, **kwargs)
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "coarse_occupancy"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any) -> SensorTypes:
+        return SensorTypes.COLOR
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any) -> Space:
+        raise NotImplementedError
+
+    def get_observation(self, *args: Any, **kwargs: Any) -> Any:
+        raise NotImplementedError
+
+
+class HighResCoarseOccSensor(Sensor):
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
+        super().__init__(*args, **kwargs)
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "highres_coarse_occupancy"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any) -> SensorTypes:
+        return SensorTypes.COLOR
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any) -> Space:
+        raise NotImplementedError
+
+    def get_observation(self, *args: Any, **kwargs: Any) -> Any:
+        raise NotImplementedError
+
+
+class ProjOccSensor(Sensor):
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
+        super().__init__(*args, **kwargs)
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "proj_occupancy"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any) -> SensorTypes:
+        return SensorTypes.COLOR
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any) -> Space:
+        raise NotImplementedError
+
+    def get_observation(self, *args: Any, **kwargs: Any) -> Any:
+        raise NotImplementedError
+
+
 class SensorSuite:
     r"""Represents a set of sensors, with each sensor being identified
     through a unique id.
diff --git a/habitat/datasets/exp_nav/__init__.py b/habitat/datasets/exp_nav/__init__.py
new file mode 100644
index 0000000..240697e
--- /dev/null
+++ b/habitat/datasets/exp_nav/__init__.py
@@ -0,0 +1,5 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
diff --git a/habitat/datasets/exp_nav/exp_nav_dataset.py b/habitat/datasets/exp_nav/exp_nav_dataset.py
new file mode 100644
index 0000000..2bad271
--- /dev/null
+++ b/habitat/datasets/exp_nav/exp_nav_dataset.py
@@ -0,0 +1,135 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+import os
+import pdb
+import gzip
+import json
+import numpy as np
+from typing import List, Optional
+
+from habitat.config import Config
+from habitat.core.dataset import Dataset
+from habitat.core.registry import registry
+from habitat.tasks.exp_nav.exp_nav_task import (
+    ExploreNavigationEpisode,
+    ImageGoal,
+    ShortestPathPoint,
+)
+
+ALL_SCENES_MASK = "*"
+CONTENT_SCENES_PATH_FIELD = "content_scenes_path"
+DEFAULT_SCENE_PATH_PREFIX = "data/scene_datasets/"
+
+
+@registry.register_dataset(name="ExpNav-v1")
+class ExpNavDatasetV1(Dataset):
+    r"""Class inherited from Dataset that loads Explore Navigation dataset.
+    """
+
+    episodes: List[ExploreNavigationEpisode]
+    content_scenes_path: str = "{data_path}/content/{scene}.json.gz"
+    shuffle_dataset: bool = True
+
+    @staticmethod
+    def check_config_paths_exist(config: Config) -> bool:
+        return os.path.exists(
+            config.DATA_PATH.format(split=config.SPLIT)
+        ) and os.path.exists(config.SCENES_DIR)
+
+    @staticmethod
+    def get_scenes_to_load(config: Config) -> List[str]:
+        r"""Return list of scene ids for which dataset has separate files with
+        episodes.
+        """
+        assert ExpNavDatasetV1.check_config_paths_exist(config)
+        dataset_dir = os.path.dirname(
+            config.DATA_PATH.format(split=config.SPLIT)
+        )
+
+        cfg = config.clone()
+        cfg.defrost()
+        cfg.CONTENT_SCENES = []
+        dataset = ExpNavDatasetV1(cfg)
+        return ExpNavDatasetV1._get_scenes_from_folder(
+            content_scenes_path=dataset.content_scenes_path,
+            dataset_dir=dataset_dir,
+        )
+
+    @staticmethod
+    def _get_scenes_from_folder(content_scenes_path, dataset_dir):
+        scenes = []
+        content_dir = content_scenes_path.split("{scene}")[0]
+        scene_dataset_ext = content_scenes_path.split("{scene}")[1]
+        content_dir = content_dir.format(data_path=dataset_dir)
+        if not os.path.exists(content_dir):
+            return scenes
+
+        for filename in os.listdir(content_dir):
+            if filename.endswith(scene_dataset_ext):
+                scene = filename[: -len(scene_dataset_ext)]
+                scenes.append(scene)
+        scenes.sort()
+        return scenes
+
+    def __init__(self, config: Optional[Config] = None) -> None:
+        self.episodes = []
+
+        if config is None:
+            return
+
+        self.shuffle_dataset = config.SHUFFLE_DATASET
+
+        datasetfile_path = config.DATA_PATH.format(split=config.SPLIT)
+        with gzip.open(datasetfile_path, "rt") as f:
+            self.from_json(f.read(), scenes_dir=config.SCENES_DIR)
+
+        # Read separate file for each scene
+        dataset_dir = os.path.dirname(datasetfile_path)
+        scenes = config.CONTENT_SCENES
+        if ALL_SCENES_MASK in scenes:
+            scenes = ExpNavDatasetV1._get_scenes_from_folder(
+                content_scenes_path=self.content_scenes_path,
+                dataset_dir=dataset_dir,
+            )
+
+        for scene in scenes:
+            scene_filename = self.content_scenes_path.format(
+                data_path=dataset_dir, scene=scene
+            )
+            with gzip.open(scene_filename, "rt") as f:
+                self.from_json(f.read(), scenes_dir=config.SCENES_DIR)
+
+        self.sample_episodes(config.NUM_EPISODE_SAMPLE)
+
+    def from_json(
+        self, json_str: str, scenes_dir: Optional[str] = None
+    ) -> None:
+        deserialized = json.loads(json_str)
+        if CONTENT_SCENES_PATH_FIELD in deserialized:
+            self.content_scenes_path = deserialized[CONTENT_SCENES_PATH_FIELD]
+
+        for episode in deserialized["episodes"]:
+            episode = ExploreNavigationEpisode(**episode)
+
+            if scenes_dir is not None:
+                if episode.scene_id.startswith(DEFAULT_SCENE_PATH_PREFIX):
+                    episode.scene_id = episode.scene_id[
+                        len(DEFAULT_SCENE_PATH_PREFIX) :
+                    ]
+
+                episode.scene_id = os.path.join(scenes_dir, episode.scene_id)
+
+            for g_index, goal in enumerate(episode.goals):
+                episode.goals[g_index] = ImageGoal(**goal)
+            if episode.shortest_paths is not None:
+                for path in episode.shortest_paths:
+                    for p_index, point in enumerate(path):
+                        path[p_index] = ShortestPathPoint(**point)
+
+            self.episodes.append(episode)
+
+        print(f'======> Effective number of episodes: {len(self.episodes)}')
diff --git a/habitat/datasets/pose_estimation/__init__.py b/habitat/datasets/pose_estimation/__init__.py
new file mode 100644
index 0000000..240697e
--- /dev/null
+++ b/habitat/datasets/pose_estimation/__init__.py
@@ -0,0 +1,5 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
diff --git a/habitat/datasets/pose_estimation/pose_estimation_dataset.py b/habitat/datasets/pose_estimation/pose_estimation_dataset.py
new file mode 100644
index 0000000..1fae1de
--- /dev/null
+++ b/habitat/datasets/pose_estimation/pose_estimation_dataset.py
@@ -0,0 +1,140 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+import gzip
+import json
+import os
+from itertools import cycle
+from typing import List, Optional
+
+from habitat.config import Config
+from habitat.core.dataset import Dataset
+from habitat.core.registry import registry
+from habitat.tasks.pose_estimation.pose_estimation_task import (
+    PoseEstimationEpisode,
+    ShortestPathPoint,
+)
+
+ALL_SCENES_MASK = "*"
+CONTENT_SCENES_PATH_FIELD = "content_scenes_path"
+DEFAULT_SCENE_PATH_PREFIX = "data/scene_datasets/"
+
+
+@registry.register_dataset(name="PoseEstimation-v1")
+class PoseEstimationDatasetV1(Dataset):
+    r"""Class inherited from Dataset that loads Pose Estimation dataset.
+    """
+
+    episodes: List[PoseEstimationEpisode]
+    content_scenes_path: str = "{data_path}/content/{scene}.json.gz"
+    shuffle_dataset: bool = True
+
+    @staticmethod
+    def check_config_paths_exist(config: Config) -> bool:
+        return os.path.exists(
+            config.DATA_PATH.format(split=config.SPLIT)
+        ) and os.path.exists(config.SCENES_DIR)
+
+    @staticmethod
+    def get_scenes_to_load(config: Config) -> List[str]:
+        r"""Return list of scene ids for which dataset has separate files with
+        episodes.
+        """
+        assert PoseEstimationDatasetV1.check_config_paths_exist(config)
+        dataset_dir = os.path.dirname(
+            config.DATA_PATH.format(split=config.SPLIT)
+        )
+
+        cfg = config.clone()
+        cfg.defrost()
+        cfg.CONTENT_SCENES = []
+        dataset = PoseEstimationDatasetV1(cfg)
+        return PoseEstimationDatasetV1._get_scenes_from_folder(
+            content_scenes_path=dataset.content_scenes_path,
+            dataset_dir=dataset_dir,
+        )
+
+    @staticmethod
+    def _get_scenes_from_folder(content_scenes_path, dataset_dir):
+        scenes = []
+        content_dir = content_scenes_path.split("{scene}")[0]
+        scene_dataset_ext = content_scenes_path.split("{scene}")[1]
+        content_dir = content_dir.format(data_path=dataset_dir)
+        if not os.path.exists(content_dir):
+            return scenes
+
+        for filename in os.listdir(content_dir):
+            if filename.endswith(scene_dataset_ext):
+                scene = filename[: -len(scene_dataset_ext)]
+                scenes.append(scene)
+        scenes.sort()
+        return scenes
+
+    def __init__(self, config: Optional[Config] = None) -> None:
+        self.episodes = []
+
+        if config is None:
+            return
+
+        self.shuffle_dataset = config.SHUFFLE_DATASET
+
+        datasetfile_path = config.DATA_PATH.format(split=config.SPLIT)
+        with gzip.open(datasetfile_path, "rt") as f:
+            self.from_json(f.read(), scenes_dir=config.SCENES_DIR)
+
+        # Read separate file for each scene
+        dataset_dir = os.path.dirname(datasetfile_path)
+        scenes = config.CONTENT_SCENES
+        if ALL_SCENES_MASK in scenes:
+            scenes = PoseEstimationDatasetV1._get_scenes_from_folder(
+                content_scenes_path=self.content_scenes_path,
+                dataset_dir=dataset_dir,
+            )
+
+        for scene in scenes:
+            scene_filename = self.content_scenes_path.format(
+                data_path=dataset_dir, scene=scene
+            )
+            with gzip.open(scene_filename, "rt") as f:
+                self.from_json(f.read(), scenes_dir=config.SCENES_DIR)
+
+        self.sample_episodes(config.NUM_EPISODE_SAMPLE)
+
+    def get_episode_iterator(self):
+        r"""
+        Creates and returns an iterator that iterates through self.episodes
+        in the desirable way specified.
+        Returns:
+            iterator for episodes
+        """
+        # TODO: support shuffling between epoch and scene switching
+        if self.shuffle_dataset:
+            self.shuffle_episodes(50)
+        return cycle(self.episodes)
+
+    def from_json(
+        self, json_str: str, scenes_dir: Optional[str] = None
+    ) -> None:
+        deserialized = json.loads(json_str)
+        if CONTENT_SCENES_PATH_FIELD in deserialized:
+            self.content_scenes_path = deserialized[CONTENT_SCENES_PATH_FIELD]
+
+        for episode in deserialized["episodes"]:
+            episode = PoseEstimationEpisode(**episode)
+
+            if scenes_dir is not None:
+                if episode.scene_id.startswith(DEFAULT_SCENE_PATH_PREFIX):
+                    episode.scene_id = episode.scene_id[
+                        len(DEFAULT_SCENE_PATH_PREFIX) :
+                    ]
+
+                episode.scene_id = os.path.join(scenes_dir, episode.scene_id)
+
+            # Ignore edge case episodes where no references were sampled
+            if len(episode.pose_ref_positions) == 0:
+                continue
+
+            self.episodes.append(episode)
diff --git a/habitat/sims/habitat_simulator/action_spaces.py b/habitat/sims/habitat_simulator/action_spaces.py
index 4cee1cb..7b72473 100644
--- a/habitat/sims/habitat_simulator/action_spaces.py
+++ b/habitat/sims/habitat_simulator/action_spaces.py
@@ -53,3 +53,27 @@ class HabitatSimV1ActionSpaceConfiguration(
         config.update(new_config)
 
         return config
+
+
+@registry.register_action_space_configuration(name="v2")
+class HabitatSimV2ActionSpaceConfiguration(ActionSpaceConfiguration):
+    """
+    Action space designed for exploration. Does not require a STOP action.
+    """
+    def get(self):
+        return {
+            SimulatorActions.MOVE_FORWARD: habitat_sim.ActionSpec(
+                "move_forward",
+                habitat_sim.ActuationSpec(
+                    amount=self.config.FORWARD_STEP_SIZE
+                ),
+            ),
+            SimulatorActions.TURN_LEFT: habitat_sim.ActionSpec(
+                "turn_left",
+                habitat_sim.ActuationSpec(amount=self.config.TURN_ANGLE),
+            ),
+            SimulatorActions.TURN_RIGHT: habitat_sim.ActionSpec(
+                "turn_right",
+                habitat_sim.ActuationSpec(amount=self.config.TURN_ANGLE),
+            ),
+        }
diff --git a/habitat/sims/habitat_simulator/habitat_simulator.py b/habitat/sims/habitat_simulator/habitat_simulator.py
index d77e13f..a8cae98 100644
--- a/habitat/sims/habitat_simulator/habitat_simulator.py
+++ b/habitat/sims/habitat_simulator/habitat_simulator.py
@@ -7,6 +7,12 @@
 from enum import Enum
 from typing import Any, List, Optional
 
+import os
+import cv2
+import gzip
+import json
+import math
+import quaternion
 import numpy as np
 from gym import Space, spaces
 
@@ -20,12 +26,21 @@ from habitat.core.simulator import (
     Observations,
     RGBSensor,
     SemanticSensor,
+    FineOccSensor,
+    CoarseOccSensor,
+    HighResCoarseOccSensor,
+    ProjOccSensor,
     Sensor,
     SensorSuite,
     ShortestPathPoint,
     Simulator,
     SimulatorActions,
 )
+from habitat.tasks.utils import (
+    cartesian_to_polar,
+    quaternion_rotate_vector
+)
+from habitat.utils.visualizations import maps
 
 RGBSENSOR_DIMENSION = 3
 
@@ -130,6 +145,106 @@ class HabitatSimSemanticSensor(SemanticSensor):
         return obs
 
 
+@registry.register_sensor
+class HabitatSimFineOccSensor(FineOccSensor):
+    sim_sensor_type: habitat_sim.SensorType
+
+    def __init__(self, config):
+        self.sim_sensor_type = habitat_sim.SensorType.COLOR
+        super().__init__(config=config)
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0,
+            high=255,
+            shape=(self.config.HEIGHT, self.config.WIDTH, RGBSENSOR_DIMENSION),
+            dtype=np.uint8,
+        )
+
+    def get_observation(self, sim_obs):
+        obs = sim_obs.get(self.uuid, None)
+        check_sim_obs(obs, self)
+
+        # remove alpha channel
+        obs = obs[:, :, :RGBSENSOR_DIMENSION]
+        return obs
+
+
+@registry.register_sensor
+class HabitatSimCoarseOccSensor(CoarseOccSensor):
+    sim_sensor_type: habitat_sim.SensorType
+
+    def __init__(self, config):
+        self.sim_sensor_type = habitat_sim.SensorType.COLOR
+        super().__init__(config=config)
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0,
+            high=255,
+            shape=(self.config.HEIGHT, self.config.WIDTH, RGBSENSOR_DIMENSION),
+            dtype=np.uint8,
+        )
+
+    def get_observation(self, sim_obs):
+        obs = sim_obs.get(self.uuid, None)
+        check_sim_obs(obs, self)
+
+        # remove alpha channel
+        obs = obs[:, :, :RGBSENSOR_DIMENSION]
+        return obs
+
+
+@registry.register_sensor
+class HabitatSimHighResCoarseOccSensor(HighResCoarseOccSensor):
+    sim_sensor_type: habitat_sim.SensorType
+
+    def __init__(self, config):
+        self.sim_sensor_type = habitat_sim.SensorType.COLOR
+        super().__init__(config=config)
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0,
+            high=255,
+            shape=(self.config.HEIGHT, self.config.WIDTH, RGBSENSOR_DIMENSION),
+            dtype=np.uint8,
+        )
+
+    def get_observation(self, sim_obs):
+        obs = sim_obs.get(self.uuid, None)
+        check_sim_obs(obs, self)
+
+        # remove alpha channel
+        obs = obs[:, :, :RGBSENSOR_DIMENSION]
+        return obs
+
+
+@registry.register_sensor
+class HabitatSimProjOccSensor(ProjOccSensor):
+    sim_sensor_type: habitat_sim.SensorType
+
+    def __init__(self, config):
+        self.sim_sensor_type = habitat_sim.SensorType.COLOR
+        super().__init__(config=config)
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0,
+            high=255,
+            shape=(self.config.HEIGHT, self.config.WIDTH, RGBSENSOR_DIMENSION),
+            dtype=np.uint8,
+        )
+
+    def get_observation(self, sim_obs):
+        obs = sim_obs.get(self.uuid, None)
+        check_sim_obs(obs, self)
+
+        # remove alpha channel
+        obs = obs[:, :, :RGBSENSOR_DIMENSION]
+        return obs
+
+
 @registry.register_simulator(name="Sim-v0")
 class HabitatSim(Simulator):
     r"""Simulator wrapper over habitat-sim
@@ -445,6 +560,34 @@ class HabitatSim(Simulator):
         else:
             return None
 
+    def get_specific_sensor_observations_at(
+        self,
+        position: List[float],
+        rotation: List[float],
+        sensor_uuid: str,
+    ) -> Optional[Observations]:
+
+        current_state = self.get_agent_state()
+
+        success = self.set_agent_state(
+            position,
+            rotation,
+            reset_sensors=False
+        )
+        if success:
+            specific_sim_obs = self._sim.get_specific_sensor_observations(
+                                   sensor_uuid
+                               )
+
+            self.set_agent_state(
+                current_state.position,
+                current_state.rotation,
+                reset_sensors=False,
+            )
+            return specific_sim_obs
+        else:
+            return None
+
     # TODO (maksymets): Remove check after simulator became stable
     def _check_agent_position(self, position, agent_id=0) -> bool:
         if not np.allclose(position, self.get_agent_state(agent_id).position):
@@ -474,3 +617,735 @@ class HabitatSim(Simulator):
             result in an action (step) being taken.
         """
         return self._prev_sim_obs.get("collided", False)
+
+    def get_environment_extents(self):
+        """
+        Returns the extents of the environments on the current floor.
+        """
+        num_samples = 20000
+        start_height = self.get_agent_state().position[1]
+
+        min_x, max_x = (math.inf, -math.inf)
+        min_z, max_z = (math.inf, -math.inf)
+        for _ in range(num_samples):
+            point = self.sample_navigable_point()
+            # Check if on same level as original
+            if np.abs(start_height - point[1]) > 0.5:
+                continue
+            min_x = min(point[0], min_x)
+            max_x = max(point[0], max_x)
+            min_z = min(point[2], min_z)
+            max_z = max(point[2], max_z)
+
+        return (min_x, min_z, max_x, max_z)
+
+@registry.register_simulator(name="Sim-v1")
+class HabitatSimOcc(HabitatSim):
+    r"""Simulator wrapper over HabitatSim that builds an occupancy 
+    map of the environment as the agent is moving.
+
+    Args:
+        config: configuration for initializing the simulator.
+        
+    Acknowledgement: large parts of the occupancy generation code were 
+    borrowed from 
+        https://github.com/taochenshh/exp4nav 
+    Modifications were made for faster generation.
+    """
+
+    def __init__(self, config: Config) -> None:
+        super().__init__(config)
+        self.initialize_map(config)
+
+    def initialize_map(self, config):
+        occ_cfg = config.OCCUPANCY_MAPS
+        occ_info = {}
+        occ_info['map_scale'] = occ_cfg.MAP_SCALE
+        occ_info['map_size'] = occ_cfg.MAP_SIZE
+        occ_info['max_depth'] = occ_cfg.MAX_DEPTH
+        occ_info['small_map_range'] = occ_cfg.SMALL_MAP_RANGE
+        occ_info['large_map_range'] = occ_cfg.LARGE_MAP_RANGE
+        occ_info['small_map_size'] = config.FINE_OCC_SENSOR.WIDTH
+        occ_info['large_map_size'] = config.COARSE_OCC_SENSOR.WIDTH
+        occ_info['height_thresh'] = (occ_cfg.HEIGHT_LOWER, 
+                                     occ_cfg.HEIGHT_UPPER)
+        occ_info['get_proj_loc_map'] = occ_cfg.GET_PROJ_LOC_MAP
+        occ_info['get_highres_loc_map'] = occ_cfg.GET_HIGHRES_LOC_MAP
+        if occ_info['get_highres_loc_map']:
+            occ_info['highres_large_map_size'] = \
+                    config.HIGHRES_COARSE_OCC_SENSOR.WIDTH
+        # NOTE: this assumes that the system is a single agent system
+        occ_info['agent_height'] = config.AGENT_0.HEIGHT
+        occ_info['Lx_min'] = None
+        occ_info['Lx_max'] = None
+        occ_info['Lz_min'] = None
+        occ_info['Lz_max'] = None
+        # NOTE: this assumes that hfov, vfov are identical
+        depth_sensor = self._sensor_suite.sensors['depth']
+        hfov = math.radians(depth_sensor.config.HFOV)
+        intrinsic_matrix = np.array([[1./np.tan(hfov/2.), 0., 0., 0.],
+                                     [0., 1./np.tan(hfov/2.), 0., 0.],
+                                     [0., 0.,  1, 0],
+                                     [0., 0., 0, 1]])
+        occ_info['intrinsic_matrix'] = intrinsic_matrix
+        occ_info['inverse_intrinsic_matrix'] = np.linalg.inv(intrinsic_matrix)
+        occ_info['use_gt_occ_map'] = occ_cfg.USE_GT_OCC_MAP
+        self.occupancy_info = occ_info
+
+        # Object annotations
+        self.has_object_annotations = config.OBJECT_ANNOTATIONS.IS_AVAILABLE
+        self.object_annotations_dir = config.OBJECT_ANNOTATIONS.PATH
+
+        # Memory to be allocated at the start of an episode
+        self.grids_mat = None
+        self.gt_grids_mat = None
+        self.proj_grids_mat = None
+
+        # GT topdown map
+        self._gt_top_down_map = None
+
+        # Cache for efficiency
+        self._cache = {}
+        # Cache meshgrid for depth projection
+        # [1, -1] for y as array indexing is y-down while world is y-up
+        W = config.DEPTH_SENSOR.WIDTH
+        self._cache['xs'], self._cache['ys'] = np.meshgrid(
+                                                  np.linspace(-1,1,W),
+                                                  np.linspace(1,-1,W)
+                                               )
+
+    def create_grid_memory(self):
+        # Pre-assign memory to grids_mat
+        grid_size = self.occupancy_info['map_scale']
+        min_x, min_z, max_x, max_z = self.get_environment_extents()
+
+        prev_Lx_min = self.occupancy_info['Lx_min']
+        prev_Lx_max = self.occupancy_info['Lx_max']
+        prev_Lz_min = self.occupancy_info['Lz_min']
+        prev_Lz_max = self.occupancy_info['Lz_max']
+
+        curr_Lx_min = min_x - 5 # Add a 5 meter buffer
+        curr_Lx_max = max_x + 5 # Add a 5 meter buffer
+        curr_Lz_min = min_z - 5 # Add a 5 meter buffer
+        curr_Lz_max = max_z + 5 # Add a 5 meter buffer
+
+        self.occupancy_info['Lx_min'] = curr_Lx_min
+        self.occupancy_info['Lx_max'] = curr_Lx_max
+        self.occupancy_info['Lz_min'] = curr_Lz_min
+        self.occupancy_info['Lz_max'] = curr_Lz_max
+
+        grid_num = (int((curr_Lx_max - curr_Lx_min)/grid_size),
+                    int((curr_Lz_max - curr_Lz_min)/grid_size))
+
+        is_same_environment = (prev_Lx_min == curr_Lx_min) and \
+                              (prev_Lx_max == curr_Lx_max) and \
+                              (prev_Lz_min == curr_Lz_min) and \
+                              (prev_Lz_max == curr_Lz_max)
+
+        if is_same_environment:
+            self.grids_mat.fill(0)
+            if self.occupancy_info['use_gt_occ_map']:
+                self.gt_grids_mat.fill(0)
+            if self.occupancy_info['get_proj_loc_map']:
+                self.proj_grids_mat.fill(0)
+        else:
+            self.grids_mat = np.zeros(grid_num, dtype=np.uint8)
+            if self.occupancy_info['use_gt_occ_map']:
+                self.gt_grids_mat = np.zeros(grid_num, dtype=np.uint8)
+            """
+            The local projection has 3 channels:
+                Channel 1 - active if occupied
+                Channel 2 - active if free space
+                Channel 3 - active if unknown
+            """
+            if self.occupancy_info['get_proj_loc_map']:
+                # Only contains local projection
+                self.proj_grids_mat = np.zeros((*grid_num, 3), dtype=np.uint8)
+
+    def reset(self):
+        # Generate simulator observations
+        sim_obs = self._sim.reset()
+        if self._update_agents_state():
+            sim_obs = self._sim.get_sensor_observations()
+
+        # Generate the occupancy maps and add to this list
+        self.create_occupancy_grid()
+        # Obtain GT map
+        self.gt_map_creation_height = self.get_agent_state().position[1]
+        if self.occupancy_info['use_gt_occ_map']:
+            self._gt_top_down_map = self.get_original_map()
+
+        sensors = self._sensor_suite.sensors
+        proc_rgb = sensors['rgb'].get_observation(sim_obs)
+        proc_depth = sensors['depth'].get_observation(sim_obs)
+        seen_area = self.get_seen_area(
+            proc_rgb,
+            proc_depth,
+            self.grids_mat,
+            self.gt_grids_mat,
+            self.proj_grids_mat,
+            False
+        )
+        inc_area = seen_area - self.occupancy_info['seen_area']
+        (
+            fine_occupancy,
+            coarse_occupancy,
+            highres_coarse_occupancy
+        ) = self.get_loc_map()
+
+        self.occupancy_info['seen_area'] = seen_area
+        self.occupancy_info['inc_area'] = inc_area
+        sim_obs['coarse_occupancy'] = coarse_occupancy
+        sim_obs['fine_occupancy'] = fine_occupancy
+        if self.occupancy_info['get_highres_loc_map']:
+            sim_obs['highres_coarse_occupancy'] = highres_coarse_occupancy
+        if self.occupancy_info['get_proj_loc_map']:
+            proj_occupancy = self.get_proj_loc_map()
+            sim_obs['proj_occupancy'] = proj_occupancy
+
+        self._prev_sim_obs = sim_obs
+        self._is_episode_active = True
+
+        # Load object annotations if available
+        if self.has_object_annotations:
+            scene_id = self._current_scene.split('/')[-1]
+            scene_path = os.path.join(self.object_annotations_dir, scene_id)
+            annot_path = scene_path + '.json.gz'
+            with gzip.open(annot_path, 'rt') as fp:
+                self.object_annotations = json.load(fp)
+
+        return self._sensor_suite.get_observations(sim_obs)
+
+    def step(self, action):
+        assert self._is_episode_active, (
+            "episode is not active, environment not RESET or "
+            "STOP action called previously"
+        )
+
+        if action == self.index_stop_action:
+            self._is_episode_active = False
+            sim_obs = self._sim.get_sensor_observations()
+        else:
+            sim_obs = self._sim.step(action)
+
+        sensors = self._sensor_suite.sensors
+        proc_rgb = sensors['rgb'].get_observation(sim_obs)
+        proc_depth = sensors['depth'].get_observation(sim_obs)
+
+        if self.occupancy_info['use_gt_occ_map']:
+            agent_height = self.get_agent_state().position[1]
+            # If the agent went to a new floor, update the GT map
+            if abs(agent_height - self.gt_map_creation_height) >= 0.5:
+                self._gt_top_down_map = self.get_original_map()
+                self.gt_map_creation_height = agent_height
+
+        seen_area = self.get_seen_area(
+            proc_rgb,
+            proc_depth,
+            self.grids_mat,
+            self.gt_grids_mat,
+            self.proj_grids_mat,
+            sim_obs.get("collided", False)
+        )
+
+        inc_area = seen_area - self.occupancy_info['seen_area']
+        (
+            fine_occupancy,
+            coarse_occupancy,
+            highres_coarse_occupancy
+        ) = self.get_loc_map()
+
+        self.occupancy_info['seen_area'] = seen_area
+        self.occupancy_info['inc_area'] = inc_area
+        sim_obs['coarse_occupancy'] = coarse_occupancy
+        sim_obs['fine_occupancy'] = fine_occupancy
+        if self.occupancy_info['get_highres_loc_map']:
+            sim_obs['highres_coarse_occupancy'] = highres_coarse_occupancy
+        if self.occupancy_info['get_proj_loc_map']:
+            proj_occupancy = self.get_proj_loc_map()
+            sim_obs['proj_occupancy'] = proj_occupancy
+        self._prev_sim_obs = sim_obs
+
+        observations = self._sensor_suite.get_observations(sim_obs)
+
+        return observations
+
+    def convert_to_pointcloud(self, rgb, depth, agent_state):
+        """
+        Inputs:
+            depth - (H, W, 1) numpy array
+
+        Returns:
+            xyz_world - (N, 3) numpy array for (X, Y, Z) 
+                        in real world coordinates
+
+        NOTE: Passing agent_state for efficiency
+        """
+        depth_sensor = self._sensor_suite.sensors['depth']
+
+        depth_float = depth.astype(np.float32)[..., 0]
+        if depth_sensor.config.NORMALIZE_DEPTH:
+            min_depth = depth_sensor.config.MIN_DEPTH
+            max_depth = depth_sensor.config.MAX_DEPTH
+            depth_float = depth_float * max_depth + min_depth
+
+        # =========== Convert to camera coordinates ============
+        W = depth.shape[1]
+        xs = np.copy(self._cache['xs']).reshape(-1)
+        ys = np.copy(self._cache['ys']).reshape(-1)
+        depth_float = depth_float.reshape(-1)
+        # Filter out invalid depths
+        valid_depths = (depth_float != 0.0) & 
+                       (depth_float <= self.occupancy_info['max_depth'])
+        xs = xs[valid_depths] 
+        ys = ys[valid_depths]
+        depth_float = depth_float[valid_depths]
+        # Unproject
+        # negate depth as the camera looks along -Z
+        xys = np.vstack((xs * depth_float, 
+                         ys * depth_float, 
+                         -depth_float, np.ones(depth_float.shape)))
+        inv_K = self.occupancy_info['inverse_intrinsic_matrix']
+        xyz_cam = np.matmul(inv_K, xys) # XYZ in the camera coordinate system
+
+        # =========== Convert to world coordinates ============
+        agent_position = agent_state.position
+        agent_rotation = agent_state.rotation
+        agent_rotation = quaternion.as_rotation_matrix(agent_rotation)
+        T_world = np.eye(4)
+        T_world[:3, :3] = agent_rotation
+        T_world[:3, 3] = agent_position
+        xyz_world = np.matmul(T_world, xyz_cam).T
+        # Convert to non-homogeneous coordinates
+        xyz_world = xyz_world[:, :3] / xyz_world[:, 3][:, np.newaxis]
+
+        return xyz_world
+
+    def reconfigure(self, config: Config) -> None:
+        super().reconfigure(config)
+        self.initialize_map(config)
+
+    def get_observations_at(
+        self,
+        position: List[float],
+        rotation: List[float],
+        keep_agent_at_new_pose: bool = False,
+    ) -> Optional[Observations]:
+
+        current_state = self.get_agent_state()
+        success = self.set_agent_state(position, rotation, reset_sensors=False)
+        if success:
+            # Get simulator observations
+            sim_obs = self._sim.get_sensor_observations()
+            self._prev_sim_obs = sim_obs
+            if keep_agent_at_new_pose:
+                sensors = self._sensor_suite.sensors
+                proc_rgb = sensors['rgb'].get_observation(sim_obs)
+                proc_depth = sensors['depth'].get_observation(sim_obs)
+                seen_area = self.get_seen_area(
+                    proc_rgb,
+                    proc_depth,
+                    self.grids_mat,
+                    self.gt_grids_mat,
+                    self.proj_grids_mat,
+                    False
+                )
+                inc_area = seen_area - self.occupancy_info['seen_area']
+                self.occupancy_info['seen_area'] = seen_area
+                self.occupancy_info['inc_area'] = inc_area
+
+            (
+                fine_occupancy,
+                coarse_occupancy,
+                highres_coarse_occupancy
+            ) = self.get_loc_map()
+
+            sim_obs['coarse_occupancy'] = coarse_occupancy
+            sim_obs['fine_occupancy'] = fine_occupancy
+           sim_obs['highres_coarse_occupancy'] = highres_coarse_occupancy
+            if self.occupancy_info['get_proj_loc_map']:
+                proj_occupancy = self.get_proj_loc_map()
+                sim_obs['proj_occupancy'] = proj_occupancy
+
+            observations = self._sensor_suite.get_observations(sim_obs)
+            if not keep_agent_at_new_pose:
+                self.set_agent_state(
+                    current_state.position,
+                    current_state.rotation,
+                    reset_sensors=False,
+                )
+            return observations
+        else:
+            return None
+
+    def get_original_map(self):
+        x_min = self.occupancy_info['Lx_min']
+        x_max = self.occupancy_info['Lx_max']
+        z_min = self.occupancy_info['Lz_min']
+        z_max = self.occupancy_info['Lz_max']
+
+        top_down_map = maps.get_topdown_map_v2(
+            self,
+            (x_min, x_max, z_min, z_max),
+            self.occupancy_info['map_scale'],
+            20000,
+        )
+
+        return top_down_map
+
+    # Note: get_observations_at(), get_specific_observations_at() do not 
+    # update the occupancy maps unless the agent is placed at the new location.
+    def create_occupancy_grid(self):
+        """
+        Based on the size of the environment and occupancy map parameters,
+        create an empty occupancy grid.
+        """
+        self.create_grid_memory()
+        self.occupancy_info['seen_area'] = 0
+        self.occupancy_info['inc_area'] = 0
+        self.grids_mat.fill(0)
+        if self.occupancy_info['get_proj_loc_map']:
+            self.proj_grids_mat.fill(0)
+
+    def _convert_world2map(self, points):
+        """
+        Convert points from world coordinates to map coordinates
+        points - (N, 3) --- x, y, z world coordinates
+        """
+        Lx_min = self.occupancy_info['Lx_min']
+        Lz_min = self.occupancy_info['Lz_min']
+        grid_size = self.occupancy_info['map_scale']
+        world_xy = points[:, [0, 2]]
+        map_xy = (world_xy - np.array([[Lx_min, Lz_min]])) / grid_size
+        map_xy = np.floor(map_xy).astype(int)
+        returns map_xy
+
+    def _crop_and_resize(image, crop_exts, shape, interp):
+        """
+        Crop image with crop_exts as the extents and rescale to shape.
+        crop_exts - (start_x, end_x, start_y, end_y) 
+        shape - (width, height)
+        """
+        start_x, end_x, start_y, end_y = crop_exts
+        cropped_image = image[start_y:end_y+1, start_x:end_x+1]
+        resized_image = cv2.resize(cropped_image, shape, interpolation=interp)
+        return resized_image
+
+    def get_seen_area(
+        self, rgb, depth, out_mat, gt_out_mat, proj_out_mat, collided
+    ):
+        """
+        Updates the occupancy map and computes total area seen after update.
+        """
+
+        agent_state = self.get_agent_state()
+        # X is right, Y is upward, Z is backward
+        XYZ_ego = self.convert_to_pointcloud(rgb, depth, agent_state)
+
+        # Normalizing the point cloud so that ground plane is Y=0
+        current_agent_y = agent_state.position[1]
+        ground_plane_y = current_agent_y - self.occupancy_info['agent_height']
+        XYZ_ego[:, 1] -= ground_plane_y
+
+        # Convert to grid coordinates
+        points = XYZ_ego
+        grid_locs = self._convert_world2map(points)
+
+        # Generate top-down map
+        grids_mat = np.zeros(self.grids_mat.shape, dtype=np.uint8)
+        height_thresh = self.occupancy_info['height_thresh']
+        high_filter_idx = points[:, 1] < height_thresh[1]
+        low_filter_idx = points[:, 1] > height_thresh[0]
+        
+        # (1) Assign explored space
+        obstacle_idx = np.logical_and(low_filter_idx, high_filter_idx)
+
+        self.safe_assign(grids_mat, grid_locs[high_filter_idx, 0], 
+                         grid_locs[high_filter_idx, 1], 2)
+
+        kernel = np.ones((3, 3), np.uint8)
+        grids_mat = cv2.morphologyEx(grids_mat, cv2.MORPH_CLOSE, kernel)
+
+        # (2) Assign obstacles
+        obs_mat = np.zeros(self.grids_mat.shape, dtype=np.uint8)
+        self.safe_assign(obs_mat, grid_locs[obstacle_idx, 0],
+                         grid_locs[obstacle_idx, 1], 1)
+
+        kernel = np.ones((3, 3), np.uint8)
+        obs_mat = cv2.morphologyEx(obs_mat, cv2.MORPH_CLOSE, kernel)
+        np.putmask(grids_mat, obs_mat == 1, 1)
+
+        # The entire global map aggregated over time
+        visible_mask = grids_mat == 2
+        occupied_mask = grids_mat == 1
+
+        np.putmask(out_mat, visible_mask, 2)
+        np.putmask(out_mat, occupied_mask, 1)
+
+        # The above map is subject to noisy odometer readings. 
+        # This is a clean map without noisy registration.
+        if self.occupancy_info['use_gt_occ_map']:
+            gt_visible_mask = visible_mask | occupied_mask
+            # Dilate the visible mask
+            dkernel = np.ones((9, 9), np.uint8)
+            gt_visible_mask = cv2.dilate(
+                gt_visible_mask.astype(np.uint8),
+                dkernel,
+                iterations=2
+            )
+            gt_visible_mask = gt_visible_mask != 0
+            gt_occupied_mask = gt_visible_mask & (self._gt_top_down_map == 0)
+
+            np.putmask(gt_out_mat, gt_visible_mask, 2)
+            np.putmask(gt_out_mat, gt_occupied_mask, 1)
+
+        seen_area = np.count_nonzero(out_mat > 0)
+
+        # The local egocentric projection for this observation
+        # Empty the matrix
+        if self.occupancy_info['get_proj_loc_map']:
+            proj_out_mat.fill(0)
+            # Set everything to unknown initially
+            proj_out_mat[..., 2] = 1
+            # Set obstacles
+            np.putmask(proj_out_mat[..., 0], grids_mat == 1, 1)
+            np.putmask(proj_out_mat[..., 2], grids_mat == 1, 0)
+            # Set free space
+            free_space_mask = (grids_mat != 1) & (grids_mat == 2)
+            np.putmask(proj_out_mat[..., 1], free_space_mask, 1)
+            np.putmask(proj_out_mat[..., 2], free_space_mask, 0)
+
+        return seen_area
+
+    def safe_assign(self, im_map, x_idx, y_idx, value):
+        try:
+            im_map[x_idx, y_idx] = value
+        except IndexError:
+            valid_idx1 = np.logical_and(x_idx >= 0, x_idx < im_map.shape[0])
+            valid_idx2 = np.logical_and(y_idx >= 0, y_idx < im_map.shape[1])
+            valid_idx = np.logical_and(valid_idx1, valid_idx2)
+            im_map[x_idx[valid_idx], y_idx[valid_idx]] = value
+
+    def get_camera_grid_pos(self):
+        """
+        Returns the agent's current position in both the
+        real world system (X, Z, phi from -Z to X) and 
+        the grid world system (Xg, Zg)
+        """
+        agent_state = self.get_agent_state()
+        position = agent_state.position
+        rotation = agent_state.rotation
+        X, Z = position[0], position[2]
+        # Grid world positions
+        grid_size = self.occupancy_info['map_scale']
+        Lx_min = self.occupancy_info['Lx_min']
+        Lz_min = self.occupancy_info['Lz_min']
+        Xg = (X - Lx_min) / grid_size
+        Zg = (Z - Lz_min) / grid_size
+        # Real world rotation
+        direction_vector = np.array([0, 0, -1])
+        heading_vector = quaternion_rotate_vector(
+            rotation.inverse(), direction_vector
+        )
+
+        phi = cartesian_to_polar(-heading_vector[2], heading_vector[0])[1]
+        phi = -phi # (rotation from -Z to X)
+        return (X, Z, phi), (Xg, Zg)
+
+    def get_loc_map(self):
+        """
+        Converts the globally registered occupancy map to an RGB version 
+        in the egocentric coordinates.
+        """
+
+        large_map_range = self.occupancy_info['large_map_range']
+        small_map_range = self.occupancy_info['small_map_range']
+        large_map_size = self.occupancy_info['large_map_size']
+        small_map_size = self.occupancy_info['small_map_size']
+
+        if self.occupancy_info['use_gt_occ_map']:
+            top_down_map = self.gt_grids_mat.copy() # (map_size, map_size)
+        else:
+            top_down_map = self.grids_mat.copy() # (map_size, map_size)
+
+        world_xyt, map_xy = self.get_camera_grid_pos()
+        map_xy = (int(map_xy[0]), int(map_xy[1]))
+        
+        # Crop out a sufficiently large region centered around the agent
+        min_range = int(1.5 * large_map_range)
+        x_start = max(0, map_xy[0] - min_range)
+        x_end = min(top_down_map.shape[0], map_xy[0] + min_range)
+        y_start = max(0, map_xy[1] - min_range)
+        y_end = min(top_down_map.shape[1], map_xy[1] + min_range)
+        ego_map = top_down_map[x_start: x_end, y_start: y_end]
+
+        # Add padding to handle out-of-bounds errors
+        top_pad = max(min_range - map_xy[0], 0)
+        left_pad = max(min_range - map_xy[1], 0)
+        bottom_pad = max(min_range - top_down_map.shape[0] + map_xy[0] + 1, 0)
+        right_pad = max(min_range - top_down_map.shape[1] + map_xy[1] + 1, 0)
+
+        ego_map = np.pad(
+            ego_map, 
+            ((top_pad, bottom_pad), (left_pad, right_pad)),
+            'constant',
+            constant_values=0,
+        )
+
+        """
+        NOTE - the map is currently addressed as follows: 
+        rows are -X to X top to bottom, cols are -Z to Z left to right
+        To get -Z top and X right, we need transpose the map.
+        """
+        ego_map = ego_map.transpose(1, 0)
+
+        # Rotate the map to egocentric coordinates
+        half_size = ego_map.shape[0] // 2
+        center = (half_size, half_size)
+        rot_angle = math.degrees(world_xyt[2])
+        # Anti-clockwise rotation about the center
+        M = cv2.getRotationMatrix2D(center, rot_angle, 1.0)
+
+        ego_map = cv2.warpAffine(
+            ego_map, 
+            M,
+            (ego_map.shape[1], ego_map.shape[0]),
+            flags=cv2.INTER_NEAREST,
+            borderMode=cv2.BORDER_CONSTANT,
+            borderValue=(255,),
+        )
+
+        # Obtain the fine occupancy map
+        start = int(half_size - small_map_range)
+        end = int(half_size + small_map_range)
+        assert start >= 0
+        assert end <= ego_map.shape[0]
+        small_ego_map = self._crop_and_resize(
+            ego_map, 
+            (start, end, start, end), 
+            (small_map_size, small_map_size), 
+            cv2.INTER_NEAREST
+        )
+        small_ego_map = np.clip(small_ego_map, 0, 2)
+
+        # Obtain the coarse occupancy map
+        start = int(half_size - large_map_range)
+        end = int(half_size + large_map_range)
+        assert start >= 0
+        assert end <= ego_map.shape[0]
+        large_ego_map = self._crop_and_resize(
+            ego_map,
+            (start, end, start, end),
+            (large_map_size, large_map_size),
+            cv2.INTER_NEAREST,
+        )
+        large_ego_map = np.clip(large_ego_map, 0, 2)
+
+        # Obtain the high resolution coarse occupancy map if needed
+        if self.occupancy_info['get_highres_loc_map']:
+            highres_large_map_size = \
+                    self.occupancy_info['highres_large_map_size']
+            highres_large_ego_map = self._crop_and_resize(
+                ego_map,
+                (start, end, start, end),
+                (highres_large_map_size, highres_large_map_size),
+                cv2.INTER_NEAREST,
+            )
+            highres_large_ego_map = np.clip(highres_large_ego_map, 0, 2)
+
+        # Assign colors to the coarse and fine occupancy maps
+        small_ego_map_color = np.zeros((*small_ego_map.shape, 3), np.uint8)
+        small_ego_map_color[small_ego_map == 0] = np.array([255, 255, 255])
+        small_ego_map_color[small_ego_map == 1] = np.array([0, 0, 255])
+        small_ego_map_color[small_ego_map == 2] = np.array([0, 255, 0])
+
+        large_ego_map_color = np.zeros((*large_ego_map.shape, 3), np.uint8)
+        large_ego_map_color[large_ego_map == 0] = np.array([255, 255, 255])
+        large_ego_map_color[large_ego_map == 1] = np.array([0, 0, 255])
+        large_ego_map_color[large_ego_map == 2] = np.array([0, 255, 0])
+
+        if self.occupancy_info['get_highres_loc_map']:
+            highres_large_ego_map_color = \
+                    np.zeros((*highres_large_ego_map.shape, 3), dtype=np.uint8)
+            highres_large_ego_map_color[highres_large_ego_map == 0] = \
+                    np.array([255, 255, 255])
+            highres_large_ego_map_color[highres_large_ego_map == 1] = \
+                    np.array([0, 0, 255])
+            highres_large_ego_map_color[highres_large_ego_map == 2] = \
+                    np.array([0, 255, 0])
+        else:
+            highres_large_ego_map_color = None
+
+        return small_ego_map_color, large_ego_map_color, highres_large_ego_map_color
+
+    def get_proj_loc_map(self):
+        """
+        Converts the globally registered occupancy map to an RGB version 
+        in the egocentric coordinates.
+        """
+        proj_map_range = self.occupancy_info['small_map_range']
+        proj_map_size = self.occupancy_info['small_map_size']
+
+        top_down_map = self.proj_grids_mat.copy() # (map_size, map_size)
+
+        world_xyt, map_xy = self.get_camera_grid_pos()
+        map_xy = (int(map_xy[0]), int(map_xy[1]))
+
+        # Crop out a sufficiently large region centered around the agent
+        min_range = int(1.5 * proj_map_range)
+        x_start = max(0, map_xy[0] - min_range)
+        x_end = min(top_down_map.shape[0], map_xy[0] + min_range)
+        y_start = max(0, map_xy[1] - min_range)
+        y_end = min(top_down_map.shape[1], map_xy[1] + min_range)
+        ego_map = top_down_map[x_start: x_end, y_start: y_end]
+
+        # Add padding to handle out-of-bounds errors
+        top_pad = max(min_range - map_xy[0], 0)
+        left_pad = max(min_range - map_xy[1], 0)
+        bottom_pad = max(min_range - top_down_map.shape[0] + map_xy[0] + 1, 0)
+        right_pad = max(min_range - top_down_map.shape[1] + map_xy[1] + 1, 0)
+
+        ego_map = np.pad(
+            ego_map,
+            ((top_pad, bottom_pad), (left_pad, right_pad), (0, 0)),
+            'constant', 
+            constant_values=0,
+        )
+
+        """
+        NOTE - the map is currently addressed as follows: 
+        rows are -X to X top to bottom, cols are -Z to Z left to right
+        To get -Z top and X right, we need transpose the map.
+        """
+        ego_map = ego_map.transpose(1, 0, 2)
+
+        # Rotate the map to egocentric coordinates
+        half_size = ego_map.shape[0] // 2
+        center = (half_size, half_size)
+        rot_angle = math.degrees(world_xyt[2])
+        # Anti-clockwise rotation about the center
+        M = cv2.getRotationMatrix2D(center, rot_angle, 1.0)
+
+        ego_map = cv2.warpAffine(
+            ego_map,
+            M,
+            (ego_map.shape[1], ego_map.shape[0]),
+            flags=cv2.INTER_NEAREST,
+            borderMode=cv2.BORDER_CONSTANT,
+            borderValue=(0,),
+        )
+
+        # Obtain the occupancy map
+        start = int(half_size - proj_map_range)
+        end = int(half_size + proj_map_range)
+        assert start >= 0
+        assert end <= ego_map.shape[0]
+        proj_ego_map = self._crop_and_resize(
+            ego_map,
+            (start, end, start, end),
+            (proj_map_size, proj_map_size),
+            cv2.INTER_NEAREST,
+        )
+        proj_ego_map = np.clip(proj_ego_map, 0, 1) # (H, W, 3)
+
+        return proj_ego_map
diff --git a/habitat/tasks/exp_nav/__init__.py b/habitat/tasks/exp_nav/__init__.py
new file mode 100644
index 0000000..e8c3661
--- /dev/null
+++ b/habitat/tasks/exp_nav/__init__.py
@@ -0,0 +1,9 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from habitat.tasks.exp_nav.exp_nav_task import ExploreNavigationTask
+
+__all__ = ["ExploreNavigationTask"]
diff --git a/habitat/tasks/exp_nav/exp_nav_task.py b/habitat/tasks/exp_nav/exp_nav_task.py
new file mode 100644
index 0000000..cde4bd3
--- /dev/null
+++ b/habitat/tasks/exp_nav/exp_nav_task.py
@@ -0,0 +1,746 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, List, Optional, Type
+
+import pdb
+import attr
+import cv2
+import math
+import numpy as np
+from gym import spaces
+
+from habitat.config import Config
+from habitat.core.dataset import Dataset, Episode
+from habitat.core.embodied_task import EmbodiedTask, Measure, Measurements
+from habitat.core.registry import registry
+from habitat.core.simulator import (
+    Sensor,
+    SensorSuite,
+    SensorTypes,
+    ShortestPathPoint,
+    Simulator,
+    SimulatorActions,
+)
+from habitat.core.utils import not_none_validator
+from habitat.tasks.utils import cartesian_to_polar, quaternion_rotate_vector
+from habitat.utils.visualizations import fog_of_war, maps
+from habitat.utils.visualizations.utils import topdown_to_image
+from habitat.tasks.exp_nav.shortest_path_follower import ShortestPathFollower
+from habitat.tasks.nav.nav_task import (
+    NavigationGoal,
+    NavigationEpisode,
+    NavigationTask
+)
+
+MAP_THICKNESS_SCALAR: int = 1250
+RGBSENSOR_DIMENSION: int = 3
+
+
+def merge_sim_episode_config(
+    sim_config: Config, episode: Type[Episode]
+) -> Any:
+    sim_config.defrost()
+    sim_config.SCENE = episode.scene_id
+    sim_config.freeze()
+    if (
+        episode.start_position is not None
+        and episode.start_rotation is not None
+    ):
+        agent_name = sim_config.AGENTS[sim_config.DEFAULT_AGENT_ID]
+        agent_cfg = getattr(sim_config, agent_name)
+        agent_cfg.defrost()
+        agent_cfg.START_POSITION = episode.start_position
+        agent_cfg.START_ROTATION = episode.start_rotation
+        agent_cfg.IS_SET_START_STATE = True
+        agent_cfg.freeze()
+    return sim_config
+
+@attr.s(auto_attribs=True, kw_only=True)
+class ImageGoal(NavigationGoal):
+    r"""Image goal that can be specified by position, rotation.
+    """
+
+    rotation: List[float] = attr.ib(default=None, validator=not_none_validator)
+
+
+@attr.s(auto_attribs=True, kw_only=True)
+class ExploreNavigationEpisode(NavigationEpisode):
+    r"""Class for episode specification that includes initial position and
+    rotation of agent, scene name, goal and optional shortest paths. An
+    episode is a description of one task instance for the agent.
+
+    Args:
+        episode_id: id of episode in the dataset, usually episode number
+        scene_id: id of scene in scene dataset
+        start_position: numpy ndarray containing 3 entries for (x, y, z)
+        start_rotation: numpy ndarray with 4 entries for (x, y, z, w)
+            elements of unit quaternion (versor) representing agent 3D
+            orientation. ref: https://en.wikipedia.org/wiki/Versor
+        start_nav_position: numpy ndarray containing 3 entries for (x, y, z)
+        start_nav_rotation: numpy ndarray with 4 entries for (x, y, z, w)
+            elements of unit quaternion (versor) representing agent 3D
+            orientation.
+        goals: list of goals specifications
+        start_room: room id
+        shortest_paths: list containing shortest paths to goals
+    """
+
+    start_nav_position: List[float] = attr.ib(
+        default=None, validator=not_none_validator
+    )
+    start_nav_rotation: List[float] = attr.ib(
+        default=None, validator=not_none_validator
+    )
+    goals: List[ImageGoal] = attr.ib(
+        default=None, validator=not_none_validator
+    )
+
+
+@registry.register_sensor
+class ImageGoalSensorExploreNavigation(Sensor):
+    r"""Sensor for ImageGoal observations. This can be used for an image-target navigation
+    task. Not currently in use.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the ImageGoal sensor.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self.T_exp = config.T_EXP
+        self.T_nav = config.T_NAV
+        super().__init__(config=config)
+        self.current_episode_id = None
+        self.current_target = None
+        self._num_steps = None
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "image_goal_exp_nav"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0,
+            high=255,
+            shape=(self.config.HEIGHT, self.config.WIDTH, RGBSENSOR_DIMENSION),
+            dtype=np.uint8,
+        )
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            self.current_episode_id = episode_id
+            self._num_steps = 0.0
+
+        if self._num_steps >= self.T_exp:
+            tgt_obs = self._sim.get_observations_at(
+                         episode.goals[0].position,
+                         episode.goals[0].rotation
+                      )
+            tgt_im = tgt_obs['rgb']
+        else:
+            tgt_im = np.zeros((
+                                 self.config.HEIGHT,
+                                 self.config.WIDTH,
+                                 RGBSENSOR_DIMENSION
+                              ), dtype=np.uint8)
+
+        self._num_steps += 1
+        return tgt_im
+
+
+@registry.register_sensor
+class GridGoalSensorExploreNavigation(Sensor):
+    r"""Sensor for PointGoal observations which are used in the ExploreNavigation task.
+    The observations are provided in an egocentric discretized map coordinate system. 
+    This allows directly feeding in the egocentric map and grid_goal observation to
+    a planning algorithm for action selection.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the ImageGoal sensor.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self.T_exp = config.T_EXP
+        self.T_nav = config.T_NAV
+        super().__init__(config=config)
+        self.current_episode_id = None
+        self.current_target = None
+        self._num_steps = None
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "grid_goal_exp_nav"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=-100000000.0,
+            high=100000000.0,
+            shape=(2, ),
+            dtype=np.float32,
+        )
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            self.current_episode_id = episode_id
+            self._num_steps = 0.0
+
+        if self._num_steps >= self.T_exp:
+            agent_position = self._sim.get_agent_state().position
+            agent_rotation = self._sim.get_agent_state().rotation
+            tgt_position = episode.goals[0].position
+            tgt_grid_pos = self._get_egocentric_grid_loc(agent_position, agent_rotation, tgt_position)
+        else:
+            tgt_grid_pos = np.zeros((2, ))
+
+        self._num_steps += 1
+        return tgt_grid_pos
+
+    def quaternion_to_heading(self, quaternion):
+        # quaternion - np.quaternion unit quaternion
+        # Real world rotation
+        direction_vector = np.array([0, 0, -1])
+        heading_vector = quaternion_rotate_vector(
+            quaternion.inverse(), direction_vector
+        )
+
+        phi = cartesian_to_polar(-heading_vector[2], heading_vector[0])[1]
+        return phi
+        
+    def _get_egocentric_grid_loc(self, agent_position, agent_rotation, tgt_position):
+        """
+        In the current egocentric occupancy map, what is the
+        target position?
+        NOTE: The egocentric occupancy map is forward facing
+        upward.
+        """
+        tgt_x = -tgt_position[2]
+        tgt_y = tgt_position[0]
+        curr_x = -agent_position[2]
+        curr_y = agent_position[0]
+        curr_t = -self.quaternion_to_heading(agent_rotation)
+
+        r_ct = math.sqrt((tgt_x - curr_x)**2 + (tgt_y - curr_y)**2)
+        t_ct = math.atan2(tgt_y - curr_y, tgt_x - curr_x) - curr_t
+
+        width = self._sim.config.HIGHRES_COARSE_OCC_SENSOR.WIDTH
+        height = self._sim.config.HIGHRES_COARSE_OCC_SENSOR.HEIGHT
+        wby2 = width // 2
+        hby2 = height // 2
+
+        grid_size = self._sim.config.OCCUPANCY_MAPS.MAP_SCALE
+        large_map_range = self._sim.config.OCCUPANCY_MAPS.LARGE_MAP_RANGE
+
+        disp_y = - r_ct * math.cos(t_ct) / grid_size
+        disp_x = r_ct * math.sin(t_ct) / grid_size
+        disp_y = height * disp_y / (2*large_map_range + 1)
+        disp_x = width * disp_x / (2*large_map_range + 1)
+        grid_y = np.clip(hby2 + disp_y, 0, height-1)
+        grid_x = np.clip(wby2 + disp_x, 0, width-1)
+
+        return grid_x, grid_y
+
+
+@registry.register_sensor
+class SPActionSensorExploreNavigation(Sensor):
+    r"""Sensor that returns the shortest path action to the navigation target.
+    Similar to SPActionSensor from nav_task.py, but returns some constant
+    action during the exploration phase.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the sensor. 
+
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        super().__init__(config=config)
+
+        self.current_episode_id = None
+        self.target_position = None
+        goal_radius = config.GOAL_RADIUS
+        self.goal_radius = goal_radius
+        self.follower = ShortestPathFollower(sim, goal_radius, False)
+        self.follower.mode = "geodesic_path"
+        self.T_exp = config.T_EXP
+        self.T_nav = config.T_NAV
+        self._step_count = None
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "sp_action_sensor_exp_nav"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0,
+            high=10,
+            shape=(1, ),
+            dtype=np.int32,
+        )
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            # A new episode has started
+            self.current_episode_id = episode_id
+            self.target_position = episode.goals[0].position
+            self._step_count = 0
+
+        if self._step_count >= self.T_exp:
+            current_target = self.target_position
+            agent_position = self._sim.get_agent_state().position
+            oracle_action = self.follower.get_next_action(current_target)
+        else:
+            # Some constant action
+            oracle_action = SimulatorActions.MOVE_FORWARD
+
+        self._step_count += 1
+
+        return np.array([oracle_action], dtype=np.int32)
+
+
+@registry.register_measure
+class TopDownMapExpNav(Measure):
+    r"""Top Down Map measure
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._config = config
+        self._grid_delta = config.MAP_PADDING
+        self.T_exp = config.T_EXP
+        self.T_nav = config.T_NAV
+        self._step_count = None
+        self._map_resolution = (config.MAP_RESOLUTION, config.MAP_RESOLUTION)
+        self._num_samples = config.NUM_TOPDOWN_MAP_SAMPLE_POINTS
+        self._ind_x_min = None
+        self._ind_x_max = None
+        self._ind_y_min = None
+        self._ind_y_max = None
+        self._previous_xy_location = None
+        self._coordinate_min = maps.COORDINATE_MIN
+        self._coordinate_max = maps.COORDINATE_MAX
+        self._top_down_map = None
+        self._shortest_path_points = None
+        self._cell_scale = (
+            self._coordinate_max - self._coordinate_min
+        ) / self._map_resolution[0]
+        self.line_thickness = int(
+            np.round(self._map_resolution[0] * 2 / MAP_THICKNESS_SCALAR)
+        )
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "top_down_map_exp_nav"
+
+    def _check_valid_nav_point(self, point: List[float]):
+        self._sim.is_navigable(point)
+
+    def get_original_map(self):
+        top_down_map = maps.get_topdown_map(
+            self._sim,
+            self._map_resolution,
+            self._num_samples,
+            self._config.DRAW_BORDER,
+        )
+
+        range_x = np.where(np.any(top_down_map, axis=1))[0]
+        range_y = np.where(np.any(top_down_map, axis=0))[0]
+
+        self._ind_x_min = range_x[0]
+        self._ind_x_max = range_x[-1]
+        self._ind_y_min = range_y[0]
+        self._ind_y_max = range_y[-1]
+
+        if self._config.FOG_OF_WAR.DRAW:
+            self._fog_of_war_mask = np.zeros_like(top_down_map)
+
+        return top_down_map
+
+    def draw_source_and_target(self, episode):
+        if self._step_count < self.T_exp:
+            # mark source point
+            s_x, s_y = maps.to_grid(
+                episode.start_position[0],
+                episode.start_position[2],
+                self._coordinate_min,
+                self._coordinate_max,
+                self._map_resolution,
+            )
+        else:
+            # mark source point
+            s_x, s_y = maps.to_grid(
+                episode.start_nav_position[0],
+                episode.start_nav_position[2],
+                self._coordinate_min,
+                self._coordinate_max,
+                self._map_resolution,
+            )
+
+        point_padding = 2 * int(
+            np.ceil(self._map_resolution[0] / MAP_THICKNESS_SCALAR)
+        )
+        self._top_down_map[
+            s_x - point_padding : s_x + point_padding + 1,
+            s_y - point_padding : s_y + point_padding + 1,
+        ] = maps.MAP_SOURCE_POINT_INDICATOR
+
+        if self._step_count >= self.T_exp:
+            # mark target point
+            t_x, t_y = maps.to_grid(
+                episode.goals[0].position[0],
+                episode.goals[0].position[2],
+                self._coordinate_min,
+                self._coordinate_max,
+                self._map_resolution,
+            )
+            self._top_down_map[
+                t_x - point_padding : t_x + point_padding + 1,
+                t_y - point_padding : t_y + point_padding + 1,
+            ] = maps.MAP_TARGET_POINT_INDICATOR
+
+    def reset_metric(self, episode):
+        self._step_count = 0
+        self._metric = None
+        self._top_down_map = self.get_original_map()
+        agent_position = self._sim.get_agent_state().position
+        a_x, a_y = maps.to_grid(
+            agent_position[0],
+            agent_position[2],
+            self._coordinate_min,
+            self._coordinate_max,
+            self._map_resolution,
+        )
+        self._previous_xy_location = (a_y, a_x)
+
+        self.update_fog_of_war_mask(np.array([a_x, a_y]))
+
+        # draw source and target points last to avoid overlap
+        if self._config.DRAW_SOURCE_AND_TARGET:
+            self.draw_source_and_target(episode)
+
+    def _clip_map(self, _map):
+        return _map[
+            self._ind_x_min
+            - self._grid_delta : self._ind_x_max
+            + self._grid_delta,
+            self._ind_y_min
+            - self._grid_delta : self._ind_y_max
+            + self._grid_delta,
+        ]
+
+    def update_metric(self, episode, action):
+        if self._step_count == self.T_exp:
+            self._top_down_map = self.get_original_map()
+            agent_position = self._sim.get_agent_state().position
+            a_x, a_y = maps.to_grid(
+                agent_position[0],
+                agent_position[2],
+                self._coordinate_min,
+                self._coordinate_max,
+                self._map_resolution,
+            )
+
+            self._previous_xy_location = (a_y, a_x)
+            if self._config.DRAW_SHORTEST_PATH:
+                # draw shortest path
+                self._shortest_path_points = self._sim.get_straight_shortest_path_points(
+                    agent_position, episode.goals[0].position
+                )
+                self._shortest_path_points = [
+                    maps.to_grid(
+                        p[0],
+                        p[2],
+                        self._coordinate_min,
+                        self._coordinate_max,
+                        self._map_resolution,
+                    )[::-1]
+                    for p in self._shortest_path_points
+                ]
+                maps.draw_path(
+                    self._top_down_map,
+                    self._shortest_path_points,
+                    maps.MAP_SHORTEST_PATH_COLOR,
+                    self.line_thickness,
+                )
+
+            self.update_fog_of_war_mask(np.array([a_x, a_y]))
+
+            # draw source and target points last to avoid overlap
+            if self._config.DRAW_SOURCE_AND_TARGET:
+                self.draw_source_and_target(episode)
+
+        self._step_count += 1
+
+        house_map, map_agent_x, map_agent_y = self.update_map(
+            self._sim.get_agent_state().position
+        )
+
+        # Rather than return the whole map which may have large empty regions,
+        # only return the occupied part (plus some padding).
+        clipped_house_map = self._clip_map(house_map)
+
+        clipped_fog_of_war_map = None
+        if self._config.FOG_OF_WAR.DRAW:
+            clipped_fog_of_war_map = self._clip_map(self._fog_of_war_mask)
+
+        self._metric = {
+            "map": clipped_house_map,
+            "fog_of_war_mask": clipped_fog_of_war_map,
+            "agent_map_coord": (
+                map_agent_x - (self._ind_x_min - self._grid_delta),
+                map_agent_y - (self._ind_y_min - self._grid_delta),
+            ),
+            "agent_angle": self.get_polar_angle(),
+        }
+
+        self._metric = topdown_to_image(self._metric)
+
+    def get_polar_angle(self):
+        agent_state = self._sim.get_agent_state()
+        # quaternion is in x, y, z, w format
+        ref_rotation = agent_state.rotation
+
+        heading_vector = quaternion_rotate_vector(
+            ref_rotation.inverse(), np.array([0, 0, -1])
+        )
+
+        phi = cartesian_to_polar(-heading_vector[2], heading_vector[0])[1]
+        x_y_flip = -np.pi / 2
+        return np.array(phi) + x_y_flip
+
+    def update_map(self, agent_position):
+        a_x, a_y = maps.to_grid(
+            agent_position[0],
+            agent_position[2],
+            self._coordinate_min,
+            self._coordinate_max,
+            self._map_resolution,
+        )
+        # Don't draw over the source point
+        if self._top_down_map[a_x, a_y] != maps.MAP_SOURCE_POINT_INDICATOR:
+            color = 10 + min(
+                self._step_count * 245 // self._config.MAX_EPISODE_STEPS, 245
+            )
+
+            thickness = int(
+                np.round(self._map_resolution[0] * 2 / MAP_THICKNESS_SCALAR)
+            )
+            cv2.line(
+                self._top_down_map,
+                self._previous_xy_location,
+                (a_y, a_x),
+                color,
+                thickness=thickness,
+            )
+
+        self.update_fog_of_war_mask(np.array([a_x, a_y]))
+
+        self._previous_xy_location = (a_y, a_x)
+        return self._top_down_map, a_x, a_y
+
+    def update_fog_of_war_mask(self, agent_position):
+        if self._config.FOG_OF_WAR.DRAW:
+            self._fog_of_war_mask = fog_of_war.reveal_fog_of_war(
+                self._top_down_map,
+                self._fog_of_war_mask,
+                agent_position,
+                self.get_polar_angle(),
+                fov=self._config.FOG_OF_WAR.FOV,
+                max_line_len=self._config.FOG_OF_WAR.VISIBILITY_DIST
+                * max(self._map_resolution)
+                / (self._coordinate_max - self._coordinate_min),
+            )
+
+
+@registry.register_measure
+class SPLExpNav(Measure):
+    r"""SPL (Success weighted by Path Length)
+
+    ref: On Evaluation of Embodied Agents - Anderson et. al
+    https://arxiv.org/pdf/1807.06757.pdf
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._previous_position = None
+        self._start_end_episode_distance = None
+        self._agent_episode_distance = None
+        self._sim = sim
+        self.T_exp = config.T_EXP
+        self.T_nav = config.T_NAV
+        self._step_count = None
+        self._config = config
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "spl_exp_nav"
+
+    def reset_metric(self, episode):
+        self._step_count = 0
+        #self._previous_position = self._sim.get_agent_state().position.tolist()
+        self._start_end_episode_distance = episode.info["geodesic_distance"]
+        self._agent_episode_distance = 0.0
+        self._metric = None
+
+    def _euclidean_distance(self, position_a, position_b):
+        return np.linalg.norm(
+            np.array(position_b) - np.array(position_a), ord=2
+        )
+
+    def update_metric(self, episode, action):
+
+        if self._step_count > self.T_exp+1:
+
+            ep_success = 0
+            current_position = self._sim.get_agent_state().position.tolist()
+
+            distance_to_target = self._sim.geodesic_distance(
+                current_position, episode.goals[0].position
+            )
+
+            if (
+                action == self._sim.index_stop_action
+                and distance_to_target < self._config.SUCCESS_DISTANCE
+            ):
+                ep_success = 1
+
+            self._agent_episode_distance += self._euclidean_distance(
+                current_position, self._previous_position
+            )
+
+            self._previous_position = current_position
+
+            self._metric = ep_success * (
+                self._start_end_episode_distance
+                / max(
+                    self._start_end_episode_distance, self._agent_episode_distance
+                )
+            )
+
+        else:
+            self._metric = 0.0
+
+        self._step_count += 1
+
+        if self._step_count == self.T_exp+1:
+            self._previous_position = self._sim.get_agent_state().position.tolist()
+
+
+@registry.register_measure
+class SuccessExpNav(Measure):
+    r"""Success Rate
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self.T_exp = config.T_EXP
+        self.T_nav = config.T_NAV
+        self._step_count = None
+        self._config = config
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "success_exp_nav"
+
+    def reset_metric(self, episode):
+        self._step_count = 0
+        self._metric = None
+
+    def update_metric(self, episode, action):
+
+        if self._step_count > self.T_exp+1:
+
+            ep_success = 0.0
+            current_position = self._sim.get_agent_state().position.tolist()
+
+            distance_to_target = self._sim.geodesic_distance(
+                current_position, episode.goals[0].position
+            )
+
+            if (
+                action == self._sim.index_stop_action
+                and distance_to_target < self._config.SUCCESS_DISTANCE
+            ):
+                ep_success = 1.0
+
+            self._metric = ep_success
+        else:
+            self._metric = 0.0
+
+        self._step_count += 1
+
+
+@registry.register_measure
+class NavigationErrorExpNav(Measure):
+    r"""Navigation Error - geodesic distance to target at the current time step.
+
+    ref: On Evaluation of Embodied Agents - Anderson et. al
+    https://arxiv.org/pdf/1807.06757.pdf
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self.T_exp = config.T_EXP
+        self.T_nav = config.T_NAV
+        self._step_count = None
+        self._config = config
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "nav_error_exp_nav"
+
+    def reset_metric(self, episode):
+        self._step_count = 0
+        self._metric = None
+
+    def update_metric(self, episode, action):
+
+        if self._step_count > self.T_exp+1:
+
+            current_position = self._sim.get_agent_state().position.tolist()
+
+            distance_to_target = self._sim.geodesic_distance(
+                current_position, episode.goals[0].position
+            )
+
+            self._metric = distance_to_target
+
+        else:
+            self._metric = math.inf
+
+        self._step_count += 1
+
+
+@registry.register_task(name="ExpNav-v0")
+class ExploreNavigationTask(NavigationTask):
+    def __init__(
+        self,
+        task_config: Config,
+        sim: Simulator,
+        dataset: Optional[Dataset] = None,
+    ) -> None:
+
+        super().__init__(task_config=task_config, sim=sim, dataset=dataset)
+
+    def overwrite_sim_config(
+        self, sim_config: Any, episode: Type[Episode]
+    ) -> Any:
+        return merge_sim_episode_config(sim_config, episode)
diff --git a/habitat/tasks/exp_nav/shortest_path_follower.py b/habitat/tasks/exp_nav/shortest_path_follower.py
new file mode 100644
index 0000000..e070322
--- /dev/null
+++ b/habitat/tasks/exp_nav/shortest_path_follower.py
@@ -0,0 +1,223 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Union
+
+import math
+import numpy as np
+
+import habitat_sim
+from habitat.core.simulator import SimulatorActions
+from habitat.sims.habitat_simulator.habitat_simulator import HabitatSim
+from habitat.utils.geometry_utils import (
+    angle_between_quaternions,
+    quaternion_from_two_vectors,
+)
+from habitat.tasks.utils import cartesian_to_polar, quaternion_rotate_vector
+
+EPSILON = 1e-6
+
+
+def action_to_one_hot(action: int) -> np.array:
+    one_hot = np.zeros(len(SimulatorActions), dtype=np.float32)
+    one_hot[action] = 1
+    return one_hot
+
+
+class ShortestPathFollower:
+    r"""Utility class for extracting the action on the shortest path to the
+        goal.
+    Args:
+        sim: HabitatSim instance.
+        goal_radius: Distance between the agent and the goal for it to be
+            considered successful.
+        return_one_hot: If true, returns a one-hot encoding of the action
+            (useful for training ML agents). If false, returns the
+            SimulatorAction.
+    """
+
+    def __init__(
+        self, sim: HabitatSim, goal_radius: float, return_one_hot: bool = True
+    ):
+        assert (
+            getattr(sim, "geodesic_distance", None) is not None
+        ), "{} must have a method called geodesic_distance".format(
+            type(sim).__name__
+        )
+
+        self._sim = sim
+        self._max_delta = self._sim.config.FORWARD_STEP_SIZE - EPSILON
+        self._goal_radius = goal_radius
+        self._step_size = self._sim.config.FORWARD_STEP_SIZE
+
+        self._mode = (
+            "geodesic_path"
+            if getattr(sim, "get_straight_shortest_path_points", None)
+            is not None
+            else "greedy"
+        )
+        self._return_one_hot = return_one_hot
+
+    def _get_return_value(self, action) -> Union[int, np.array]:
+        if self._return_one_hot:
+            return action_to_one_hot(action)
+        else:
+            return action
+
+    def get_next_action(self, goal_pos: np.array) -> Union[int, np.array]:
+        """Returns the next action along the shortest path.
+        """
+        if (
+            np.linalg.norm(goal_pos - self._sim.get_agent_state().position)
+            <= self._goal_radius
+        ):
+            return self._get_return_value(SimulatorActions.STOP)
+
+        max_grad_dir = self._est_max_grad_dir(goal_pos)
+        if max_grad_dir is None:
+            return self._get_return_value(SimulatorActions.MOVE_FORWARD)
+        return self._step_along_grad(max_grad_dir)
+
+    def _quaternion_to_heading(self, quaternion):
+        # quaternion - np.quaternion unit quaternion
+        # Real world rotation
+        direction_vector = np.array([0, 0, -1])
+        heading_vector = quaternion_rotate_vector(
+            quaternion.inverse(), direction_vector
+        )
+
+        phi = cartesian_to_polar(-heading_vector[2], heading_vector[0])[1]
+        return phi
+        
+    def _step_along_grad(
+        self, grad_dir: np.quaternion
+    ) -> Union[int, np.array]:
+        current_state = self._sim.get_agent_state()
+        alpha = angle_between_quaternions(grad_dir, current_state.rotation)
+        if alpha <= np.deg2rad(self._sim.config.TURN_ANGLE) + EPSILON:
+            return self._get_return_value(SimulatorActions.MOVE_FORWARD)
+        else:
+            # These angles represent the rightward rotation from forward direction
+            grad_angle = -self._quaternion_to_heading(grad_dir)
+            curr_angle = -self._quaternion_to_heading(current_state.rotation)
+
+            diff_angle = grad_angle - curr_angle
+            diff_angle = math.atan2(math.sin(diff_angle), math.cos(diff_angle))
+            
+            if diff_angle > 0:
+                best_turn = SimulatorActions.TURN_RIGHT
+            else:
+                best_turn = SimulatorActions.TURN_LEFT
+
+            #sim_action = SimulatorActions.TURN_LEFT
+            #self._sim.step(sim_action)
+            #best_turn = (
+            #    SimulatorActions.TURN_LEFT
+            #    if (
+            #        angle_between_quaternions(
+            #            grad_dir, self._sim.get_agent_state().rotation
+            #        )
+            #        < alpha
+            #    )
+            #    else SimulatorActions.TURN_RIGHT
+            #)
+            #self._reset_agent_state(current_state)
+            return self._get_return_value(best_turn)
+
+    def _reset_agent_state(self, state: habitat_sim.AgentState) -> None:
+        self._sim.set_agent_state(
+            state.position, state.rotation, reset_sensors=False
+        )
+
+    def _geo_dist(self, goal_pos: np.array) -> float:
+        return self._sim.geodesic_distance(
+            self._sim.get_agent_state().position, goal_pos
+        )
+
+    def _est_max_grad_dir(self, goal_pos: np.array) -> np.array:
+
+        current_state = self._sim.get_agent_state()
+        current_pos = current_state.position
+
+        if self.mode == "geodesic_path":
+            points = self._sim.get_straight_shortest_path_points(
+                self._sim.get_agent_state().position, goal_pos
+            )
+            # Add a little offset as things get weird if
+            # points[1] - points[0] is anti-parallel with forward
+            if len(points) < 2:
+                return None
+            max_grad_dir = quaternion_from_two_vectors(
+                self._sim.forward_vector,
+                points[1]
+                - points[0]
+                + EPSILON
+                * np.cross(self._sim.up_vector, self._sim.forward_vector),
+            )
+            max_grad_dir.x = 0
+            max_grad_dir = np.normalized(max_grad_dir)
+        else:
+            current_rotation = self._sim.get_agent_state().rotation
+            current_dist = self._geo_dist(goal_pos)
+
+            best_geodesic_delta = -2 * self._max_delta
+            best_rotation = current_rotation
+            for _ in range(0, 360, self._sim.config.TURN_ANGLE):
+                sim_action = SimulatorActions.MOVE_FORWARD
+                self._sim.step(sim_action)
+                new_delta = current_dist - self._geo_dist(goal_pos)
+
+                if new_delta > best_geodesic_delta:
+                    best_rotation = self._sim.get_agent_state().rotation
+                    best_geodesic_delta = new_delta
+
+                # If the best delta is within (1 - cos(TURN_ANGLE))% of the
+                # best delta (the step size), then we almost certainly have
+                # found the max grad dir and should just exit
+                if np.isclose(
+                    best_geodesic_delta,
+                    self._max_delta,
+                    rtol=1 - np.cos(np.deg2rad(self._sim.config.TURN_ANGLE)),
+                ):
+                    break
+
+                self._sim.set_agent_state(
+                    current_pos,
+                    self._sim.get_agent_state().rotation,
+                    reset_sensors=False,
+                )
+
+                sim_action = SimulatorActions.TURN_LEFT
+                self._sim.step(sim_action)
+
+            self._reset_agent_state(current_state)
+
+            max_grad_dir = best_rotation
+
+        return max_grad_dir
+
+    @property
+    def mode(self):
+        return self._mode
+
+    @mode.setter
+    def mode(self, new_mode: str):
+        r"""Sets the mode for how the greedy follower determines the best next
+            step.
+        Args:
+            new_mode: geodesic_path indicates using the simulator's shortest
+                path algorithm to find points on the map to navigate between.
+                greedy indicates trying to move forward at all possible
+                orientations and selecting the one which reduces the geodesic
+                distance the most.
+        """
+        assert new_mode in {"geodesic_path", "greedy"}
+        if new_mode == "geodesic_path":
+            assert (
+                getattr(self._sim, "get_straight_shortest_path_points", None)
+                is not None
+            )
+        self._mode = new_mode
diff --git a/habitat/tasks/nav/nav_task.py b/habitat/tasks/nav/nav_task.py
index 27b1451..5c32e67 100644
--- a/habitat/tasks/nav/nav_task.py
+++ b/habitat/tasks/nav/nav_task.py
@@ -8,6 +8,7 @@ from typing import Any, List, Optional, Type
 
 import attr
 import cv2
+import math
 import numpy as np
 from gym import spaces
 
@@ -25,6 +26,7 @@ from habitat.core.simulator import (
 from habitat.core.utils import not_none_validator
 from habitat.tasks.utils import cartesian_to_polar, quaternion_rotate_vector
 from habitat.utils.visualizations import fog_of_war, maps
+from habitat.tasks.nav.shortest_path_follower import ShortestPathFollower
 
 MAP_THICKNESS_SCALAR: int = 1250
 
@@ -318,6 +320,181 @@ class ProximitySensor(Sensor):
         )
 
 
+@registry.register_sensor
+class LocalTopDownSensor(Sensor):
+    r"""A Local topdown map sensor that returns ground-truth 
+    information about obstacles, free space within a local region 
+    centered around the agent. This is useful for evaluating 
+    noise robustness and for pointnav evaluation.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._config = config
+        self._cell_scale = config.MAP_SCALE
+        self._num_samples = config.NUM_TOPDOWN_MAP_SAMPLE_POINTS
+        self._ind_x_min = None
+        self._ind_x_max = None
+        self._ind_y_min = None
+        self._ind_y_max = None
+        self._previous_xy_location = None
+        self._coordinate_min = maps.COORDINATE_MIN
+        self._coordinate_max = maps.COORDINATE_MAX
+        self._top_down_map = None
+        self._shortest_path_points = None
+        resolution = (
+            self._coordinate_max - self._coordinate_min
+        ) / self._cell_scale
+        self._map_resolution = (int(resolution), int(resolution))
+        self._map_range = config.MAP_RANGE
+        self.current_episode_id = None
+        self._width = config.WIDTH
+        self._height = config.HEIGHT
+
+        super().__init__(config=config)
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "local_top_down_sensor"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.COLOR
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0.0,
+            high=1.0,
+            shape=(self._height, self._width),
+            dtype=np.float,
+        )
+
+    def _check_valid_nav_point(self, point: List[float]):
+        self._sim.is_navigable(point)
+
+    def get_original_map(self):
+        top_down_map = maps.get_topdown_map(
+            self._sim,
+            self._map_resolution,
+            self._num_samples,
+            False,
+        )
+
+        return top_down_map
+
+    def quaternion_to_heading(self, quaternion):
+        # quaternion - np.quaternion unit quaternion
+        # Real world rotation
+        direction_vector = np.array([0, 0, -1])
+        heading_vector = quaternion_rotate_vector(
+            quaternion.inverse(), direction_vector
+        )
+
+        phi = cartesian_to_polar(-heading_vector[2], heading_vector[0])[1]
+        return phi
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            self.current_episode_id = episode_id
+            # Transpose to make x rightward and y downward
+            self._top_down_map = self.get_original_map().T
+
+        agent_position = self._sim.get_agent_state().position
+        agent_rotation = self._sim.get_agent_state().rotation
+        a_x, a_y = maps.to_grid(
+            agent_position[0],
+            agent_position[2],
+            self._coordinate_min,
+            self._coordinate_max,
+            self._map_resolution,
+        )
+        a_head = self.quaternion_to_heading(agent_rotation)
+        a_head = math.degrees(a_head)
+
+        # Crop region centered around the agent
+        mrange = int(1.5*self._map_range)
+        ego_map = self._top_down_map[(a_y-mrange):(a_y+mrange),
+                                     (a_x-mrange):(a_x+mrange)]
+        if ego_map.shape[0] == 0 or ego_map.shape[1] == 0:
+            ego_map = np.zeros((2*mrange+1, 2*mrange+1), dtype=np.uint8)
+
+        half_size = ego_map.shape[0]//2
+        center = (half_size, half_size)
+        M = cv2.getRotationMatrix2D(center, a_head, 1.0)
+
+        ego_map = cv2.warpAffine(ego_map, M,
+                                 (ego_map.shape[1], ego_map.shape[0]),
+                                 flags=cv2.INTER_NEAREST,
+                                 borderMode=cv2.BORDER_CONSTANT,
+                                 borderValue=(1, ))
+        ego_map = ego_map.astype(np.float32)
+        mrange = self._map_range
+        ego_map = ego_map[(half_size-mrange):(half_size+mrange),
+                          (half_size-mrange):(half_size+mrange)]
+
+
+        # This map is currently 0 if occupied and 1 if unoccupied
+        # Flip this to get 0 for unoccupied and 1 for occupied
+        ego_map = 1.0 - ego_map.astype(np.float32)
+
+        # Flip the x axis because to_grid() flips the conventions
+        ego_map = np.flip(ego_map, axis=1)
+
+        ego_map = cv2.resize(ego_map,
+                             (self._width, self._height),
+                             interpolation=cv2.INTER_NEAREST)
+
+        return ego_map
+
+
+@registry.register_sensor
+class SPActionSensor(Sensor):
+    r"""Sensor that returns the shortest path action to the navigation target.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the sensor.
+
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        super().__init__(config=config)
+
+        self.current_episode_id = None
+        self.target_position = None
+        goal_radius = config.GOAL_RADIUS
+        self.goal_radius = goal_radius
+        self.follower = ShortestPathFollower(sim, goal_radius, False)
+        self.follower.mode = "geodesic_path"
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "sp_action_sensor"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0,
+            high=10,
+            shape=(1, ),
+            dtype=np.int32,
+        )
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            # A new episode has started
+            self.current_episode_id = episode_id
+            self.target_position = episode.goals[0].position
+
+        current_target = self.target_position
+        agent_position = self._sim.get_agent_state().position
+        oracle_action = self.follower.get_next_action(current_target)
+
+        return np.array([oracle_action], dtype=np.int32)
+
+
 @registry.register_measure
 class SPL(Measure):
     r"""SPL (Success weighted by Path Length)
diff --git a/habitat/tasks/nav/shortest_path_follower.py b/habitat/tasks/nav/shortest_path_follower.py
index 2d7b2f1..0c24946 100644
--- a/habitat/tasks/nav/shortest_path_follower.py
+++ b/habitat/tasks/nav/shortest_path_follower.py
@@ -8,6 +8,7 @@ from typing import Union
 
 import numpy as np
 
+import math
 import habitat_sim
 from habitat.core.simulator import SimulatorActions
 from habitat.sims.habitat_simulator.habitat_simulator import HabitatSim
@@ -15,6 +16,7 @@ from habitat.utils.geometry_utils import (
     angle_between_quaternions,
     quaternion_from_two_vectors,
 )
+from habitat.tasks.utils import cartesian_to_polar, quaternion_rotate_vector
 
 EPSILON = 1e-6
 
@@ -79,6 +81,17 @@ class ShortestPathFollower:
             return self._get_return_value(SimulatorActions.MOVE_FORWARD)
         return self._step_along_grad(max_grad_dir)
 
+    def _quaternion_to_heading(self, quaternion):
+        # quaternion - np.quaternion unit quaternion
+        # Real world rotation
+        direction_vector = np.array([0, 0, -1])
+        heading_vector = quaternion_rotate_vector(
+            quaternion.inverse(), direction_vector
+        )
+
+        phi = cartesian_to_polar(-heading_vector[2], heading_vector[0])[1]
+        return phi
+
     def _step_along_grad(
         self, grad_dir: np.quaternion
     ) -> Union[int, np.array]:
@@ -87,19 +100,37 @@ class ShortestPathFollower:
         if alpha <= np.deg2rad(self._sim.config.TURN_ANGLE) + EPSILON:
             return self._get_return_value(SimulatorActions.MOVE_FORWARD)
         else:
-            sim_action = SimulatorActions.TURN_LEFT
-            self._sim.step(sim_action)
-            best_turn = (
-                SimulatorActions.TURN_LEFT
-                if (
-                    angle_between_quaternions(
-                        grad_dir, self._sim.get_agent_state().rotation
-                    )
-                    < alpha
-                )
-                else SimulatorActions.TURN_RIGHT
-            )
-            self._reset_agent_state(current_state)
+            """
+            Instead of having to execute actions to figure out which is the best turn,
+            reason about it geometrically. The target direction, agent' current direction 
+            are known. If the difference between the target and current angle is in [-pi, 0), 
+            turn left. Otherwise, turn right.
+            """
+            # These angles represent the rightward rotation from forward direction
+            grad_angle = -self._quaternion_to_heading(grad_dir)
+            curr_angle = -self._quaternion_to_heading(current_state.rotation)
+
+            diff_angle = grad_angle - curr_angle
+            diff_angle = math.atan2(math.sin(diff_angle), math.cos(diff_angle))
+            
+            if diff_angle > 0:
+                best_turn = SimulatorActions.TURN_RIGHT
+            else:
+                best_turn = SimulatorActions.TURN_LEFT
+
+            #sim_action = SimulatorActions.TURN_LEFT
+            #self._sim.step(sim_action)
+            #best_turn = (
+            #    SimulatorActions.TURN_LEFT
+            #    if (
+            #        angle_between_quaternions(
+            #            grad_dir, self._sim.get_agent_state().rotation
+            #        )
+            #        < alpha
+            #    )
+            #    else SimulatorActions.TURN_RIGHT
+            #)
+            #self._reset_agent_state(current_state)
             return self._get_return_value(best_turn)
 
     def _reset_agent_state(self, state: habitat_sim.AgentState) -> None:
diff --git a/habitat/tasks/pose_estimation/__init__.py b/habitat/tasks/pose_estimation/__init__.py
new file mode 100644
index 0000000..2b99fc3
--- /dev/null
+++ b/habitat/tasks/pose_estimation/__init__.py
@@ -0,0 +1,9 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from habitat.tasks.pose_estimation.pose_estimation_task import PoseEstimationTask
+
+__all__ = ["PoseEstimationTask"]
diff --git a/habitat/tasks/pose_estimation/pose_estimation_task.py b/habitat/tasks/pose_estimation/pose_estimation_task.py
new file mode 100644
index 0000000..6bf7916
--- /dev/null
+++ b/habitat/tasks/pose_estimation/pose_estimation_task.py
@@ -0,0 +1,1195 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, List, Optional, Type
+
+import pdb
+import attr
+import cv2
+import copy
+import math
+import quaternion
+import numpy as np
+from gym import spaces
+
+from habitat.config import Config
+from habitat.core.dataset import Dataset, Episode
+from habitat.core.embodied_task import EmbodiedTask, Measure, Measurements
+from habitat.core.registry import registry
+from habitat.core.simulator import (
+    Sensor,
+    SensorSuite,
+    SensorTypes,
+    ShortestPathPoint,
+    Simulator,
+)
+from habitat.core.utils import not_none_validator
+from habitat.tasks.utils import cartesian_to_polar, quaternion_rotate_vector
+from habitat.utils.geometry_utils import angle_between_quaternions, quaternion_to_list
+from habitat.utils.visualizations import fog_of_war, maps
+from habitat.utils.visualizations.utils import topdown_to_image
+from habitat.tasks.nav.nav_task import TopDownMap
+from habitat_sim.utils import quat_from_coeffs
+from habitat.tasks.pose_estimation.shortest_path_follower import ShortestPathFollower
+
+MAP_THICKNESS_SCALAR: int = 1250
+RGBSENSOR_DIMENSION = 3
+
+def merge_sim_episode_config(
+    sim_config: Config, episode: Type[Episode]
+) -> Any:
+    sim_config.defrost()
+    sim_config.SCENE = episode.scene_id
+    sim_config.freeze()
+    if (
+        episode.start_position is not None
+        and episode.start_rotation is not None
+    ):
+        agent_name = sim_config.AGENTS[sim_config.DEFAULT_AGENT_ID]
+        agent_cfg = getattr(sim_config, agent_name)
+        agent_cfg.defrost()
+        agent_cfg.START_POSITION = episode.start_position
+        agent_cfg.START_ROTATION = episode.start_rotation
+        agent_cfg.IS_SET_START_STATE = True
+        agent_cfg.freeze()
+    return sim_config
+
+@attr.s(auto_attribs=True, kw_only=True)
+class PoseEstimationEpisode(Episode):
+    r"""Class for episode specification that includes initial position and
+    rotation of agent, scene name, pose reference details. An
+    episode is a description of one task instance for the agent.
+
+    Args:
+        episode_id: id of episode in the dataset, usually episode number
+        scene_id: id of scene in scene dataset
+        start_position: numpy ndarray containing 3 entries for (x, y, z)
+        start_rotation: numpy ndarray with 4 entries for (x, y, z, w)
+            elements of unit quaternion (versor) representing agent 3D
+            orientation. ref: https://en.wikipedia.org/wiki/Versor
+        pose_ref_positions: numpy ndarray containing 3 entries for (x, y, z)
+            for each pose reference - (nRef, 3)
+        pose_ref_rotations: numpy ndarray with 4 entries for (x, y, z, w)
+            for each pose reference - (nRef, 4)
+    """
+
+    pose_ref_positions: np.array = attr.ib(
+        default=None, validator=not_none_validator
+    )
+    pose_ref_rotations: np.array = attr.ib(
+        default=None, validator=not_none_validator
+    )
+
+@registry.register_sensor
+class PoseEstimationRGBSensor(Sensor):
+    r"""Sensor for PoseEstimation observations.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the PoseEstimation sensor. 
+
+    Attributes:
+        _nRef: number of pose references
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._nRef = getattr(config, "NREF")
+        super().__init__(config=config)
+
+        self.current_episode_id = None
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "pose_estimation_rgb"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0,
+            high=255,
+            shape=(self._nRef, self.config.HEIGHT, self.config.WIDTH, RGBSENSOR_DIMENSION),
+            dtype=np.uint8,
+        )
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            # Only render the pose references once
+            self.current_episode_id = episode_id
+            pose_ref_positions = episode.pose_ref_positions
+            pose_ref_rotations = episode.pose_ref_rotations
+
+            pose_ref_rgb = []
+            for position, rotation in zip(pose_ref_positions, pose_ref_rotations):
+                # Get data only from the RGB sensor
+                obs = self._sim.get_specific_sensor_observations_at(position, rotation, 'rgb')
+                # remove alpha channel
+                obs = obs[:, :, :RGBSENSOR_DIMENSION]
+                pose_ref_rgb.append(obs)
+
+            if len(pose_ref_rgb) < self._nRef:
+                dummy_image = np.zeros_like(pose_ref_rgb[0])
+                for i in range(len(pose_ref_rgb), self._nRef):
+                    pose_ref_rgb.append(dummy_image)
+
+            self._pose_ref_rgb = np.stack(pose_ref_rgb, axis=0)
+            return np.copy(self._pose_ref_rgb)
+        else:
+            return None
+
+@registry.register_sensor
+class PoseEstimationDepthSensor(Sensor):
+    r"""Sensor for PoseEstimation observations.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the PoseEstimation sensor. 
+
+    Attributes:
+        _nRef: number of pose references
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._nRef = getattr(config, "NREF")
+
+        if config.NORMALIZE_DEPTH:
+            self.min_depth_value = 0
+            self.max_depth_value = 1
+        else:
+            self.min_depth_value = config.MIN_DEPTH
+            self.max_depth_value = config.MAX_DEPTH
+
+        super().__init__(config=config)
+
+        self.current_episode_id = None
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "pose_estimation_depth"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=self.min_depth_value,
+            high=self.max_depth_value,
+            shape=(self._nRef, self.config.HEIGHT, self.config.WIDTH, 1),
+            dtype=np.float32,
+        )
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            # Only render the pose references once
+            self.current_episode_id = episode_id
+            pose_ref_positions = episode.pose_ref_positions
+            pose_ref_rotations = episode.pose_ref_rotations
+
+            pose_ref_depth = []
+            for position, rotation in zip(pose_ref_positions, pose_ref_rotations):
+                # Get data only from the Depth sensor
+                obs = self._sim.get_specific_sensor_observations_at(position, rotation, 'depth')
+
+                # Process data similar to HabitatSimDepthSensor
+                obs = np.clip(obs, self.config.MIN_DEPTH, self.config.MAX_DEPTH)
+                if self.config.NORMALIZE_DEPTH:
+                    # normalize depth observation to [0, 1]
+                    obs = (obs - self.config.MIN_DEPTH) / self.config.MAX_DEPTH
+
+                obs = np.expand_dims(obs, axis=2)  # make depth observation a 3D array
+
+                pose_ref_depth.append(obs)
+
+            if len(pose_ref_depth) < self._nRef:
+                dummy_image = np.zeros_like(pose_ref_depth[0])
+                for i in range(len(pose_ref_depth), self._nRef):
+                    pose_ref_depth.append(dummy_image)
+
+            self._pose_ref_depth = np.stack(pose_ref_depth, axis=0)
+            return np.copy(self._pose_ref_depth)
+        else:
+            return None
+
+@registry.register_sensor
+class PoseEstimationRegressSensor(Sensor):
+    r"""Sensor for PoseEstimation observations. Returns the full GT pose
+        of references w.r.t agent's starting point. Useful for evaluation
+        and rewarding.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the PoseEstimation sensor. 
+
+    Attributes:
+        _nRef: number of pose references
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._nRef = getattr(config, "NREF")
+        super().__init__(config=config)
+
+        self.current_episode_id = None
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "pose_estimation_reg"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=-1000000.0,
+            high=1000000.0,
+            shape=(self._nRef, 4),
+            dtype=np.float32,
+        )
+
+    def quaternion_to_heading(self, quaternion):
+        # quaternion - np.quaternion unit quaternion
+        # Real world rotation
+        direction_vector = np.array([0, 0, -1])
+        heading_vector = quaternion_rotate_vector(
+            quaternion.inverse(), direction_vector
+        )
+
+        phi = cartesian_to_polar(-heading_vector[2], heading_vector[0])[1]
+        return phi
+        
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            # Only render the pose references once
+            self.current_episode_id = episode_id
+            pose_ref_positions = episode.pose_ref_positions
+            pose_ref_rotations = episode.pose_ref_rotations
+            start_position = episode.start_position
+            start_rotation = episode.start_rotation
+            # Negate the sign due to my changed conventions where
+            # X is forward, Y is rightward, heading is measured from
+            # X to Y.
+            start_quat = quat_from_coeffs(start_rotation)
+            start_heading = -self.quaternion_to_heading(start_quat)
+            xs, ys = -start_position[2], start_position[0]
+
+            pose_ref_reg = []
+            for ref_position, ref_rotation in zip(pose_ref_positions, pose_ref_rotations):
+                # Negate the sign due to my changed conventions where
+                # X is forward, Y is rightward, heading is measured from
+                # X to Y.
+                ref_quat = quat_from_coeffs(ref_rotation)
+                ref_heading = -self.quaternion_to_heading(ref_quat)
+                xr, yr = -ref_position[2], ref_position[0]
+
+                # Compute vector from start to ref assuming start is 
+                # facing forward @ (0, 0)
+                rad_sr = np.sqrt((xr-xs)**2 + (yr-ys)**2)
+                phi_sr = np.arctan2(yr-ys, xr-xs) - start_heading
+                theta_sr = ref_heading - start_heading
+                # Normalize theta_sr
+                theta_sr = np.arctan2(np.sin(theta_sr), np.cos(theta_sr))
+
+                pose_ref_reg.append((rad_sr, phi_sr, theta_sr, 0.0))
+
+            if len(pose_ref_reg) < self._nRef:
+                for i in range(len(pose_ref_reg), self._nRef):
+                    pose_ref_reg.append((0.0, 0.0, 0.0, 0.0))
+
+            self._pose_ref_reg = np.array(pose_ref_reg)
+
+            return np.copy(self._pose_ref_reg)
+        else:
+            return None
+
+@registry.register_sensor
+class PoseEstimationMaskSensor(Sensor):
+    r"""Sensor for PoseEstimation observations. Returns the mask indicating which references are valid.
+    This sensor mainly places a role for the reconstruction task where different environments have 
+    different number of grid points.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the PoseEstimation sensor. 
+
+    Attributes:
+        _nRef: number of pose references
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._nRef = getattr(config, "NREF")
+        super().__init__(config=config)
+
+        self.current_episode_id = None
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "pose_estimation_mask"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=-1000000.0,
+            high=1000000.0,
+            shape=(self._nRef, ),
+            dtype=np.float32,
+        )
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            pose_ref_mask = np.ones((self._nRef, ))
+            pose_ref_mask[len(episode.pose_ref_positions):] = 0
+
+            self._pose_ref_mask = pose_ref_mask
+
+            return np.copy(self._pose_ref_mask)
+        else:
+            return None
+
+@registry.register_sensor
+class DeltaSensor(Sensor):
+    r"""Sensor that returns the odometer readings from the previous action.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the PoseEstimation sensor. 
+
+    Attributes:
+        _nRef: number of pose references
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        super().__init__(config=config)
+
+        self.current_episode_id = None
+        self.prev_position = None
+        self.prev_rotation = None
+        self.start_position = None
+        self.start_rotation = None
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "delta_sensor"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=-1000000.0,
+            high=1000000.0,
+            shape=(4, ),
+            dtype=np.float32,
+        )
+
+    def quaternion_to_heading(self, quaternion):
+        # quaternion - np.quaternion unit quaternion
+        # Real world rotation
+        direction_vector = np.array([0, 0, -1])
+        heading_vector = quaternion_rotate_vector(
+            quaternion.inverse(), direction_vector
+        )
+
+        phi = cartesian_to_polar(-heading_vector[2], heading_vector[0])[1]
+        return phi
+        
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            # A new episode has started
+            self.current_episode_id = episode_id
+            delta = np.array([0., 0., 0., 0.])
+            self.start_position = copy.deepcopy(self._sim.get_agent_state().position)
+            self.start_rotation = copy.deepcopy(self._sim.get_agent_state().rotation)
+        else:
+            current_position = self._sim.get_agent_state().position
+            current_rotation = self._sim.get_agent_state().rotation
+            # For the purposes of this sensor, forward is X and rightward is Y
+            # The heading is measured positively from X to Y
+            curr_x, curr_y = -current_position[2], current_position[0]
+            curr_heading = -self.quaternion_to_heading(current_rotation)
+            prev_x, prev_y = -self.prev_position[2], self.prev_position[0]
+            prev_heading = -self.quaternion_to_heading(self.prev_rotation)
+
+            dr = math.sqrt((curr_x - prev_x) ** 2 + (curr_y - prev_y) ** 2)
+            dphi = math.atan2(curr_y - prev_y, curr_x - prev_x)
+            dhead = curr_heading - prev_heading
+
+            # Convert these to the starting point's coordinate system
+            start_heading = -self.quaternion_to_heading(self.start_rotation)
+            dphi = dphi - start_heading
+
+            delta = np.array([dr, dphi, dhead, 0.0])
+
+        self.prev_position = copy.deepcopy(self._sim.get_agent_state().position)
+        self.prev_rotation = copy.deepcopy(self._sim.get_agent_state().rotation)
+
+        return delta
+
+@registry.register_sensor
+class OracleActionSensor(Sensor):
+    r"""Sensor that returns the shortest path action to specific targets.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the PoseEstimation sensor. 
+
+    Attributes:
+        _nRef: number of pose references
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        super().__init__(config=config)
+
+        self.current_episode_id = None
+        self.target_positions = []
+        goal_radius = config.GOAL_RADIUS
+        self.goal_radius = goal_radius
+        self.follower = ShortestPathFollower(sim, goal_radius, False)
+        self.follower.mode = "geodesic_path"
+        self.oracle_type = config.ORACLE_TYPE
+        self.num_targets = config.NUM_TARGETS
+        if self.oracle_type == 'object':
+            # This metric is used to keep track of which objects were visited.
+            # Already visited objects are removed from the target list.
+            self.objects_covered_metric = ObjectsCoveredGeometric(sim, config)
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "oracle_action_sensor"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0,
+            high=10, # Arbitrary value greater than maximum # of actions
+            shape=(1, ),
+            dtype=np.int32,
+        )
+
+    def _sample_random_targets(self, episode):
+        """
+        Samples random viewpoints as targets.
+        """
+        num_targets = self.num_targets
+        self.target_positions = []
+        agent_y = self._sim.get_agent_state().position[1]
+        for i in range(num_targets):
+            point = self._sim.sample_navigable_point()
+            point_y = point[1]
+            # Sample points within the same floor
+            while abs(agent_y - point_y) > 0.5:
+                point = self._sim.sample_navigable_point()
+                point_y = point[1]
+            self.target_positions.append(point)
+
+    def _sample_pose_targets(self, episode):
+        """
+        Samples landmark viewpoints as targets.
+        """
+        num_targets = self.num_targets
+        self.target_positions = []
+        pose_ref_positions = set([tuple(pos) for pos in episode.pose_ref_positions])
+        curr_position = episode.start_position
+
+        # Sample the pose references as targets in a greedy fashion
+        while len(pose_ref_positions) > 0:
+            min_dist = 1000000000.0
+            min_dist_position = None
+            for position in pose_ref_positions:
+                dist = self._sim.geodesic_distance(curr_position, position)
+                if dist < min_dist:
+                    min_dist = dist
+                    min_dist_position = position
+            curr_position = min_dist_position
+            # None of the other points are reachable
+            if min_dist_position is None:
+                break
+
+            self.target_positions.append(min_dist_position)
+            pose_ref_positions.remove(min_dist_position)
+
+        # Add more random targets to fill the quota of num_targets
+        nRefValid = len(self.target_positions)
+        agent_y = self._sim.get_agent_state().position[1]
+        for i in range(nRefValid, num_targets):
+            point = self._sim.sample_navigable_point()
+            point_y = point[1]
+            # Sample points within the same floor
+            while abs(agent_y - point_y) > 0.5:
+                point = self._sim.sample_navigable_point()
+                point_y = point[1]
+            self.target_positions.append(point)
+
+    def _sample_object_targets(self, episode):
+        """
+        Samples objects as targets.
+        """
+        num_targets = self.num_targets
+        self.target_positions = []
+        self.target_objects = []
+
+        obj_id_pos = set([(obj['id'], tuple(obj['center'])) for obj in self._sim.object_annotations])
+        curr_position = episode.start_position
+        start_y = curr_position[1]
+
+        # Sample the pose references as targets in a greedy fashion
+        while len(obj_id_pos) > 0:
+            min_dist = 1000000000.0
+            min_dist_id_pos = None
+            for curr_id_pos in obj_id_pos:
+                curr_pos = curr_id_pos[1]
+                dist = self._sim.geodesic_distance(curr_position, curr_pos)
+                if dist < min_dist:
+                    min_dist = dist
+                    min_dist_id_pos = curr_id_pos
+            # None of the other points are reachable
+            if min_dist_id_pos is None:
+                break
+
+            # Ignore objects that cannot be traversed to 
+            # Ignore objects that are on a different floor
+            min_dist_position = min_dist_id_pos[1]
+            min_dist_id = min_dist_id_pos[0]
+            if self._sim.is_navigable(min_dist_position) and abs(min_dist_position[1] - start_y) <= 1.0:
+                self.target_positions.append(min_dist_position)
+                self.target_objects.append(min_dist_id)
+                curr_position = min_dist_position
+            obj_id_pos.remove(min_dist_id_pos)
+
+        # Add more random targets to fill the quota of num_targets
+        nRefValid = len(self.target_positions)
+        for i in range(nRefValid, num_targets):
+            point = self._sim.sample_navigable_point()
+            while abs(point[1] - start_y) > 0.5:
+                point = self._sim.sample_navigable_point()
+            self.target_positions.append(point)
+            self.target_objects.append(None)
+
+    def _sample_targets(self, episode):
+        if self.oracle_type == 'random':
+            self._sample_random_targets(episode)
+        elif self.oracle_type == 'pose':
+            self._sample_pose_targets(episode)
+        elif self.oracle_type == 'object':
+            self._sample_object_targets(episode)
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            # A new episode has started
+            self.current_episode_id = episode_id
+            self._sample_targets(episode)
+
+        if self.oracle_type == 'object':
+            # Note - action does not matter here, so just put None
+            self.objects_covered_metric.update_metric(episode, None)
+            current_target = self.target_positions[0]
+            current_obj_id = self.target_objects[0]
+            agent_position = self._sim.get_agent_state().position
+            while (self._sim.geodesic_distance(agent_position, current_target) <= self.goal_radius) or \
+                  ((current_obj_id is not None) and current_obj_id in self.objects_covered_metric.seen_objects):
+                self.target_positions = self.target_positions[1:]
+                self.target_objects = self.target_objects[1:]
+                current_target = self.target_positions[0]
+                current_obj_id = self.target_objects[0]
+        else:
+            current_target = self.target_positions[0]
+            agent_position = self._sim.get_agent_state().position
+            while self._sim.geodesic_distance(agent_position, current_target) <= self.goal_radius:
+                self.target_positions = self.target_positions[1:]
+                current_target = self.target_positions[0]
+
+        oracle_action = self.follower.get_next_action(current_target)
+
+        return np.array([oracle_action], dtype=np.int32)
+
+@registry.register_sensor
+class CollisionSensor(Sensor):
+    def __init__(self, sim, config):
+        self._sim = sim
+        super().__init__(config=config)
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "collision_sensor"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0.0,
+            high=1.0,
+            shape=(1, ),
+            dtype=np.float32,
+        )
+
+    def get_observation(self, observations, episode):
+        if self._sim.previous_step_collided:
+            return np.array([1.0])
+        else:
+            return np.array([0.0])
+
+@registry.register_measure
+class OPSR(Measure):
+    r"""OPSR (Oracle Pose Success Rate)
+    Measures if the agent has come close to the ground truth pose.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._config = config
+        self._successes = None
+        self._geodesic_dist_thresh = config.GEODESIC_DIST_THRESH
+        self._angular_dist_thresh = config.ANGULAR_DIST_THRESH
+        self.current_episode_id = None
+        self._points_of_interest = None
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "opsr"
+
+    def reset_metric(self, episode):
+        self._metric = 0
+        nRef = len(episode.pose_ref_positions)
+        self._successes = [0.0 for _ in range(nRef)]
+
+    def quaternion_to_heading(self, quaternion):
+        # quaternion - np.quaternion unit quaternion
+        # Real world rotation
+        direction_vector = np.array([0, 0, -1])
+        heading_vector = quaternion_rotate_vector(
+            quaternion.inverse(), direction_vector
+        )
+
+        phi = cartesian_to_polar(-heading_vector[2], heading_vector[0])[1]
+        return phi
+        
+    def update_metric(self, episode, action):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            # Only render the pose references once
+            self.current_episode_id = episode_id
+            pose_ref_positions = episode.pose_ref_positions
+            pose_ref_rotations = episode.pose_ref_rotations
+
+            points_of_interest = []
+            fwd_direction_vector = np.array([0, 0, -1])
+
+            for position, rotation in zip(pose_ref_positions, pose_ref_rotations):
+                # Compute depth of the central patch of the pose reference
+                obs = self._sim.get_specific_sensor_observations_at(position, rotation, 'depth')
+                H, W = obs.shape[:2]
+                Hby2, Wby2 = H//2, W//2
+                center_depth = np.median(obs[(Hby2-5):(Hby2+6), 
+                                             (Wby2-5):(Wby2+6)]) # Raw depth in meters
+
+                # Compute 3D coordinate of the central patch
+                rotation_quat = quat_from_coeffs(rotation)
+                # This will rotate fwd_direction_vector about Y axis (vertical) in right-hand rule direction.
+                heading_vector = quaternion_rotate_vector(
+                    rotation_quat, fwd_direction_vector
+                )
+                point_of_interest = center_depth * heading_vector + np.array(position)
+                points_of_interest.append(point_of_interest)
+
+            self._points_of_interest = np.stack(points_of_interest, axis=0)
+
+        # Compute the current agent position
+        agent_state = self._sim.get_agent_state()
+        current_position = agent_state.position.tolist()
+        current_rotation = agent_state.rotation # This is np.quaternion
+
+        # Compute agent's heading angle
+        # Negate the sign due to my changed conventions where
+        # X is forward, Y is rightward, heading is measured from
+        # X to Y.
+        current_heading = -self.quaternion_to_heading(current_rotation)
+
+        # Compute depth of central patch of agent's view
+        current_rotation_list = quaternion_to_list(current_rotation)
+        curr_depth = self._sim.get_specific_sensor_observations_at(
+                         current_position,
+                         current_rotation_list,
+                         'depth'
+                     )
+        H, W = curr_depth.shape[:2]
+        Hby2, Wby2 = H//2, W//2
+        curr_center_depth = np.median(curr_depth[(Hby2-5):(Hby2+6), 
+                                                 (Wby2-5):(Wby2+6)]) # Raw depth in meters
+
+
+        # Verify that the agent is looking at the same 
+        # thing as the pose reference.
+        pose_ref_positions = episode.pose_ref_positions
+        pose_ref_rotations = np.array(episode.pose_ref_rotations)
+        for i in range(len(pose_ref_positions)):
+            distance = self._sim.geodesic_distance(
+                           current_position, pose_ref_positions[i]
+                       ) 
+
+            # First, the agent has to be close enough to the actual viewpoint.
+            if distance < self._geodesic_dist_thresh:
+                point_of_interest = self._points_of_interest[i]
+                x_poi =  -point_of_interest[2] # -Z is forward --> X
+                y_poi = point_of_interest[0] # X is rightward --> Y
+                x_curr = -current_position[2]
+                y_curr = current_position[0]
+                heading_curr2poi = math.atan2(y_poi - y_curr, x_poi - x_curr)
+                diff_angle = heading_curr2poi - current_heading
+                diff_angle = abs(math.atan2(math.sin(diff_angle), math.cos(diff_angle)))
+                diff_angle = math.degrees(diff_angle)
+
+                dist2poi = math.sqrt((x_poi - x_curr)**2 + (y_poi - y_curr)**2)
+                occlusion_err = abs(dist2poi - curr_center_depth)
+                if diff_angle < self._angular_dist_thresh and  occlusion_err < 0.5:
+                    self._successes[i] = 1
+
+        self._metric = np.sum(self._successes)
+
+@registry.register_measure
+class AreaCovered(Measure):
+    r"""Area covered metric. Returns area seen so far in the episode in terms of # of grid-cells. 
+    Can only be used in the occupancy environment.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._config = config
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "area_covered"
+
+    def reset_metric(self, episode):
+        self._metric = 0
+
+    def update_metric(self, episode, action):
+        self._metric = self._sim.occupancy_info['seen_area']
+
+@registry.register_measure
+class IncAreaCovered(Measure):
+    r"""Increase in area covered over the past action metric. 
+    Can only be used in the occupancy environment.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._config = config
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "inc_area_covered"
+
+    def reset_metric(self, episode):
+        self._metric = 0
+
+    def update_metric(self, episode, action):
+        self._metric = self._sim.occupancy_info['inc_area']
+
+@registry.register_measure
+class FracAreaCovered(Measure):
+    r"""Fraction area covered metric. Fraction of the overall environment area seen during
+    the current episode. Can only be used in the occupancy environment.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._config = config
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "frac_area_covered"
+
+    def reset_metric(self, episode):
+        self._metric = 0
+
+    def update_metric(self, episode, action):
+        self._metric = self._sim.occupancy_info['seen_area'] / float(self._sim.occupancy_info['max_area'])
+
+@registry.register_measure
+class ObjectsCoveredGeometric(Measure):
+    r"""
+    Number of objects covered (estimated based on geometric knowledge rather than
+    rendering semantic images and counting pixels)
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._config = config
+        self.current_episode_id = None
+        self.seen_objects = set()
+        self.seen_categories = set()
+        # These classes are ignored for object visitation metric.
+        self.IGNORE_CLASSES = [
+                                  'floor',
+                                  'wall',
+                                  'door',
+                                  'misc',
+                                  'ceiling',
+                                  'void',
+                                  'stairs',
+                                  'railing',
+                                  'column',
+                                  'beam',
+                                  '',
+                                  'board_panel'
+                              ]
+        self.intrinsic_matrix = self._sim.occupancy_info['intrinsic_matrix']
+        self.agent_height = self._sim.occupancy_info['agent_height']
+        self.min_depth = float(self._sim.config.DEPTH_SENSOR.MIN_DEPTH)
+        self.max_depth = float(self._sim.config.DEPTH_SENSOR.MAX_DEPTH)
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "objects_covered_geometric"
+
+    def reset_metric(self, episode):
+        self._metric = {
+                          'small_objects_visited': 0.0,
+                          'medium_objects_visited': 0.0,
+                          'large_objects_visited': 0.0,
+                          'categories_visited': 0.0,
+                       }
+        self.seen_objects = set()
+        self.seen_categories = set()
+
+    def update_metric(self, episode, action):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if episode_id != self.current_episode_id:
+            self.current_episode_id = episode_id
+            self.reset_metric(episode)
+
+        objects = self._sim.object_annotations
+        agent_position = self._sim.get_agent_state().position
+        agent_rotation = self._sim.get_agent_state().rotation
+        agent_position_list = agent_position.tolist()
+        agent_rotation_list = quaternion_to_list(agent_rotation)
+        curr_depth = self._sim.get_specific_sensor_observations_at(agent_position_list, agent_rotation_list, 'depth')
+        curr_depth = np.clip(curr_depth, self.min_depth, self.max_depth) # Raw depth values
+        curr_depth = curr_depth[..., np.newaxis]
+        for obj in objects:
+            obj_name = obj['category_name']
+            obj_dims = obj['sizes']
+            obj_id = obj['id']
+            obj_center = obj['center']
+            if obj_id in self.seen_objects:
+                continue
+            looking_flag, obj_img_pos = self.is_looking_at_object(
+                                            agent_position,
+                                            agent_rotation,
+                                            obj_center,
+                                            obj_name,
+                                            curr_depth
+                                        )
+            if looking_flag:
+                size_class = self.get_object_size_class(sorted(obj_dims))
+                self.seen_objects.add(obj_id)
+                self.seen_categories.add(obj_name)
+                if size_class == 0:
+                    self._metric['small_objects_visited'] += 1.0
+                elif size_class == 1:
+                    self._metric['medium_objects_visited'] += 1.0
+                else:
+                    self._metric['large_objects_visited'] += 1.0
+
+        self._metric['categories_visited'] = float(len(self.seen_categories))
+
+    def convert_to_camera_coords(
+        self,
+        agent_position,
+        agent_rotation,
+        target_position
+    ):
+        T_cam2world = np.eye(4)
+        T_cam2world[:3, :3] = quaternion.as_rotation_matrix(agent_rotation)
+        T_cam2world[:3, 3] = agent_position
+        target_position = np.concatenate([target_position, np.array([1.0])], axis=0)
+        # The agent's position does not account for it's height
+        target_position[1] -= self.agent_height
+
+        T_cam2img = self.intrinsic_matrix
+        T_world2cam = np.linalg.inv(T_cam2world)
+        T_world2img = np.matmul(T_cam2img, T_world2cam)
+
+        target_in_img = np.matmul(T_world2img, target_position)
+        # target_in_img = [x * depth, y * depth, -depth, 1]
+        # where x is -1 to 1 from left to right and
+        # y is 1 to -1 from top to bottom
+        target_in_img = target_in_img[:2] / -target_in_img[2]
+
+        # Flip the y to make it go from -1 to 1 from top to bottom (similar to indexing)
+        target_in_img[1] = -target_in_img[1]
+
+        # Add 1 and divide by 2 to make x go from 0 to 1 from left to right
+        # and y go from 0 to 1 from top to bottom
+        target_in_img = (target_in_img + 1)/2
+
+        return target_in_img
+
+    def is_looking_at_object(
+        self,
+        agent_position,
+        agent_rotation,
+        object_position,
+        obj_name,
+        depth
+    ):
+        # Ignore non-object classes
+        if obj_name in self.IGNORE_CLASSES:
+            return False, None
+
+        # More than 3 meters away
+        dist_to_object = math.sqrt(
+                            (agent_position[0] - object_position[0])**2 +
+                            (agent_position[2] - object_position[2])**2
+                         )
+        if dist_to_object > 3.0:
+            return False, None
+
+        # Not on the same floor
+        if abs(agent_position[1] - object_position[1]) > 1.0:
+            return False, None
+
+        direction_vector = np.array([0, 0, -1])
+        heading_vector = quaternion_rotate_vector(
+            agent_rotation.inverse(), direction_vector
+        )
+        # This is positive from -Z to -X
+        heading_angle = cartesian_to_polar(
+                            -heading_vector[2],
+                            heading_vector[0]
+                        )[1]
+
+        object_dir_vector = np.array(object_position) - np.array(agent_position)
+        # X is assumed to be -Z and Y is assumed to be -X, only then the angle is measured
+        # from -Z to -X
+        object_dir_angle = cartesian_to_polar(
+                              -object_dir_vector[2],
+                              -object_dir_vector[0]
+                           )[1]
+
+        diff_angle = heading_angle - object_dir_angle
+        diff_angle = abs(math.atan2(math.sin(diff_angle), math.cos(diff_angle)))
+
+        # Close, but not looking at it
+        if diff_angle > math.radians(60):
+            return False, None
+
+        obj_img_position = self.convert_to_camera_coords(
+                              agent_position,
+                              agent_rotation,
+                              object_position,
+                           )
+
+        # Out of the image range
+        if (
+               (obj_img_position[0] < 0.0 or obj_img_position[0] >= 1.0) or
+               (obj_img_position[1] < 0.0 or obj_img_position[1] >= 1.0)
+           ):
+            return False, None
+
+        HEIGHT, WIDTH = depth.shape[:2]
+
+        obj_img_position[0] = obj_img_position[0] * WIDTH
+        obj_img_position[1] = obj_img_position[1] * HEIGHT
+        obj_img_position = (int(obj_img_position[0]), int(obj_img_position[1]))
+
+        # Obtain depth of central pixel patch in meters
+        obj_depth_patch = depth[(obj_img_position[1]-3):(obj_img_position[1]+4),
+                          (obj_img_position[0]-3):(obj_img_position[0]+4), 0]
+
+        # Unable to sample a valid patch
+        if obj_depth_patch.shape[0] == 0 or obj_depth_patch.shape[1] == 0:
+            return False, None
+
+        obj_depth = obj_depth_patch.mean()
+
+        # Looking at it, but it is blocked
+        occlusion_error = abs(obj_depth - dist_to_object*math.cos(object_dir_angle - heading_angle))
+        if occlusion_error > 0.3:
+            return False, None
+
+        return True, obj_img_position
+
+    def get_object_size_class(self, sizes):
+        value = (sizes[1] * sizes[2]) ** (1.0 / 2.0)
+        if value < 0.5:
+            return 0
+        elif value < 1.5:
+            return 1
+        else:
+            return 2
+
+@registry.register_measure
+class TopDownMapPose(TopDownMap):
+    r"""Top Down Map measure for Pose estimation 
+    """
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "top_down_map_pose"
+
+    def draw_source_and_references(self, episode):
+        # mark source point
+        s_x, s_y = maps.to_grid(
+            episode.start_position[0],
+            episode.start_position[2],
+            self._coordinate_min,
+            self._coordinate_max,
+            self._map_resolution,
+        )
+        point_padding = 2 * int(
+            np.ceil(self._map_resolution[0] / MAP_THICKNESS_SCALAR)
+        )
+        self._top_down_map[
+            s_x - point_padding : s_x + point_padding + 1,
+            s_y - point_padding : s_y + point_padding + 1,
+        ] = maps.MAP_SOURCE_POINT_INDICATOR
+
+        # mark reference points
+        #nRef = len(episode.pose_ref_positions)
+        #for i in range(nRef):
+        #    t_x, t_y = maps.to_grid(
+        #        episode.pose_ref_positions[i][0],
+        #        episode.pose_ref_positions[i][2],
+        #        self._coordinate_min,
+        #        self._coordinate_max,
+        #        self._map_resolution,
+        #    )
+        #    self._top_down_map[
+        #        t_x - point_padding : t_x + point_padding + 1,
+        #        t_y - point_padding : t_y + point_padding + 1,
+        #    ] = maps.MAP_TARGET_POINT_INDICATOR
+
+    def reset_metric(self, episode):
+        self._step_count = 0
+        self._metric = None
+        self._top_down_map = self.get_original_map()
+        agent_position = self._sim.get_agent_state().position
+        a_x, a_y = maps.to_grid(
+            agent_position[0],
+            agent_position[2],
+            self._coordinate_min,
+            self._coordinate_max,
+            self._map_resolution,
+        )
+        self._previous_xy_location = (a_y, a_x)
+
+        self.update_fog_of_war_mask(np.array([a_x, a_y]))
+
+        # draw source and reference points last to avoid overlap
+        if self._config.DRAW_SOURCE_AND_REFERENCES:
+            self.draw_source_and_references(episode)
+
+    def update_metric(self, episode, action):
+        super().update_metric(episode, action)
+        self._metric = topdown_to_image(self._metric)
+
+@registry.register_measure
+class NoveltyReward(Measure):
+    r"""
+    Assigns rewards based on the novelty of states visited. The environment is divided
+    into uniform grids of size GRID_SIZE. Each valid grid location is considered to be a 
+    unique state. When an agent visits any location within a grid cell, the count for
+    that state is incremented. The novetly reward is given by:
+
+            r_t = 1/sqrt(n_s) 
+
+    where n_s is the visitation count for state s_t.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._config = config
+        self.current_episode_id = None
+        self._state_map = None
+        self.L_min = None
+        self.L_max = None
+        self._metric = 0.0
+        self.grid_size = config.GRID_SIZE
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "novelty_reward"
+
+    def reset_metric(self, episode):
+        self._metric = 0.0
+        self.L_min = maps.COORDINATE_MIN
+        self.L_max = maps.COORDINATE_MAX
+        map_size = int((self.L_max - self.L_min) / self.grid_size) + 1
+
+        self._state_map = np.zeros((map_size, map_size))
+
+    def _convert_to_grid(self, position):
+        """position - (x, y, z) in real-world coordinates """
+        grid_x = (position[0] - self.L_min) / self.grid_size
+        grid_y = (position[2] - self.L_min) / self.grid_size
+        grid_x = int(grid_x)
+        grid_y = int(grid_y)
+        return (grid_x, grid_y)
+
+    def update_metric(self, episode, action):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if episode_id != self.current_episode_id:
+            self.current_episode_id = episode_id
+            self.reset_metric(episode)
+
+        agent_position = self._sim.get_agent_state().position
+        grid_x, grid_y = self._convert_to_grid(agent_position)
+        self._state_map[grid_y, grid_x] += 1.0
+
+        novelty_reward = 1/math.sqrt(self._state_map[grid_y, grid_x])
+
+        self._metric = novelty_reward
+
+@registry.register_task(name="Pose-v0")
+class PoseEstimationTask(EmbodiedTask):
+    def __init__(
+        self,
+        task_config: Config,
+        sim: Simulator,
+        dataset: Optional[Dataset] = None,
+    ) -> None:
+
+        task_measurements = []
+        for measurement_name in task_config.MEASUREMENTS:
+            measurement_cfg = getattr(task_config, measurement_name)
+            measure_type = registry.get_measure(measurement_cfg.TYPE)
+            assert (
+                measure_type is not None
+            ), "invalid measurement type {}".format(measurement_cfg.TYPE)
+            task_measurements.append(measure_type(sim, measurement_cfg))
+        self.measurements = Measurements(task_measurements)
+
+        task_sensors = []
+        for sensor_name in task_config.SENSORS:
+            sensor_cfg = getattr(task_config, sensor_name)
+            sensor_type = registry.get_sensor(sensor_cfg.TYPE)
+            assert sensor_type is not None, "invalid sensor type {}".format(
+                sensor_cfg.TYPE
+            )
+            task_sensors.append(sensor_type(sim, sensor_cfg))
+
+        self.sensor_suite = SensorSuite(task_sensors)
+        super().__init__(config=task_config, sim=sim, dataset=dataset)
+
+    def overwrite_sim_config(
+        self, sim_config: Any, episode: Type[Episode]
+    ) -> Any:
+        return merge_sim_episode_config(sim_config, episode)
diff --git a/habitat/tasks/pose_estimation/shortest_path_follower.py b/habitat/tasks/pose_estimation/shortest_path_follower.py
new file mode 100644
index 0000000..44f474a
--- /dev/null
+++ b/habitat/tasks/pose_estimation/shortest_path_follower.py
@@ -0,0 +1,222 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Union
+
+import math
+import numpy as np
+
+import habitat_sim
+from habitat.core.simulator import SimulatorActions
+from habitat.sims.habitat_simulator.habitat_simulator import HabitatSim
+from habitat.utils.geometry_utils import (
+    angle_between_quaternions,
+    quaternion_from_two_vectors,
+)
+from habitat.tasks.utils import cartesian_to_polar, quaternion_rotate_vector
+
+EPSILON = 1e-6
+
+
+def action_to_one_hot(action: int) -> np.array:
+    one_hot = np.zeros(len(SimulatorActions), dtype=np.float32)
+    one_hot[action] = 1
+    return one_hot
+
+
+class ShortestPathFollower:
+    r"""Utility class for extracting the action on the shortest path to the
+        goal.
+    Args:
+        sim: HabitatSim instance.
+        goal_radius: Distance between the agent and the goal for it to be
+            considered successful.
+        return_one_hot: If true, returns a one-hot encoding of the action
+            (useful for training ML agents). If false, returns the
+            SimulatorAction.
+    """
+
+    def __init__(
+        self, sim: HabitatSim, goal_radius: float, return_one_hot: bool = True
+    ):
+        assert (
+            getattr(sim, "geodesic_distance", None) is not None
+        ), "{} must have a method called geodesic_distance".format(
+            type(sim).__name__
+        )
+
+        self._sim = sim
+        self._max_delta = self._sim.config.FORWARD_STEP_SIZE - EPSILON
+        self._goal_radius = goal_radius
+        self._step_size = self._sim.config.FORWARD_STEP_SIZE
+
+        self._mode = (
+            "geodesic_path"
+            if getattr(sim, "get_straight_shortest_path_points", None)
+            is not None
+            else "greedy"
+        )
+        self._return_one_hot = return_one_hot
+
+    def _get_return_value(self, action) -> Union[int, np.array]:
+        if self._return_one_hot:
+            return action_to_one_hot(action)
+        else:
+            return action
+
+    def get_next_action(self, goal_pos: np.array) -> Union[int, np.array]:
+        """Returns the next action along the shortest path.
+        """
+        #if (
+        #    np.linalg.norm(goal_pos - self._sim.get_agent_state().position)
+        #    <= self._goal_radius
+        #):
+        #    return self._get_return_value(SimulatorActions.STOP)
+
+        max_grad_dir = self._est_max_grad_dir(goal_pos)
+        if max_grad_dir is None:
+            return self._get_return_value(SimulatorActions.MOVE_FORWARD)
+        return self._step_along_grad(max_grad_dir)
+
+    def _quaternion_to_heading(self, quaternion):
+        # quaternion - np.quaternion unit quaternion
+        # Real world rotation
+        direction_vector = np.array([0, 0, -1])
+        heading_vector = quaternion_rotate_vector(
+            quaternion.inverse(), direction_vector
+        )
+
+        phi = cartesian_to_polar(-heading_vector[2], heading_vector[0])[1]
+        return phi
+        
+    def _step_along_grad(
+        self, grad_dir: np.quaternion
+    ) -> Union[int, np.array]:
+        current_state = self._sim.get_agent_state()
+        alpha = angle_between_quaternions(grad_dir, current_state.rotation)
+        if alpha <= np.deg2rad(self._sim.config.TURN_ANGLE) + EPSILON:
+            return self._get_return_value(SimulatorActions.MOVE_FORWARD)
+        else:
+            # These angles represent the rightward rotation from forward direction
+            grad_angle = -self._quaternion_to_heading(grad_dir)
+            curr_angle = -self._quaternion_to_heading(current_state.rotation)
+
+            diff_angle = grad_angle - curr_angle
+            diff_angle = math.atan2(math.sin(diff_angle), math.cos(diff_angle))
+            
+            if diff_angle > 0:
+                best_turn = SimulatorActions.TURN_RIGHT
+            else:
+                best_turn = SimulatorActions.TURN_LEFT
+
+            #self._sim.step(sim_action)
+            #best_turn = (
+            #    SimulatorActions.TURN_LEFT
+            #    if (
+            #        angle_between_quaternions(
+            #            grad_dir, self._sim.get_agent_state().rotation
+            #        )
+            #        < alpha
+            #    )
+            #    else SimulatorActions.TURN_RIGHT
+            #)
+            #self._reset_agent_state(current_state)
+            return self._get_return_value(best_turn)
+
+    def _reset_agent_state(self, state: habitat_sim.AgentState) -> None:
+        self._sim.set_agent_state(
+            state.position, state.rotation, reset_sensors=False
+        )
+
+    def _geo_dist(self, goal_pos: np.array) -> float:
+        return self._sim.geodesic_distance(
+            self._sim.get_agent_state().position, goal_pos
+        )
+
+    def _est_max_grad_dir(self, goal_pos: np.array) -> np.array:
+
+        current_state = self._sim.get_agent_state()
+        current_pos = current_state.position
+
+        if self.mode == "geodesic_path":
+            points = self._sim.get_straight_shortest_path_points(
+                self._sim.get_agent_state().position, goal_pos
+            )
+            # Add a little offset as things get weird if
+            # points[1] - points[0] is anti-parallel with forward
+            if len(points) < 2:
+                return None
+            max_grad_dir = quaternion_from_two_vectors(
+                self._sim.forward_vector,
+                points[1]
+                - points[0]
+                + EPSILON
+                * np.cross(self._sim.up_vector, self._sim.forward_vector),
+            )
+            max_grad_dir.x = 0
+            max_grad_dir = np.normalized(max_grad_dir)
+        else:
+            current_rotation = self._sim.get_agent_state().rotation
+            current_dist = self._geo_dist(goal_pos)
+
+            best_geodesic_delta = -2 * self._max_delta
+            best_rotation = current_rotation
+            for _ in range(0, 360, self._sim.config.TURN_ANGLE):
+                sim_action = SimulatorActions.MOVE_FORWARD
+                self._sim.step(sim_action)
+                new_delta = current_dist - self._geo_dist(goal_pos)
+
+                if new_delta > best_geodesic_delta:
+                    best_rotation = self._sim.get_agent_state().rotation
+                    best_geodesic_delta = new_delta
+
+                # If the best delta is within (1 - cos(TURN_ANGLE))% of the
+                # best delta (the step size), then we almost certainly have
+                # found the max grad dir and should just exit
+                if np.isclose(
+                    best_geodesic_delta,
+                    self._max_delta,
+                    rtol=1 - np.cos(np.deg2rad(self._sim.config.TURN_ANGLE)),
+                ):
+                    break
+
+                self._sim.set_agent_state(
+                    current_pos,
+                    self._sim.get_agent_state().rotation,
+                    reset_sensors=False,
+                )
+
+                sim_action = SimulatorActions.TURN_LEFT
+                self._sim.step(sim_action)
+
+            self._reset_agent_state(current_state)
+
+            max_grad_dir = best_rotation
+
+        return max_grad_dir
+
+    @property
+    def mode(self):
+        return self._mode
+
+    @mode.setter
+    def mode(self, new_mode: str):
+        r"""Sets the mode for how the greedy follower determines the best next
+            step.
+        Args:
+            new_mode: geodesic_path indicates using the simulator's shortest
+                path algorithm to find points on the map to navigate between.
+                greedy indicates trying to move forward at all possible
+                orientations and selecting the one which reduces the geodesic
+                distance the most.
+        """
+        assert new_mode in {"geodesic_path", "greedy"}
+        if new_mode == "geodesic_path":
+            assert (
+                getattr(self._sim, "get_straight_shortest_path_points", None)
+                is not None
+            )
+        self._mode = new_mode
diff --git a/habitat/utils/visualizations/maps.py b/habitat/utils/visualizations/maps.py
index 714fd4e..bcd010a 100644
--- a/habitat/utils/visualizations/maps.py
+++ b/habitat/utils/visualizations/maps.py
@@ -39,7 +39,7 @@ TOP_DOWN_MAP_COLORS[10:] = cv2.applyColorMap(
     np.arange(246, dtype=np.uint8), cv2.COLORMAP_JET
 ).squeeze(1)[:, ::-1]
 TOP_DOWN_MAP_COLORS[MAP_INVALID_POINT] = [255, 255, 255]
-TOP_DOWN_MAP_COLORS[MAP_VALID_POINT] = [150, 150, 150]
+TOP_DOWN_MAP_COLORS[MAP_VALID_POINT] = [50, 170, 50]
 TOP_DOWN_MAP_COLORS[MAP_BORDER_INDICATOR] = [50, 50, 50]
 TOP_DOWN_MAP_COLORS[MAP_SOURCE_POINT_INDICATOR] = [0, 0, 200]
 TOP_DOWN_MAP_COLORS[MAP_TARGET_POINT_INDICATOR] = [200, 0, 0]
@@ -198,6 +198,22 @@ def to_grid(
     return grid_x, grid_y
 
 
+def to_grid_v2(
+    realworld_x: float,
+    realworld_y: float,
+    x_min: float,
+    y_min: float,
+    map_scale: float,
+) -> Tuple[int, int]:
+    r"""Return gridworld index of realworld coordinates assuming top-left corner
+    is the origin. x_min, y_min are the minimum x and y coordinates from the environment.
+    map_scale is the real-world length that corresponds to one grid-cell in the map. 
+    """
+    grid_x = int((realworld_x - x_min) / map_scale)
+    grid_y = int((realworld_y - y_min) / map_scale)
+    return grid_x, grid_y
+
+
 def from_grid(
     grid_x: int,
     grid_y: int,
@@ -219,6 +235,23 @@ def from_grid(
     return realworld_x, realworld_y
 
 
+def from_grid_v2(
+    grid_x: int,
+    grid_y: int,
+    x_min: float,
+    y_min: float,
+    map_scale: float,
+) -> Tuple[float, float]:
+    r"""Inverse of to_grid_v2 function. Return real world coordinate from
+    gridworld assuming top-left corner is the origin. 
+    x_min, y_min are the minimum x and y coordinates from the environment.
+    map_scale is the real-world length that corresponds to one grid-cell in the map. 
+    """
+    realworld_x = x_min + grid_x * map_scale
+    realworld_y = y_min + grid_y * map_scale
+    return realworld_x, realworld_y
+
+
 def _outline_border(top_down_map):
     left_right_block_nav = (top_down_map[:, :-1] == 1) & (
         top_down_map[:, :-1] != top_down_map[:, 1:]
@@ -327,8 +360,67 @@ def get_topdown_map(
     return top_down_map
 
 
-FOG_OF_WAR_COLOR_DESAT = np.array([[0.7], [1.0]])
+def get_topdown_map_v2(
+    sim: Simulator,
+    map_extents: Tuple[int, int, int, int],
+    map_scale: float,
+    num_samples: int = 20000,
+) -> np.ndarray:
+    r"""Return a top-down occupancy map for a sim. Note, this only returns valid
+    values for whatever floor the agent is currently on.
+
+    Args:
+        sim: The simulator.
+        map_extents: (x_min, x_max, y_min, y_max) extents of the environment
+        map_scale: the real-world length corresponding to the size of a grid-cell
+        num_samples: The number of random navigable points which will be
+            initially
+            sampled. For large environments it may need to be increased.
+
+    Returns:
+        Image containing 0 if occupied, 1 if unoccupied, and 2 if border (if
+        the flag is set).
+    """
+
+    x_min, x_max, y_min, y_max = map_extents
+    map_resolution = (int((x_max - x_min)/map_scale), int((y_max - y_min)/map_scale))
+    top_down_map = np.zeros(map_resolution, dtype=np.uint8)
+
+    start_height = sim.get_agent_state().position[1]
+
+    # Use sampling to find the extrema points that might be navigable.
+    range_x = (map_resolution[0], 0)
+    range_y = (map_resolution[1], 0)
+    for _ in range(num_samples):
+        point = sim.sample_navigable_point()
+        # Check if on same level as original
+        if np.abs(start_height - point[1]) > 0.5:
+            continue
+        g_x, g_y = to_grid_v2(
+                       point[0], point[2], x_min, y_min, map_scale
+                   )
+        range_x = (min(range_x[0], g_x), max(range_x[1], g_x))
+        range_y = (min(range_y[0], g_y), max(range_y[1], g_y))
+
+    range_x = (max(0, range_x[0]), min(map_resolution[0], range_x[1]))
+    range_y = (max(0, range_y[0]), min(map_resolution[1], range_y[1]))
+
+    # Search over grid for valid points.
+    for ii in range(range_x[0], range_x[1]):
+        for jj in range(range_y[0], range_y[1]):
+            realworld_x, realworld_y = from_grid_v2(
+                ii, jj, x_min, y_min, map_scale
+            )
+            valid_point = sim.is_navigable(
+                [realworld_x, start_height, realworld_y]
+            )
+            top_down_map[ii, jj] = (
+                MAP_VALID_POINT if valid_point else MAP_INVALID_POINT
+            )
+
+    return top_down_map
 
+FOG_OF_WAR_COLOR_DESAT = np.array([[0.2], [1.0]])
 
 def colorize_topdown_map(
     top_down_map: np.ndarray, fog_of_war_mask: Optional[np.ndarray] = None
diff --git a/habitat/utils/visualizations/utils.py b/habitat/utils/visualizations/utils.py
index 48ffc02..d6c920f 100644
--- a/habitat/utils/visualizations/utils.py
+++ b/habitat/utils/visualizations/utils.py
@@ -200,3 +200,46 @@ def observations_to_image(observation: Dict, info: Dict) -> np.ndarray:
         )
         frame = np.concatenate((egocentric_view, top_down_map), axis=1)
     return frame
+
+
+def topdown_to_image(topdown_info: np.ndarray) -> np.ndarray:
+    r"""Generate image of the topdown map.
+    """
+    top_down_map = topdown_info["map"]
+    fog_of_war_mask = topdown_info["fog_of_war_mask"]
+    top_down_map = maps.colorize_topdown_map(top_down_map, fog_of_war_mask)
+    map_agent_pos = topdown_info["agent_map_coord"]
+
+    # Add zero padding
+    min_map_size = 200
+    if top_down_map.shape[0] != top_down_map.shape[1]:
+        H = top_down_map.shape[0]
+        W = top_down_map.shape[1]
+        if H > W:
+            pad_value = (H - W)//2
+            padding = ((0, 0), (pad_value, pad_value), (0, 0))
+            map_agent_pos = (map_agent_pos[0], map_agent_pos[1] + pad_value)
+        else:
+            pad_value = (W - H)//2
+            padding = ((pad_value, pad_value), (0, 0), (0, 0))
+            map_agent_pos = (map_agent_pos[0] + pad_value, map_agent_pos[1])
+        top_down_map = np.pad(top_down_map, padding, mode='constant', constant_values=255)
+
+    # Scale the map before adding the agent to ensure high-resolution for agent 
+    # irrespective of map size.
+    if top_down_map.shape[0] < min_map_size:
+        H, W = top_down_map.shape[:2]
+        top_down_map = cv2.resize(top_down_map, (min_map_size, min_map_size))
+        map_agent_pos = (
+                            int(map_agent_pos[0] * min_map_size // H),
+                            int(map_agent_pos[1] * min_map_size // W)
+                        )
+
+    top_down_map = maps.draw_agent(
+        image=top_down_map,
+        agent_center_coord=map_agent_pos,
+        agent_rotation=topdown_info["agent_angle"],
+        agent_radius_px=top_down_map.shape[0] // 16,
+    )
+
+    return top_down_map
