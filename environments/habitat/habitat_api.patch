diff --git .github/ISSUE_TEMPLATE.md .github/ISSUE_TEMPLATE.md
deleted file mode 100644
index 5eb7193..0000000
--- .github/ISSUE_TEMPLATE.md
+++ /dev/null
@@ -1,19 +0,0 @@
-## Steps to reproduce
-
-  1. _____
-  2. _____
-  3. _____
-
-## Observed Results
-
-  * What happened?  This could be a description, log output, etc.
-
-## Expected Results
-
-  * What did you expect to happen?
-
-## Relevant Code
-
-```
-// TODO(you): code here to reproduce the problem
-```
diff --git .github/PULL_REQUEST_TEMPLATE.md .github/PULL_REQUEST_TEMPLATE.md
deleted file mode 100644
index c02c7d8..0000000
--- .github/PULL_REQUEST_TEMPLATE.md
+++ /dev/null
@@ -1,29 +0,0 @@
-## Motivation and Context
-
-<!--- Why is this change required? What problem does it solve? -->
-<!--- Please link to an existing issue here if one exists. -->
-<!--- (we recommend to have an existing issue for each pull request) -->
-
-## How Has This Been Tested
-
-<!--- Please describe here how your modifications have been tested. -->
-
-## Types of changes
-
-<!--- What types of changes does your code introduce? Leave all the items that apply: -->
-- Docs change / refactoring / dependency upgrade
-- Bug fix (non-breaking change which fixes an issue)
-- New feature (non-breaking change which adds functionality)
-- Breaking change (fix or feature that would cause existing functionality to change)
-
-## Checklist
-
-<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
-<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
-- [ ] My code follows the code style of this project.
-- [ ] My change requires a change to the documentation.
-- [ ] I have updated the documentation accordingly.
-- [ ] I have read the **CONTRIBUTING** document.
-- [ ] I have completed my CLA (see **CONTRIBUTING**)
-- [ ] I have added tests to cover my changes.
-- [ ] All new and existing tests passed.
diff --git configs/datasets/pose_estimation/gibson.yaml configs/datasets/pose_estimation/gibson.yaml
new file mode 100644
index 0000000..b9cc444
--- /dev/null
+++ configs/datasets/pose_estimation/gibson.yaml
@@ -0,0 +1,4 @@
+DATASET:
+  TYPE: PoseEstimation-v1
+  SPLIT: train
+  DATA_PATH: data/datasets/pose_estimation/gibson/v1/{split}/{split}.json.gz
diff --git configs/tasks/pointnav_gibson.yaml configs/tasks/pointnav_gibson.yaml
deleted file mode 100644
index 17681b4..0000000
--- configs/tasks/pointnav_gibson.yaml
+++ /dev/null
@@ -1,28 +0,0 @@
-ENVIRONMENT:
-  MAX_EPISODE_STEPS: 500
-SIMULATOR:
-  AGENT_0:
-    SENSORS: ['RGB_SENSOR']
-  HABITAT_SIM_V0:
-    GPU_DEVICE_ID: 0
-  RGB_SENSOR:
-    WIDTH: 256
-    HEIGHT: 256
-  DEPTH_SENSOR:
-    WIDTH: 256
-    HEIGHT: 256
-TASK:
-  TYPE: Nav-v0
-  SUCCESS_DISTANCE: 0.2
-  SENSORS: ['POINTGOAL_SENSOR']
-  POINTGOAL_SENSOR:
-    TYPE: PointGoalSensor
-    GOAL_FORMAT: POLAR
-  MEASUREMENTS: ['SPL']
-  SPL:
-    TYPE: SPL
-    SUCCESS_DISTANCE: 0.2
-DATASET:
-  TYPE: PointNav-v1
-  SPLIT: train
-  DATA_PATH: data/datasets/pointnav/gibson/v1/{split}/{split}.json.gz
diff --git configs/tasks/pointnav_mp3d.yaml configs/tasks/pointnav_mp3d.yaml
deleted file mode 100644
index b22c329..0000000
--- configs/tasks/pointnav_mp3d.yaml
+++ /dev/null
@@ -1,28 +0,0 @@
-ENVIRONMENT:
-  MAX_EPISODE_STEPS: 500
-SIMULATOR:
-  AGENT_0:
-    SENSORS: ['RGB_SENSOR']
-  HABITAT_SIM_V0:
-    GPU_DEVICE_ID: 0
-  RGB_SENSOR:
-    WIDTH: 256
-    HEIGHT: 256
-  DEPTH_SENSOR:
-    WIDTH: 256
-    HEIGHT: 256
-TASK:
-  TYPE: Nav-v0
-  SUCCESS_DISTANCE: 0.2
-  SENSORS: ['POINTGOAL_SENSOR']
-  POINTGOAL_SENSOR:
-    TYPE: PointGoalSensor
-    GOAL_FORMAT: POLAR
-  MEASUREMENTS: ['SPL']
-  SPL:
-    TYPE: SPL
-    SUCCESS_DISTANCE: 0.2
-DATASET:
-  TYPE: PointNav-v1
-  SPLIT: train
-  DATA_PATH: data/datasets/pointnav/mp3d/v1/{split}/{split}.json.gz
diff --git configs/tasks/pointnav_rgbd.yaml configs/tasks/pointnav_rgbd.yaml
deleted file mode 100644
index 9635a93..0000000
--- configs/tasks/pointnav_rgbd.yaml
+++ /dev/null
@@ -1,24 +0,0 @@
-ENVIRONMENT:
-  MAX_EPISODE_STEPS: 500
-SIMULATOR:
-  AGENT_0:
-    SENSORS: ['RGB_SENSOR', 'DEPTH_SENSOR']
-  HABITAT_SIM_V0:
-    GPU_DEVICE_ID: 0
-  RGB_SENSOR:
-    WIDTH: 256
-    HEIGHT: 256
-  DEPTH_SENSOR:
-    WIDTH: 256
-    HEIGHT: 256
-TASK:
-  TYPE: Nav-v0
-  SUCCESS_DISTANCE: 0.2
-  SENSORS: ['POINTGOAL_SENSOR']
-  POINTGOAL_SENSOR:
-    TYPE: PointGoalSensor
-    GOAL_FORMAT: POLAR
-  MEASUREMENTS: ['SPL']
-  SPL:
-    TYPE: SPL
-    SUCCESS_DISTANCE: 0.2
diff --git configs/test/habitat_poseestimation_test.yaml configs/test/habitat_poseestimation_test.yaml
new file mode 100644
index 0000000..7bf662d
--- /dev/null
+++ configs/test/habitat_poseestimation_test.yaml
@@ -0,0 +1,30 @@
+ENVIRONMENT:
+  MAX_EPISODE_STEPS: 10
+SIMULATOR:
+  AGENT_0:
+    SENSORS: ['RGB_SENSOR', 'DEPTH_SENSOR']
+  RGB_SENSOR:
+    WIDTH: 84
+    HEIGHT: 84
+  DEPTH_SENSOR:
+    WIDTH: 84
+    HEIGHT: 84
+DATASET:
+  TYPE: PoseEstimation-v1
+  SPLIT: val
+  DATA_PATH: data/datasets/pose_estimation/gibson/v1/{split}/{split}.json.gz
+TASK:
+  TYPE: Pose-v0
+  SENSORS: ['POSE_RGB_SENSOR', 'POSE_DEPTH_SENSOR', 'POSE_REGRESS_SENSOR']
+  POSE_RGB_SENSOR:
+    TYPE: PoseEstimationRGBSensor
+    NREF: 20
+  POSE_DEPTH_SENSOR:
+    TYPE: PoseEstimationDepthSensor
+    NREF: 20
+  POSE_REGRESS_SENSOR:
+    TYPE: PoseEstimationRegressSensor
+    NREF: 20
+  MEASUREMENTS: ['OPSR', 'TOP_DOWN_MAP_POSE']
+  OPSR:
+    TYPE: OPSR
diff --git data_generation_scripts/configs/semantic_objects/mp3d_objects_test.yaml data_generation_scripts/configs/semantic_objects/mp3d_objects_test.yaml
new file mode 100644
index 0000000..410c0ce
--- /dev/null
+++ data_generation_scripts/configs/semantic_objects/mp3d_objects_test.yaml
@@ -0,0 +1,47 @@
+ENVIRONMENT:
+  MAX_EPISODE_STEPS: 501
+SIMULATOR:
+  TYPE: "Sim-v1"
+  AGENT_0:
+    SENSORS: ['RGB_SENSOR', 'DEPTH_SENSOR', 'FINE_OCC_SENSOR', 'COARSE_OCC_SENSOR']
+  ACTION_SPACE_CONFIG: "v2"
+  HABITAT_SIM_V0:
+    GPU_DEVICE_ID: 0
+  RGB_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  DEPTH_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  OCCUPANCY_MAPS:
+    MAP_SCALE: 0.05
+    MAP_SIZE: 800
+    MAX_DEPTH: 3
+    SMALL_MAP_RANGE: 40
+    LARGE_MAP_RANGE: 200
+    HEIGHT_LOWER: 0.5
+    HEIGHT_UPPER: 2.0
+  FINE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  COARSE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+TASK:
+  TYPE: Pose-v0
+  SENSORS: ['POSE_RGB_SENSOR', 'POSE_REGRESS_SENSOR', 'DELTA_SENSOR', 'ORACLE_ACTION_SENSOR']
+  POSE_RGB_SENSOR:
+    NREF: 20
+  POSE_REGRESS_SENSOR:
+    NREF: 20
+  ORACLE_ACTION_SENSOR:
+    ORACLE_TYPE: 'random'
+  MEASUREMENTS: ['OPSR', 'AREA_COVERED', 'INC_AREA_COVERED', 'TOP_DOWN_MAP_POSE']
+  OPSR:
+    TYPE: OPSR
+  TOP_DOWN_MAP_POSE:
+    TYPE: TopDownMapPose
+DATASET:
+  TYPE: PoseEstimation-v1
+  SPLIT: test
+  DATA_PATH: data/datasets/pose_estimation/mp3d/v1_unique/{split}/{split}.json.gz
diff --git data_generation_scripts/configs/semantic_objects/mp3d_objects_train.yaml data_generation_scripts/configs/semantic_objects/mp3d_objects_train.yaml
new file mode 100644
index 0000000..c0e768e
--- /dev/null
+++ data_generation_scripts/configs/semantic_objects/mp3d_objects_train.yaml
@@ -0,0 +1,47 @@
+ENVIRONMENT:
+  MAX_EPISODE_STEPS: 501
+SIMULATOR:
+  TYPE: "Sim-v1"
+  AGENT_0:
+    SENSORS: ['RGB_SENSOR', 'DEPTH_SENSOR', 'FINE_OCC_SENSOR', 'COARSE_OCC_SENSOR']
+  ACTION_SPACE_CONFIG: "v2"
+  HABITAT_SIM_V0:
+    GPU_DEVICE_ID: 0
+  RGB_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  DEPTH_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  OCCUPANCY_MAPS:
+    MAP_SCALE: 0.05
+    MAP_SIZE: 800
+    MAX_DEPTH: 3
+    SMALL_MAP_RANGE: 40
+    LARGE_MAP_RANGE: 200
+    HEIGHT_LOWER: 0.5
+    HEIGHT_UPPER: 2.0
+  FINE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  COARSE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+TASK:
+  TYPE: Pose-v0
+  SENSORS: ['POSE_RGB_SENSOR', 'POSE_REGRESS_SENSOR', 'DELTA_SENSOR', 'ORACLE_ACTION_SENSOR']
+  POSE_RGB_SENSOR:
+    NREF: 20
+  POSE_REGRESS_SENSOR:
+    NREF: 20
+  ORACLE_ACTION_SENSOR:
+    ORACLE_TYPE: 'random'
+  MEASUREMENTS: ['OPSR', 'AREA_COVERED', 'INC_AREA_COVERED', 'TOP_DOWN_MAP_POSE']
+  OPSR:
+    TYPE: OPSR
+  TOP_DOWN_MAP_POSE:
+    TYPE: TopDownMapPose
+DATASET:
+  TYPE: PoseEstimation-v1
+  SPLIT: train
+  DATA_PATH: data/datasets/pose_estimation/mp3d/v1_unique/{split}/{split}.json.gz
diff --git data_generation_scripts/configs/semantic_objects/mp3d_objects_val.yaml data_generation_scripts/configs/semantic_objects/mp3d_objects_val.yaml
new file mode 100644
index 0000000..089cee8
--- /dev/null
+++ data_generation_scripts/configs/semantic_objects/mp3d_objects_val.yaml
@@ -0,0 +1,47 @@
+ENVIRONMENT:
+  MAX_EPISODE_STEPS: 501
+SIMULATOR:
+  TYPE: "Sim-v1"
+  AGENT_0:
+    SENSORS: ['RGB_SENSOR', 'DEPTH_SENSOR', 'FINE_OCC_SENSOR', 'COARSE_OCC_SENSOR']
+  ACTION_SPACE_CONFIG: "v2"
+  HABITAT_SIM_V0:
+    GPU_DEVICE_ID: 0
+  RGB_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  DEPTH_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  OCCUPANCY_MAPS:
+    MAP_SCALE: 0.05
+    MAP_SIZE: 800
+    MAX_DEPTH: 3
+    SMALL_MAP_RANGE: 40
+    LARGE_MAP_RANGE: 200
+    HEIGHT_LOWER: 0.5
+    HEIGHT_UPPER: 2.0
+  FINE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  COARSE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+TASK:
+  TYPE: Pose-v0
+  SENSORS: ['POSE_RGB_SENSOR', 'POSE_REGRESS_SENSOR', 'DELTA_SENSOR', 'ORACLE_ACTION_SENSOR']
+  POSE_RGB_SENSOR:
+    NREF: 20
+  POSE_REGRESS_SENSOR:
+    NREF: 20
+  ORACLE_ACTION_SENSOR:
+    ORACLE_TYPE: 'random'
+  MEASUREMENTS: ['OPSR', 'AREA_COVERED', 'INC_AREA_COVERED', 'TOP_DOWN_MAP_POSE']
+  OPSR:
+    TYPE: OPSR
+  TOP_DOWN_MAP_POSE:
+    TYPE: TopDownMapPose
+DATASET:
+  TYPE: PoseEstimation-v1
+  SPLIT: val
+  DATA_PATH: data/datasets/pose_estimation/mp3d/v1_unique/{split}/{split}.json.gz
diff --git data_generation_scripts/extract_object_annotations_per_env.py data_generation_scripts/extract_object_annotations_per_env.py
new file mode 100644
index 0000000..c854950
--- /dev/null
+++ data_generation_scripts/extract_object_annotations_per_env.py
@@ -0,0 +1,66 @@
+import os
+import pdb
+import json
+import gzip
+import habitat
+import progressbar
+
+IGNORE_CLASSES = [
+    "floor",
+    "wall",
+    "door",
+    "misc",
+    "ceiling",
+    "void",
+    "stairs",
+    "railing",
+    "column",
+    "beam",
+    "",
+    "board_panel",
+]
+
+
+def safe_mkdir(path):
+    try:
+        os.mkdir(path)
+    except:
+        pass
+
+
+safe_mkdir("data/object_annotations")
+safe_mkdir("data/object_annotations/mp3d")
+save_dir = "data/object_annotations/mp3d"
+
+object_counts = {}
+for split in ["val", "test", "train"]:
+    config_path = (
+        f"data_generation_scripts/configs/semantic_objects/mp3d_objects_{split}.yaml"
+    )
+    config = habitat.get_config_pose(config_path)
+
+    env = habitat.Env(config=config)
+    env.seed(1234)
+
+    num_episodes = len(env._dataset.episodes)
+
+    for epcount in progressbar.progressbar(range(num_episodes)):
+        obs = env.reset()
+        semantic_scene = env._sim._sim.semantic_scene
+        semantic_objects = [
+            {
+                "center": obj.aabb.center.tolist(),
+                "sizes": obj.aabb.sizes.tolist(),
+                "id": obj.id,
+                "category_name": obj.category.name(),
+                "category_idx": obj.category.index(),
+            }
+            for obj in semantic_scene.objects
+            if obj.category.name() not in IGNORE_CLASSES
+        ]
+        scene_id = env._current_episode.scene_id.split("/")[-1]
+        save_path = os.path.join(save_dir, scene_id + ".json.gz")
+        with gzip.open(save_path, "wt") as fp:
+            json.dump(semantic_objects, fp)
+
+    env.close()
diff --git demos/.gitignore demos/.gitignore
new file mode 100644
index 0000000..b38a525
--- /dev/null
+++ demos/.gitignore
@@ -0,0 +1,5 @@
+# Byte-compiled / optimized / DLL files
+__pycache__/
+
+# vim
+*.swp
diff --git demos/configs/exploration_mp3d.yaml demos/configs/exploration_mp3d.yaml
new file mode 100644
index 0000000..d23e059
--- /dev/null
+++ demos/configs/exploration_mp3d.yaml
@@ -0,0 +1,57 @@
+ENVIRONMENT:
+  MAX_EPISODE_STEPS: 201
+SIMULATOR:
+  TYPE: "Sim-v1"
+  AGENT_0:
+    SENSORS: ['RGB_SENSOR', 'DEPTH_SENSOR', 'FINE_OCC_SENSOR', 'COARSE_OCC_SENSOR']
+  ACTION_SPACE_CONFIG: "v2"
+  HABITAT_SIM_V0:
+    GPU_DEVICE_ID: 0
+  RGB_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  DEPTH_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  OCCUPANCY_MAPS:
+    MAP_SCALE: 0.05
+    MAP_SIZE: 800
+    MAX_DEPTH: 3
+    SMALL_MAP_RANGE: 40
+    LARGE_MAP_RANGE: 200
+    HEIGHT_LOWER: 0.2
+    HEIGHT_UPPER: 1.5
+    USE_GT_OCC_MAP: False
+    MEASURE_NOISE_FREE_AREA: True
+  FINE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  COARSE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  OBJECT_ANNOTATIONS:
+    IS_AVAILABLE: True
+    PATH: 'data/object_annotations/mp3d'
+  ENABLE_ODOMETRY_NOISE: False
+  ODOMETER_NOISE_SCALING: 0.2
+TASK:
+  TYPE: Pose-v0
+  SENSORS: ['ORACLE_ACTION_SENSOR']
+  ORACLE_ACTION_SENSOR:
+    ORACLE_TYPE: 'random'
+  MEASUREMENTS: ['TOP_DOWN_MAP_POSE', 'AREA_COVERED', 'OBJECTS_COVERED_GEOMETRIC', 'OPSR', 'NOVELTY_REWARD', 'COVERAGE_NOVELTY_REWARD']
+  TOP_DOWN_MAP_POSE:
+    TYPE: TopDownMapPose
+    FOG_OF_WAR:
+        DRAW: True
+        FOV: 90
+        VISIBILITY_DIST: 4.0
+  OPSR:
+    TYPE: OPSR
+    GEODESIC_DIST_THRESH: 2.0 # Meters
+    ANGULAR_DIST_THRESH: 20.0 # Degrees
+DATASET:
+  TYPE: PoseEstimation-v1
+  SPLIT: test
+  DATA_PATH: data/datasets/pose_estimation/mp3d/v1/{split}/{split}.json.gz
+  SHUFFLE_DATASET: False
diff --git demos/configs/pointnav_mp3d.yaml demos/configs/pointnav_mp3d.yaml
new file mode 100644
index 0000000..5b3d659
--- /dev/null
+++ demos/configs/pointnav_mp3d.yaml
@@ -0,0 +1,57 @@
+ENVIRONMENT:
+  MAX_EPISODE_STEPS: 700
+  T_EXP: 500
+  T_NAV: 200
+SIMULATOR:
+  TYPE: "Sim-v1"
+  AGENT_0:
+    SENSORS: ['RGB_SENSOR', 'DEPTH_SENSOR', 'FINE_OCC_SENSOR', 'COARSE_OCC_SENSOR', 'HIGHRES_COARSE_OCC_SENSOR']
+  ACTION_SPACE_CONFIG: "v2"
+  HABITAT_SIM_V0:
+    GPU_DEVICE_ID: 0
+  RGB_SENSOR:
+    WIDTH: 84
+    HEIGHT: 84
+  DEPTH_SENSOR:
+    WIDTH: 84
+    HEIGHT: 84
+  OCCUPANCY_MAPS:
+    MAP_SCALE: 0.05
+    MAP_SIZE: 800
+    MAX_DEPTH: 3
+    SMALL_MAP_RANGE: 40
+    LARGE_MAP_RANGE: 200
+    HEIGHT_LOWER: 0.5
+    HEIGHT_UPPER: 2.0
+  FINE_OCC_SENSOR:
+    WIDTH: 84
+    HEIGHT: 84
+  COARSE_OCC_SENSOR:
+    WIDTH: 84
+    HEIGHT: 84
+  HIGHRES_COARSE_OCC_SENSOR:
+    WIDTH: 800
+    HEIGHT: 800
+TASK:
+  TYPE: ExpNav-v0
+  SUCCESS_DISTANCE: 0.2
+  SENSORS: ['ORACLE_ACTION_SENSOR', 'GRID_GOAL_SENSOR', 'SP_ACTION_SENSOR']
+  ORACLE_ACTION_SENSOR:
+    ORACLE_TYPE: 'random'
+  GRID_GOAL_SENSOR:
+    TYPE: GridGoalSensorExploreNavigation
+    T_EXP: 500
+    T_NAV: 200
+  SP_ACTION_SENSOR:
+    TYPE: SPActionSensorExploreNavigation
+    T_EXP: 500
+    T_NAV: 200
+  MEASUREMENTS: ['TOP_DOWN_MAP_EXP_NAV']
+  TOP_DOWN_MAP_EXP_NAV:
+    T_EXP: 500
+    T_NAV: 200
+DATASET:
+  TYPE: ExpNav-v1
+  SPLIT: test
+  DATA_PATH: <HABITAT_ROOT>/habitat-api/data/datasets/exp_nav/mp3d/v1/{split}/{split}.json.gz
+  SHUFFLE_DATASET: False
diff --git demos/configs/pose_estimation_mp3d.yaml demos/configs/pose_estimation_mp3d.yaml
new file mode 100644
index 0000000..400a6a6
--- /dev/null
+++ demos/configs/pose_estimation_mp3d.yaml
@@ -0,0 +1,51 @@
+ENVIRONMENT:
+  MAX_EPISODE_STEPS: 501
+SIMULATOR:
+  TYPE: "Sim-v1"
+  AGENT_0:
+    SENSORS: ['RGB_SENSOR', 'DEPTH_SENSOR', 'FINE_OCC_SENSOR', 'COARSE_OCC_SENSOR']
+  ACTION_SPACE_CONFIG: "v2"
+  HABITAT_SIM_V0:
+    GPU_DEVICE_ID: 0
+  RGB_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  DEPTH_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  OCCUPANCY_MAPS:
+    MAP_SCALE: 0.05
+    MAP_SIZE: 800
+    MAX_DEPTH: 3
+    SMALL_MAP_RANGE: 40
+    LARGE_MAP_RANGE: 200
+    HEIGHT_LOWER: 0.5
+    HEIGHT_UPPER: 2.0
+    USE_GT_OCC_MAP: True
+  FINE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  COARSE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+TASK:
+  TYPE: Pose-v0
+  SENSORS: ['POSE_RGB_SENSOR', 'POSE_REGRESS_SENSOR', 'ORACLE_ACTION_SENSOR']
+  POSE_RGB_SENSOR:
+    NREF: 100
+  POSE_REGRESS_SENSOR:
+    NREF: 100
+  ORACLE_ACTION_SENSOR:
+    ORACLE_TYPE: 'random'
+  MEASUREMENTS: ['TOP_DOWN_MAP_POSE']
+  TOP_DOWN_MAP_POSE:
+    TYPE: TopDownMapPose
+    FOG_OF_WAR:
+        DRAW: True
+        FOV: 90
+        VISIBILITY_DIST: 4.0
+DATASET:
+  TYPE: PoseEstimation-v1
+  SPLIT: test
+  DATA_PATH: <HABITAT_ROOT>/habitat-api/data/datasets/pose_estimation/mp3d/v1/{split}/{split}.json.gz
+  SHUFFLE_DATASET: False
diff --git demos/configs/reconstruction_mp3d.yaml demos/configs/reconstruction_mp3d.yaml
new file mode 100644
index 0000000..b06f69c
--- /dev/null
+++ demos/configs/reconstruction_mp3d.yaml
@@ -0,0 +1,49 @@
+ENVIRONMENT:
+  MAX_EPISODE_STEPS: 501
+SIMULATOR:
+  TYPE: "Sim-v1"
+  AGENT_0:
+    SENSORS: ['RGB_SENSOR', 'DEPTH_SENSOR', 'FINE_OCC_SENSOR', 'COARSE_OCC_SENSOR']
+  ACTION_SPACE_CONFIG: "v2"
+  HABITAT_SIM_V0:
+    GPU_DEVICE_ID: 0
+  RGB_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  DEPTH_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  OCCUPANCY_MAPS:
+    MAP_SCALE: 0.05
+    MAP_SIZE: 800
+    MAX_DEPTH: 3
+    SMALL_MAP_RANGE: 40
+    LARGE_MAP_RANGE: 200
+    HEIGHT_LOWER: 0.5
+    HEIGHT_UPPER: 2.0
+    USE_GT_OCC_MAP: True
+  FINE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+  COARSE_OCC_SENSOR:
+    WIDTH: 400
+    HEIGHT: 400
+TASK:
+  TYPE: Pose-v0
+  SENSORS: ['POSE_RGB_SENSOR', 'POSE_REGRESS_SENSOR', 'ORACLE_ACTION_SENSOR']
+  POSE_RGB_SENSOR:
+    NREF: 100
+  POSE_REGRESS_SENSOR:
+    NREF: 100
+  MEASUREMENTS: ['TOP_DOWN_MAP_POSE']
+  TOP_DOWN_MAP_POSE:
+    TYPE: TopDownMapPose
+    FOG_OF_WAR:
+        DRAW: True
+        FOV: 90
+        VISIBILITY_DIST: 4.0
+DATASET:
+  TYPE: PoseEstimation-v1
+  SPLIT: test
+  DATA_PATH: <HABITAT_ROOT>/habitat-api/data/datasets/uniform_pose_estimation/mp3d/v1/{split}/{split}.json.gz
+  SHUFFLE_DATASET: False
diff --git demos/exploration_demo.py demos/exploration_demo.py
new file mode 100644
index 0000000..1e08b6b
--- /dev/null
+++ demos/exploration_demo.py
@@ -0,0 +1,83 @@
+import cv2
+import pdb
+import math
+import numpy as np
+from utils import *
+
+import habitat
+
+
+class DummyRLEnv(habitat.RLEnv):
+    def __init__(self, config, dataset=None, env_ind=0):
+        super(DummyRLEnv, self).__init__(config, dataset)
+        self._env_ind = env_ind
+
+    def get_reward_range(self):
+        return -1.0, 1.0
+
+    def get_reward(self, observations):
+        return 0.0
+
+    def get_done(self, observations):
+        done = False
+        if self._env.episode_over:
+            done = True
+        return done
+
+    def get_info(self, observations):
+        return self.habitat_env.get_metrics()
+
+    def get_env_ind(self):
+        return self._env_ind
+
+
+config = habitat.get_config_pose("demos/configs/exploration_mp3d.yaml")
+
+env = DummyRLEnv(config=config)
+env.seed(1234)
+
+obs = env.reset()
+
+"""
+Action space:
+    MOVE_FORWARD = 0
+    TURN_LEFT = 1
+    TURN_RIGHT = 2
+    STOP = 3
+"""
+
+steps = 0
+while True:
+    action = obs["oracle_action_sensor"][0].item()
+    obs, reward, done, info = env.step(action)
+    if done:
+        obs = env.reset()
+        steps = 0
+
+    rgb_im = proc_rgb(obs["rgb"])
+    fine_occ_im = proc_rgb(obs["fine_occupancy"])
+    coarse_occ_im = proc_rgb(obs["coarse_occupancy"])
+    topdown_im = proc_rgb(info["top_down_map_pose"])
+    objects_covered = (
+        info["objects_covered_geometric"]["small_objects_visited"]
+        + info["objects_covered_geometric"]["medium_objects_visited"]
+        + info["objects_covered_geometric"]["large_objects_visited"]
+    )
+
+    print(f"===========> Steps: {steps:3d}")
+    metrics_to_print = {
+        "Area covered (m^2)": info["area_covered"],
+        "Objects covered": objects_covered,
+        "Landmarks covered": info["opsr"],
+        "Novelty": info["novelty_reward"],
+        "Smooth coverage": info["coverage_novelty_reward"],
+    }
+    for k, v in metrics_to_print.items():
+        print(f"{k:<25s}: {v:6.2f}")
+
+    steps += 1
+    cv2.imshow(
+        "Exploration demo",
+        np.concatenate([rgb_im, fine_occ_im, coarse_occ_im, topdown_im], axis=1),
+    )
+    cv2.waitKey(30)
diff --git demos/pointnav_demo.py demos/pointnav_demo.py
new file mode 100644
index 0000000..08fe5dc
--- /dev/null
+++ demos/pointnav_demo.py
@@ -0,0 +1,109 @@
+import cv2
+import pdb
+import math
+import numpy as np
+from utils import *
+
+import habitat
+
+
+class DummyRLEnv(habitat.RLEnv):
+    def __init__(self, config, dataset=None, env_ind=0):
+        super(DummyRLEnv, self).__init__(config, dataset)
+        self.T_exp = config.ENVIRONMENT.T_EXP
+        self.T_nav = config.ENVIRONMENT.T_NAV
+        assert self.T_exp + self.T_nav == config.ENVIRONMENT.MAX_EPISODE_STEPS
+        self._env_ind = env_ind
+
+    def step(self, action):
+        observations, reward, done, info = super().step(action)
+        if self._env._elapsed_steps == self.T_exp:
+            observations = self._respawn_agent()
+
+        return observations, reward, done, info
+
+    def _respawn_agent(self):
+        position = self.habitat_env.current_episode.start_nav_position
+        rotation = self.habitat_env.current_episode.start_nav_rotation
+        observations = self.habitat_env._sim.get_observations_at(
+            position, rotation, keep_agent_at_new_pose=True
+        )
+
+        observations.update(
+            self.habitat_env.task.sensor_suite.get_observations(
+                observations=observations, episode=self.current_episode
+            )
+        )
+
+        return observations
+
+    def get_reward_range(self):
+        return -1.0, 1.0
+
+    def get_reward(self, observations):
+        return 0.0
+
+    def get_done(self, observations):
+        done = False
+        if self._env.episode_over:
+            done = True
+        return done
+
+    def get_info(self, observations):
+        return self.habitat_env.get_metrics()
+
+    def get_env_ind(self):
+        return self._env_ind
+
+
+config = habitat.get_config_exp_nav("demos/configs/pointnav_mp3d.yaml")
+goal_radius = config.SIMULATOR.FORWARD_STEP_SIZE
+
+env = DummyRLEnv(config=config)
+env.seed(1234)
+
+"""
+Action space:
+    MOVE_FORWARD = 0
+    TURN_LEFT = 1
+    TURN_RIGHT = 2
+    STOP = 3
+"""
+
+for i in range(10):
+    obs = env.reset()
+    for t in range(config.ENVIRONMENT.MAX_EPISODE_STEPS):
+        if t < config.ENVIRONMENT.T_EXP:
+            action = obs["oracle_action_sensor"][0].item()
+        else:
+            action = obs["sp_action_sensor_exp_nav"][0].item()
+
+        obs, reward, done, info = env.step(action)
+
+        if done:
+            cv2.destroyWindow("PointNav: navigation phase")
+            break
+
+        rgb_im = proc_rgb(obs["rgb"])
+        fine_occ_im = proc_rgb(obs["fine_occupancy"])
+        coarse_occ_im = proc_rgb(obs["highres_coarse_occupancy"])
+        topdown_im = proc_rgb(info["top_down_map_exp_nav"])
+
+        if t < config.ENVIRONMENT.T_EXP:
+            cv2.imshow(
+                "PointNav: exploration phase",
+                np.concatenate(
+                    [rgb_im, fine_occ_im, coarse_occ_im, topdown_im], axis=1
+                ),
+            )
+        else:
+            if t == config.ENVIRONMENT.T_EXP:
+                cv2.destroyWindow("PointNav: exploration phase")
+            cv2.imshow(
+                "PointNav: navigation phase",
+                np.concatenate(
+                    [rgb_im, fine_occ_im, coarse_occ_im, topdown_im], axis=1
+                ),
+            )
+
+        cv2.waitKey(150)
diff --git demos/pose_estimation_demo.py demos/pose_estimation_demo.py
new file mode 100644
index 0000000..71f31ae
--- /dev/null
+++ demos/pose_estimation_demo.py
@@ -0,0 +1,93 @@
+import cv2
+import pdb
+import math
+import numpy as np
+
+from utils import *
+
+import habitat
+
+
+class DummyRLEnv(habitat.RLEnv):
+    def __init__(self, config, dataset=None, env_ind=0):
+        super(DummyRLEnv, self).__init__(config, dataset)
+        self._env_ind = env_ind
+
+    def get_reward_range(self):
+        return -1.0, 1.0
+
+    def get_reward(self, observations):
+        return 0.0
+
+    def get_done(self, observations):
+        done = False
+        if self._env.episode_over:
+            done = True
+        return done
+
+    def get_info(self, observations):
+        return self.habitat_env.get_metrics()
+
+    def get_env_ind(self):
+        return self._env_ind
+
+
+def create_reference_grid(refs_uint8):
+    """
+    Inputs:
+        refs_uint8 - (nRef, H, W, C) numpy array
+    """
+    refs_uint8 = np.copy(refs_uint8)
+    nRef, H, W, C = refs_uint8.shape
+
+    nrow = int(math.sqrt(nRef))
+
+    ncol = nRef // nrow  # (number of images per column)
+    if nrow * ncol < nRef:
+        ncol += 1
+    final_grid = np.zeros((nrow * ncol, *refs_uint8.shape[1:]), dtype=np.uint8)
+    font = cv2.FONT_HERSHEY_SIMPLEX
+
+    final_grid[:nRef] = refs_uint8
+    final_grid = final_grid.reshape(
+        ncol, nrow, *final_grid.shape[1:]
+    )  # (ncol, nrow, H, W, C)
+    final_grid = final_grid.transpose(0, 2, 1, 3, 4)
+    final_grid = final_grid.reshape(ncol * H, nrow * W, C)
+    return final_grid
+
+
+config = habitat.get_config_pose("demos/configs/pose_estimation_mp3d.yaml")
+goal_radius = config.SIMULATOR.FORWARD_STEP_SIZE
+
+env = DummyRLEnv(config=config)
+env.seed(1234)
+
+obs = env.reset()
+
+"""
+Action space:
+    MOVE_FORWARD = 0
+    TURN_LEFT = 1
+    TURN_RIGHT = 2
+    STOP = 3
+"""
+
+pose_refs_rgb = proc_rgb(create_reference_grid(obs["pose_estimation_rgb"]))
+
+while True:
+    action = obs["oracle_action_sensor"][0].item()
+    obs, reward, done, info = env.step(action)
+
+    if done:
+        obs = env.reset()
+        pose_refs_rgb = proc_rgb(create_reference_grid(obs["pose_estimation_rgb"]))
+
+    rgb_im = proc_rgb(obs["rgb"])
+    topdown_im = proc_rgb(info["top_down_map_pose"])
+
+    cv2.imshow(
+        "Pose estimation demo",
+        np.concatenate([rgb_im, topdown_im, pose_refs_rgb], axis=1),
+    )
+    cv2.waitKey(60)
diff --git demos/reconstruction_demo.py demos/reconstruction_demo.py
new file mode 100644
index 0000000..2550467
--- /dev/null
+++ demos/reconstruction_demo.py
@@ -0,0 +1,93 @@
+import cv2
+import pdb
+import math
+import numpy as np
+
+from utils import *
+
+import habitat
+
+
+class DummyRLEnv(habitat.RLEnv):
+    def __init__(self, config, dataset=None, env_ind=0):
+        super(DummyRLEnv, self).__init__(config, dataset)
+        self._env_ind = env_ind
+
+    def get_reward_range(self):
+        return -1.0, 1.0
+
+    def get_reward(self, observations):
+        return 0.0
+
+    def get_done(self, observations):
+        done = False
+        if self._env.episode_over:
+            done = True
+        return done
+
+    def get_info(self, observations):
+        return self.habitat_env.get_metrics()
+
+    def get_env_ind(self):
+        return self._env_ind
+
+
+def create_reference_grid(refs_uint8):
+    """
+    Inputs:
+        refs_uint8 - (nRef, H, W, C) numpy array
+    """
+    refs_uint8 = np.copy(refs_uint8)
+    nRef, H, W, C = refs_uint8.shape
+
+    nrow = int(math.sqrt(nRef))
+
+    ncol = nRef // nrow  # (number of images per column)
+    if nrow * ncol < nRef:
+        ncol += 1
+    final_grid = np.zeros((nrow * ncol, *refs_uint8.shape[1:]), dtype=np.uint8)
+    font = cv2.FONT_HERSHEY_SIMPLEX
+
+    final_grid[:nRef] = refs_uint8
+    final_grid = final_grid.reshape(
+        ncol, nrow, *final_grid.shape[1:]
+    )  # (ncol, nrow, H, W, C)
+    final_grid = final_grid.transpose(0, 2, 1, 3, 4)
+    final_grid = final_grid.reshape(ncol * H, nrow * W, C)
+    return final_grid
+
+
+config = habitat.get_config_pose("demos/config/pose_estimation_mp3d.yaml")
+goal_radius = config.SIMULATOR.FORWARD_STEP_SIZE
+
+env = DummyRLEnv(config=config)
+env.seed(1234)
+
+obs = env.reset()
+
+"""
+Action space:
+    MOVE_FORWARD = 0
+    TURN_LEFT = 1
+    TURN_RIGHT = 2
+    STOP = 3
+"""
+
+pose_refs_rgb = proc_rgb(create_reference_grid(obs["pose_estimation_rgb"]))
+
+while True:
+    action = obs["oracle_action_sensor"][0].item()
+    obs, reward, done, info = env.step(action)
+
+    if done:
+        obs = env.reset()
+        pose_refs_rgb = proc_rgb(create_reference_grid(obs["pose_estimation_rgb"]))
+
+    rgb_im = proc_rgb(obs["rgb"])
+    topdown_im = proc_rgb(info["top_down_map_pose"])
+
+    cv2.imshow(
+        "Reconstruction demo",
+        np.concatenate([rgb_im, topdown_im, pose_refs_rgb], axis=1),
+    )
+    cv2.waitKey(60)
diff --git demos/utils.py demos/utils.py
new file mode 100644
index 0000000..5c6ce7a
--- /dev/null
+++ demos/utils.py
@@ -0,0 +1,8 @@
+import cv2
+import pdb
+import math
+import numpy as np
+
+
+def proc_rgb(rgb):
+    return cv2.resize(np.flip(rgb, axis=2), (300, 300))
diff --git docs/source/conf.py docs/source/conf.py
index 317c59a..29c1b88 100644
--- docs/source/conf.py
+++ docs/source/conf.py
@@ -195,9 +195,7 @@ texinfo_documents = [
     )
 ]
 
-github_doc_root = (
-    f"https://github.com/facebookresearch/habitat-api/blob/{version}/"
-)
+github_doc_root = f"https://github.com/facebookresearch/habitat-api/blob/{version}/"
 
 # At the bottom of conf.py
 def setup(app):
diff --git examples/new_actions.py examples/new_actions.py
index 8958448..2ebab18 100644
--- examples/new_actions.py
+++ examples/new_actions.py
@@ -81,9 +81,7 @@ class NoisyStrafeRight(habitat_sim.SceneNodeControl):
         scene_node: habitat_sim.SceneNode,
         actuation_spec: NoisyStrafeActuationSpec,
     ):
-        print(
-            f"strafing right with noise_amount={actuation_spec.noise_amount}"
-        )
+        print(f"strafing right with noise_amount={actuation_spec.noise_amount}")
         _strafe_impl(
             scene_node,
             actuation_spec.move_amount,
@@ -98,12 +96,10 @@ class NoNoiseStrafe(HabitatSimV1ActionSpaceConfiguration):
         config = super().get()
 
         config[habitat.SimulatorActions.STRAFE_LEFT] = habitat_sim.ActionSpec(
-            "noisy_strafe_left",
-            NoisyStrafeActuationSpec(0.25, noise_amount=0.0),
+            "noisy_strafe_left", NoisyStrafeActuationSpec(0.25, noise_amount=0.0),
         )
         config[habitat.SimulatorActions.STRAFE_RIGHT] = habitat_sim.ActionSpec(
-            "noisy_strafe_right",
-            NoisyStrafeActuationSpec(0.25, noise_amount=0.0),
+            "noisy_strafe_right", NoisyStrafeActuationSpec(0.25, noise_amount=0.0),
         )
 
         return config
@@ -115,12 +111,10 @@ class NoiseStrafe(HabitatSimV1ActionSpaceConfiguration):
         config = super().get()
 
         config[habitat.SimulatorActions.STRAFE_LEFT] = habitat_sim.ActionSpec(
-            "noisy_strafe_left",
-            NoisyStrafeActuationSpec(0.25, noise_amount=0.05),
+            "noisy_strafe_left", NoisyStrafeActuationSpec(0.25, noise_amount=0.05),
         )
         config[habitat.SimulatorActions.STRAFE_RIGHT] = habitat_sim.ActionSpec(
-            "noisy_strafe_right",
-            NoisyStrafeActuationSpec(0.25, noise_amount=0.05),
+            "noisy_strafe_right", NoisyStrafeActuationSpec(0.25, noise_amount=0.05),
         )
 
         return config
diff --git examples/shortest_path_follower_example.py examples/shortest_path_follower_example.py
index b70c69d..de05cc8 100644
--- examples/shortest_path_follower_example.py
+++ examples/shortest_path_follower_example.py
@@ -39,17 +39,15 @@ def draw_top_down_map(info, heading, output_size):
         info["top_down_map"]["map"], info["top_down_map"]["fog_of_war_mask"]
     )
     original_map_size = top_down_map.shape[:2]
-    map_scale = np.array(
-        (1, original_map_size[1] * 1.0 / original_map_size[0])
-    )
+    map_scale = np.array((1, original_map_size[1] * 1.0 / original_map_size[0]))
     new_map_size = np.round(output_size * map_scale).astype(np.int32)
     # OpenCV expects w, h but map size is in h, w
     top_down_map = cv2.resize(top_down_map, (new_map_size[1], new_map_size[0]))
 
     map_agent_pos = info["top_down_map"]["agent_map_coord"]
-    map_agent_pos = np.round(
-        map_agent_pos * new_map_size / original_map_size
-    ).astype(np.int32)
+    map_agent_pos = np.round(map_agent_pos * new_map_size / original_map_size).astype(
+        np.int32
+    )
     top_down_map = maps.draw_agent(
         top_down_map,
         map_agent_pos,
@@ -60,7 +58,7 @@ def draw_top_down_map(info, heading, output_size):
 
 
 def shortest_path_example(mode):
-    config = habitat.get_config(config_paths="configs/tasks/pointnav.yaml")
+    config = habitat.get_config(config_paths="configs/tasks/pointnav_mp3d.yaml")
     config.defrost()
     config.TASK.MEASUREMENTS.append("TOP_DOWN_MAP")
     config.TASK.SENSORS.append("HEADING_SENSOR")
@@ -89,9 +87,7 @@ def shortest_path_example(mode):
             )
             observations, reward, done, info = env.step(best_action)
             im = observations["rgb"]
-            top_down_map = draw_top_down_map(
-                info, observations["heading"], im.shape[0]
-            )
+            top_down_map = draw_top_down_map(info, observations["heading"], im.shape[0])
             output_im = np.concatenate((im, top_down_map), axis=1)
             images.append(output_im)
         images_to_video(images, dirname, "trajectory")
diff --git examples/visualization_examples.py examples/visualization_examples.py
index 863294f..6fb101c 100644
--- examples/visualization_examples.py
+++ examples/visualization_examples.py
@@ -40,9 +40,7 @@ def example_pointnav_draw_target_birdseye_view():
         agent_radius_px=25,
     )
 
-    imageio.imsave(
-        os.path.join(IMAGE_DIR, "pointnav_target_image.png"), target_image
-    )
+    imageio.imsave(os.path.join(IMAGE_DIR, "pointnav_target_image.png"), target_image)
 
 
 def example_pointnav_draw_target_birdseye_view_agent_on_border():
@@ -72,9 +70,7 @@ def example_pointnav_draw_target_birdseye_view_agent_on_border():
                 agent_radius_px=25,
             )
             imageio.imsave(
-                os.path.join(
-                    IMAGE_DIR, "pointnav_target_image_edge_%d.png" % ii
-                ),
+                os.path.join(IMAGE_DIR, "pointnav_target_image_edge_%d.png" % ii),
                 target_image,
             )
 
@@ -101,9 +97,7 @@ def example_get_topdown_map():
         max(range_y[0] - padding, 0),
         min(range_y[-1] + padding + 1, top_down_map.shape[1]),
     )
-    top_down_map = top_down_map[
-        range_x[0] : range_x[1], range_y[0] : range_y[1]
-    ]
+    top_down_map = top_down_map[range_x[0] : range_x[1], range_y[0] : range_y[1]]
     top_down_map = recolor_map[top_down_map]
     imageio.imsave(os.path.join(IMAGE_DIR, "top_down_map.png"), top_down_map)
 
diff --git habitat/__init__.py habitat/__init__.py
index 2b82de2..7fd8fb9 100644
--- habitat/__init__.py
+++ habitat/__init__.py
@@ -4,7 +4,7 @@
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
-from habitat.config import Config, get_config
+from habitat.config import Config, get_config, get_config_pose, get_config_exp_nav
 from habitat.core.agent import Agent
 from habitat.core.benchmark import Benchmark
 from habitat.core.challenge import Challenge
diff --git habitat/config/__init__.py habitat/config/__init__.py
index 6734e3f..56a368f 100644
--- habitat/config/__init__.py
+++ habitat/config/__init__.py
@@ -7,5 +7,7 @@
 from yacs.config import CfgNode as Config
 
 from habitat.config.default import get_config
+from habitat.config.default_pose import get_config_pose
+from habitat.config.default_exp_nav import get_config_exp_nav
 
-__all__ = ["Config", "get_config"]
+__all__ = ["Config", "get_config", "get_config_pose", "get_config_exp_nav"]
diff --git habitat/config/default.py habitat/config/default.py
index 8fd60e7..cc968fd 100644
--- habitat/config/default.py
+++ habitat/config/default.py
@@ -8,7 +8,7 @@ from typing import List, Optional, Union
 
 from habitat.config import Config as CN  # type: ignore
 
-DEFAULT_CONFIG_DIR = "configs/"
+DEFAULT_CONFIG_DIR = "configs"
 CONFIG_FILE_SEPARATOR = ","
 
 # -----------------------------------------------------------------------------
@@ -55,6 +55,20 @@ _C.TASK.PROXIMITY_SENSOR = CN()
 _C.TASK.PROXIMITY_SENSOR.TYPE = "ProximitySensor"
 _C.TASK.PROXIMITY_SENSOR.MAX_DETECTION_RADIUS = 2.0
 # -----------------------------------------------------------------------------
+# # ORACLE ACTION SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.ORACLE_ACTION_SENSOR = CN()
+_C.TASK.ORACLE_ACTION_SENSOR.TYPE = "OracleActionSensor"
+_C.TASK.ORACLE_ACTION_SENSOR.GOAL_RADIUS = 0.25
+_C.TASK.ORACLE_ACTION_SENSOR.ORACLE_TYPE = "random"
+_C.TASK.ORACLE_ACTION_SENSOR.NUM_TARGETS = 500
+# -----------------------------------------------------------------------------
+# # SHORTEST PATH ACTION SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.SP_ACTION_SENSOR = CN()
+_C.TASK.SP_ACTION_SENSOR.TYPE = "SPActionSensor"
+_C.TASK.SP_ACTION_SENSOR.GOAL_RADIUS = 0.25
+# -----------------------------------------------------------------------------
 # # SPL MEASUREMENT
 # -----------------------------------------------------------------------------
 _C.TASK.SPL = CN()
@@ -88,19 +102,19 @@ _C.SIMULATOR = CN()
 _C.SIMULATOR.TYPE = "Sim-v0"
 _C.SIMULATOR.ACTION_SPACE_CONFIG = "v0"
 _C.SIMULATOR.FORWARD_STEP_SIZE = 0.25  # in metres
-_C.SIMULATOR.SCENE = (
-    "data/scene_datasets/habitat-test-scenes/" "van-gogh-room.glb"
-)
+_C.SIMULATOR.SCENE = "data/scene_datasets/habitat-test-scenes/" "van-gogh-room.glb"
 _C.SIMULATOR.SEED = _C.SEED
 _C.SIMULATOR.TURN_ANGLE = 10  # angle to rotate left or right in degrees
 _C.SIMULATOR.TILT_ANGLE = 15  # angle to tilt the camera up or down in degrees
 _C.SIMULATOR.DEFAULT_AGENT_ID = 0
+_C.SIMULATOR.ENABLE_ODOMETRY_NOISE = False
+_C.SIMULATOR.ODOMETER_NOISE_SCALING = 0.0
 # -----------------------------------------------------------------------------
 # # SENSORS
 # -----------------------------------------------------------------------------
 SENSOR = CN()
-SENSOR.HEIGHT = 480
-SENSOR.WIDTH = 640
+SENSOR.HEIGHT = 84
+SENSOR.WIDTH = 84
 SENSOR.HFOV = 90  # horizontal field of view in degrees
 SENSOR.POSITION = [0, 1.25, 0]
 # -----------------------------------------------------------------------------
@@ -122,6 +136,48 @@ _C.SIMULATOR.DEPTH_SENSOR.NORMALIZE_DEPTH = True
 _C.SIMULATOR.SEMANTIC_SENSOR = SENSOR.clone()
 _C.SIMULATOR.SEMANTIC_SENSOR.TYPE = "HabitatSimSemanticSensor"
 # -----------------------------------------------------------------------------
+# # FINE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.FINE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.FINE_OCC_SENSOR.TYPE = "HabitatSimFineOccSensor"
+# -----------------------------------------------------------------------------
+# # COARSE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.COARSE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.COARSE_OCC_SENSOR.TYPE = "HabitatSimCoarseOccSensor"
+# -----------------------------------------------------------------------------
+# # PROJ-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.PROJ_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.PROJ_OCC_SENSOR.TYPE = "HabitatSimProjOccSensor"
+# -----------------------------------------------------------------------------
+# # HIGHRES-COARSE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.HIGHRES_COARSE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.HIGHRES_COARSE_OCC_SENSOR.TYPE = "HabitatSimHighResCoarseOccSensor"
+# -----------------------------------------------------------------------------
+# # OCCUPANCY MAPS
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.OCCUPANCY_MAPS = CN()
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT = SENSOR.HEIGHT
+_C.SIMULATOR.OCCUPANCY_MAPS.WIDTH = SENSOR.WIDTH
+_C.SIMULATOR.OCCUPANCY_MAPS.MAP_SCALE = 0.1
+_C.SIMULATOR.OCCUPANCY_MAPS.MAP_SIZE = 800
+_C.SIMULATOR.OCCUPANCY_MAPS.MAX_DEPTH = 3
+_C.SIMULATOR.OCCUPANCY_MAPS.SMALL_MAP_RANGE = 20
+_C.SIMULATOR.OCCUPANCY_MAPS.LARGE_MAP_RANGE = 100
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT_LOWER = 0.5
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT_UPPER = 2.0
+_C.SIMULATOR.OCCUPANCY_MAPS.GET_PROJ_LOC_MAP = False
+_C.SIMULATOR.OCCUPANCY_MAPS.GET_HIGHRES_LOC_MAP = False
+_C.SIMULATOR.OCCUPANCY_MAPS.USE_GT_OCC_MAP = False
+# -----------------------------------------------------------------------------
+# # OBJECT ANNOTATIONS
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.OBJECT_ANNOTATIONS = CN()
+_C.SIMULATOR.OBJECT_ANNOTATIONS.IS_AVAILABLE = False
+_C.SIMULATOR.OBJECT_ANNOTATIONS.PATH = "./"
+# -----------------------------------------------------------------------------
 # AGENT
 # -----------------------------------------------------------------------------
 _C.SIMULATOR.AGENT_0 = CN()
@@ -155,14 +211,14 @@ _C.DATASET.CONTENT_SCENES = ["*"]
 _C.DATASET.DATA_PATH = (
     "data/datasets/pointnav/habitat-test-scenes/v1/{split}/{split}.json.gz"
 )
+_C.DATASET.SHUFFLE_DATASET = True
 
 
 # -----------------------------------------------------------------------------
 
 
 def get_config(
-    config_paths: Optional[Union[List[str], str]] = None,
-    opts: Optional[list] = None,
+    config_paths: Optional[Union[List[str], str]] = None, opts: Optional[list] = None,
 ) -> CN:
     r"""Create a unified config with default values overwritten by values from
     `config_paths` and overwritten by options from `opts`.
diff --git habitat/config/default_exp_nav.py habitat/config/default_exp_nav.py
new file mode 100644
index 0000000..5c62a9c
--- /dev/null
+++ habitat/config/default_exp_nav.py
@@ -0,0 +1,298 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List, Optional, Union
+
+from habitat.config import Config as CN  # type: ignore
+
+DEFAULT_CONFIG_DIR = "configs/"
+CONFIG_FILE_SEPARATOR = ","
+
+# -----------------------------------------------------------------------------
+# Config definition
+# -----------------------------------------------------------------------------
+_C = CN()
+_C.SEED = 100
+# -----------------------------------------------------------------------------
+# ENVIRONMENT
+# -----------------------------------------------------------------------------
+_C.ENVIRONMENT = CN()
+_C.ENVIRONMENT.MAX_EPISODE_STEPS = 1000
+_C.ENVIRONMENT.MAX_EPISODE_SECONDS = 10000000
+_C.ENVIRONMENT.T_EXP = 500
+_C.ENVIRONMENT.T_NAV = 500
+# -----------------------------------------------------------------------------
+# TASK
+# -----------------------------------------------------------------------------
+_C.TASK = CN()
+_C.TASK.TYPE = "ExpNav-v0"
+_C.TASK.SUCCESS_DISTANCE = 0.2
+_C.TASK.SENSORS = []
+_C.TASK.MEASUREMENTS = []
+# -----------------------------------------------------------------------------
+# # HEADING SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.HEADING_SENSOR = CN()
+_C.TASK.HEADING_SENSOR.TYPE = "HeadingSensor"
+# -----------------------------------------------------------------------------
+# # PROXIMITY SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.PROXIMITY_SENSOR = CN()
+_C.TASK.PROXIMITY_SENSOR.TYPE = "ProximitySensor"
+_C.TASK.PROXIMITY_SENSOR.MAX_DETECTION_RADIUS = 2.0
+# -----------------------------------------------------------------------------
+# # LOCAL TOP DOWN SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.LOCAL_TOP_DOWN_SENSOR = CN()
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.TYPE = "LocalTopDownSensor"
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.WIDTH = 480
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.HEIGHT = 640
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.NUM_TOPDOWN_MAP_SAMPLE_POINTS = 20000
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.MAP_SCALE = 0.1
+_C.TASK.LOCAL_TOP_DOWN_SENSOR.MAP_RANGE = 100
+# -----------------------------------------------------------------------------
+# # DELTA SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.DELTA_SENSOR = CN()
+_C.TASK.DELTA_SENSOR.TYPE = "DeltaSensor"
+# -----------------------------------------------------------------------------
+# # ORACLE ACTION SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.ORACLE_ACTION_SENSOR = CN()
+_C.TASK.ORACLE_ACTION_SENSOR.TYPE = "OracleActionSensor"
+_C.TASK.ORACLE_ACTION_SENSOR.GOAL_RADIUS = 0.25
+_C.TASK.ORACLE_ACTION_SENSOR.ORACLE_TYPE = "random"
+_C.TASK.ORACLE_ACTION_SENSOR.NUM_TARGETS = 500
+# -----------------------------------------------------------------------------
+# # COLLISION SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.COLLISION_SENSOR = CN()
+_C.TASK.COLLISION_SENSOR.TYPE = "CollisionSensor"
+# -----------------------------------------------------------------------------
+# # Grid Goal Sensor
+# -----------------------------------------------------------------------------
+_C.TASK.GRID_GOAL_SENSOR = CN()
+_C.TASK.GRID_GOAL_SENSOR.TYPE = "GridGoalSensorExploreNavigation"
+_C.TASK.GRID_GOAL_SENSOR.T_EXP = _C.ENVIRONMENT.T_EXP
+_C.TASK.GRID_GOAL_SENSOR.T_NAV = _C.ENVIRONMENT.T_NAV
+# -----------------------------------------------------------------------------
+# # Shortest Path Action Sensor
+# -----------------------------------------------------------------------------
+_C.TASK.SP_ACTION_SENSOR = CN()
+_C.TASK.SP_ACTION_SENSOR.TYPE = "SPActionSensorExploreNavigation"
+_C.TASK.SP_ACTION_SENSOR.GOAL_RADIUS = 0.25
+_C.TASK.SP_ACTION_SENSOR.T_EXP = _C.ENVIRONMENT.T_EXP
+_C.TASK.SP_ACTION_SENSOR.T_NAV = _C.ENVIRONMENT.T_NAV
+# -----------------------------------------------------------------------------
+# # TopDownMapPose MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.TOP_DOWN_MAP_EXP_NAV = CN()
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.TYPE = "TopDownMapExpNav"
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.MAX_EPISODE_STEPS = _C.ENVIRONMENT.MAX_EPISODE_STEPS
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.T_EXP = _C.ENVIRONMENT.T_EXP
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.T_NAV = _C.ENVIRONMENT.T_NAV
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.MAP_PADDING = 3
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.NUM_TOPDOWN_MAP_SAMPLE_POINTS = 20000
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.MAP_RESOLUTION = 1250
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.DRAW_SOURCE_AND_TARGET = True
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.DRAW_BORDER = True
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.DRAW_SHORTEST_PATH = True
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.FOG_OF_WAR = CN()
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.FOG_OF_WAR.DRAW = True
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.FOG_OF_WAR.VISIBILITY_DIST = 5.0
+_C.TASK.TOP_DOWN_MAP_EXP_NAV.FOG_OF_WAR.FOV = 90
+# -----------------------------------------------------------------------------
+# # COLLISIONS MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.COLLISIONS = CN()
+_C.TASK.COLLISIONS.TYPE = "Collisions"
+# -----------------------------------------------------------------------------
+# SIMULATOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR = CN()
+_C.SIMULATOR.TYPE = "Sim-v1"
+_C.SIMULATOR.ACTION_SPACE_CONFIG = "v2"
+_C.SIMULATOR.FORWARD_STEP_SIZE = 0.25  # in metres
+_C.SIMULATOR.SCENE = "data/scene_datasets/habitat-test-scenes/" "van-gogh-room.glb"
+_C.SIMULATOR.SEED = _C.SEED
+_C.SIMULATOR.TURN_ANGLE = 10  # angle to rotate left or right in degrees
+_C.SIMULATOR.TILT_ANGLE = 15  # angle to tilt the camera up or down in degrees
+_C.SIMULATOR.DEFAULT_AGENT_ID = 0
+_C.SIMULATOR.ENABLE_ODOMETRY_NOISE = False
+_C.SIMULATOR.ODOMETER_NOISE_SCALING = 0.0
+# -----------------------------------------------------------------------------
+# # SENSORS
+# -----------------------------------------------------------------------------
+SENSOR = CN()
+SENSOR.HEIGHT = 84
+SENSOR.WIDTH = 84
+SENSOR.HFOV = 90  # horizontal field of view in degrees
+SENSOR.POSITION = [0, 1.25, 0]
+# -----------------------------------------------------------------------------
+# # RGB SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.RGB_SENSOR = SENSOR.clone()
+_C.SIMULATOR.RGB_SENSOR.TYPE = "HabitatSimRGBSensor"
+# -----------------------------------------------------------------------------
+# DEPTH SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.DEPTH_SENSOR = SENSOR.clone()
+_C.SIMULATOR.DEPTH_SENSOR.TYPE = "HabitatSimDepthSensor"
+_C.SIMULATOR.DEPTH_SENSOR.MIN_DEPTH = 0
+_C.SIMULATOR.DEPTH_SENSOR.MAX_DEPTH = 10
+_C.SIMULATOR.DEPTH_SENSOR.NORMALIZE_DEPTH = True
+# -----------------------------------------------------------------------------
+# SEMANTIC SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.SEMANTIC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.SEMANTIC_SENSOR.TYPE = "HabitatSimSemanticSensor"
+# -----------------------------------------------------------------------------
+# # FINE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.FINE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.FINE_OCC_SENSOR.TYPE = "HabitatSimFineOccSensor"
+# -----------------------------------------------------------------------------
+# # COARSE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.COARSE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.COARSE_OCC_SENSOR.TYPE = "HabitatSimCoarseOccSensor"
+# -----------------------------------------------------------------------------
+# # HIGHRES-COARSE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.HIGHRES_COARSE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.HIGHRES_COARSE_OCC_SENSOR.TYPE = "HabitatSimHighResCoarseOccSensor"
+# -----------------------------------------------------------------------------
+# # OCCUPANCY MAPS
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.OCCUPANCY_MAPS = CN()
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT = SENSOR.HEIGHT
+_C.SIMULATOR.OCCUPANCY_MAPS.WIDTH = SENSOR.WIDTH
+_C.SIMULATOR.OCCUPANCY_MAPS.MAP_SCALE = 0.1
+_C.SIMULATOR.OCCUPANCY_MAPS.MAP_SIZE = 800
+_C.SIMULATOR.OCCUPANCY_MAPS.MAX_DEPTH = 3
+_C.SIMULATOR.OCCUPANCY_MAPS.SMALL_MAP_RANGE = 20
+_C.SIMULATOR.OCCUPANCY_MAPS.LARGE_MAP_RANGE = 100
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT_LOWER = 0.5
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT_UPPER = 2.0
+_C.SIMULATOR.OCCUPANCY_MAPS.GET_PROJ_LOC_MAP = False
+_C.SIMULATOR.OCCUPANCY_MAPS.GET_HIGHRES_LOC_MAP = True
+_C.SIMULATOR.OCCUPANCY_MAPS.USE_GT_OCC_MAP = False
+_C.SIMULATOR.OCCUPANCY_MAPS.MEASURE_NOISE_FREE_AREA = False
+_C.SIMULATOR.OCCUPANCY_MAPS.COVERAGE_NOVELTY_POOLING = "mean"
+# -----------------------------------------------------------------------------
+# # OBJECT ANNOTATIONS
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.OBJECT_ANNOTATIONS = CN()
+_C.SIMULATOR.OBJECT_ANNOTATIONS.IS_AVAILABLE = False
+_C.SIMULATOR.OBJECT_ANNOTATIONS.PATH = "./"
+# -----------------------------------------------------------------------------
+# # IMAGE GOAL SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.IMAGE_GOAL_SENSOR = SENSOR.clone()
+_C.TASK.IMAGE_GOAL_SENSOR.TYPE = "ImageGoalSensorExploreNavigation"
+_C.TASK.IMAGE_GOAL_SENSOR.T_EXP = _C.ENVIRONMENT.T_EXP
+_C.TASK.IMAGE_GOAL_SENSOR.T_NAV = _C.ENVIRONMENT.T_NAV
+# -----------------------------------------------------------------------------
+# # SPL MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.SPL_EXP_NAV = CN()
+_C.TASK.SPL_EXP_NAV.TYPE = "SPLExpNav"
+_C.TASK.SPL_EXP_NAV.SUCCESS_DISTANCE = 0.2
+_C.TASK.SPL_EXP_NAV.T_EXP = _C.ENVIRONMENT.T_EXP
+_C.TASK.SPL_EXP_NAV.T_NAV = _C.ENVIRONMENT.T_NAV
+# -----------------------------------------------------------------------------
+# # SUCCESS MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.SUCCESS_EXP_NAV = CN()
+_C.TASK.SUCCESS_EXP_NAV.TYPE = "SuccessExpNav"
+_C.TASK.SUCCESS_EXP_NAV.SUCCESS_DISTANCE = 0.2
+_C.TASK.SUCCESS_EXP_NAV.T_EXP = _C.ENVIRONMENT.T_EXP
+_C.TASK.SUCCESS_EXP_NAV.T_NAV = _C.ENVIRONMENT.T_NAV
+# -----------------------------------------------------------------------------
+# # NAVIGATION ERROR MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.NAVIGATION_ERROR_EXP_NAV = CN()
+_C.TASK.NAVIGATION_ERROR_EXP_NAV.TYPE = "NavigationErrorExpNav"
+_C.TASK.NAVIGATION_ERROR_EXP_NAV.T_EXP = _C.ENVIRONMENT.T_EXP
+_C.TASK.NAVIGATION_ERROR_EXP_NAV.T_NAV = _C.ENVIRONMENT.T_NAV
+# -----------------------------------------------------------------------------
+# # AREA COVERED MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.AREA_COVERED = CN()
+_C.TASK.AREA_COVERED.TYPE = "AreaCovered"
+# -----------------------------------------------------------------------------
+# # COLLISIONS MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.COLLISIONS = CN()
+_C.TASK.COLLISIONS.TYPE = "Collisions"
+# -----------------------------------------------------------------------------
+# AGENT
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.AGENT_0 = CN()
+_C.SIMULATOR.AGENT_0.HEIGHT = 1.5
+_C.SIMULATOR.AGENT_0.RADIUS = 0.1
+_C.SIMULATOR.AGENT_0.MASS = 32.0
+_C.SIMULATOR.AGENT_0.LINEAR_ACCELERATION = 20.0
+_C.SIMULATOR.AGENT_0.ANGULAR_ACCELERATION = 4 * 3.14
+_C.SIMULATOR.AGENT_0.LINEAR_FRICTION = 0.5
+_C.SIMULATOR.AGENT_0.ANGULAR_FRICTION = 1.0
+_C.SIMULATOR.AGENT_0.COEFFICIENT_OF_RESTITUTION = 0.0
+_C.SIMULATOR.AGENT_0.SENSORS = ["RGB_SENSOR"]
+_C.SIMULATOR.AGENT_0.IS_SET_START_STATE = False
+_C.SIMULATOR.AGENT_0.START_POSITION = [0, 0, 0]
+_C.SIMULATOR.AGENT_0.START_ROTATION = [0, 0, 0, 1]
+_C.SIMULATOR.AGENTS = ["AGENT_0"]
+# -----------------------------------------------------------------------------
+# SIMULATOR HABITAT_SIM_V0
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.HABITAT_SIM_V0 = CN()
+_C.SIMULATOR.HABITAT_SIM_V0.GPU_DEVICE_ID = 0
+# -----------------------------------------------------------------------------
+# DATASET
+# -----------------------------------------------------------------------------
+_C.DATASET = CN()
+_C.DATASET.TYPE = "ExpNav-v1"
+_C.DATASET.SPLIT = "train"
+_C.DATASET.SCENES_DIR = "data/scene_datasets"
+_C.DATASET.NUM_EPISODE_SAMPLE = -1
+_C.DATASET.CONTENT_SCENES = ["*"]
+_C.DATASET.DATA_PATH = (
+    "data/datasets/exp_nav/habitat-test-scenes/v1/{split}/{split}.json.gz"
+)
+_C.DATASET.SHUFFLE_DATASET = True
+
+
+# -----------------------------------------------------------------------------
+
+
+def get_config_exp_nav(
+    config_paths: Optional[Union[List[str], str]] = None, opts: Optional[list] = None,
+) -> CN:
+    r"""Create a unified config with default values overwritten by values from
+    `config_paths` and overwritten by options from `opts`.
+    Args:
+        config_paths: List of config paths or string that contains comma
+        separated list of config paths.
+        opts: Config options (keys, values) in a list (e.g., passed from
+        command line into the config. For example, `opts = ['FOO.BAR',
+        0.5]`. Argument can be used for parameter sweeping or quick tests.
+    """
+    config = _C.clone()
+    if config_paths:
+        if isinstance(config_paths, str):
+            if CONFIG_FILE_SEPARATOR in config_paths:
+                config_paths = config_paths.split(CONFIG_FILE_SEPARATOR)
+            else:
+                config_paths = [config_paths]
+
+        for config_path in config_paths:
+            config.merge_from_file(config_path)
+
+    if opts:
+        config.merge_from_list(opts)
+
+    config.freeze()
+    return config
diff --git habitat/config/default_pose.py habitat/config/default_pose.py
new file mode 100644
index 0000000..064599e
--- /dev/null
+++ habitat/config/default_pose.py
@@ -0,0 +1,283 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List, Optional, Union
+
+from habitat.config import Config as CN  # type: ignore
+
+DEFAULT_CONFIG_DIR = "configs/"
+CONFIG_FILE_SEPARATOR = ","
+
+# -----------------------------------------------------------------------------
+# Config definition
+# -----------------------------------------------------------------------------
+_C = CN()
+_C.SEED = 100
+# -----------------------------------------------------------------------------
+# ENVIRONMENT
+# -----------------------------------------------------------------------------
+_C.ENVIRONMENT = CN()
+_C.ENVIRONMENT.MAX_EPISODE_STEPS = 1000
+_C.ENVIRONMENT.MAX_EPISODE_SECONDS = 10000000
+# -----------------------------------------------------------------------------
+# TASK
+# -----------------------------------------------------------------------------
+_C.TASK = CN()
+_C.TASK.TYPE = "Pose-v0"
+_C.TASK.SENSORS = []
+_C.TASK.MEASUREMENTS = []
+# -----------------------------------------------------------------------------
+# # HEADING SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.HEADING_SENSOR = CN()
+_C.TASK.HEADING_SENSOR.TYPE = "HeadingSensor"
+# -----------------------------------------------------------------------------
+# # PROXIMITY SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.PROXIMITY_SENSOR = CN()
+_C.TASK.PROXIMITY_SENSOR.TYPE = "ProximitySensor"
+_C.TASK.PROXIMITY_SENSOR.MAX_DETECTION_RADIUS = 2.0
+# -----------------------------------------------------------------------------
+# # POSE REGRESS SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.POSE_REGRESS_SENSOR = CN()
+_C.TASK.POSE_REGRESS_SENSOR.TYPE = "PoseEstimationRegressSensor"
+_C.TASK.POSE_REGRESS_SENSOR.NREF = 20
+# -----------------------------------------------------------------------------
+# # DELTA SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.DELTA_SENSOR = CN()
+_C.TASK.DELTA_SENSOR.TYPE = "DeltaSensor"
+# -----------------------------------------------------------------------------
+# # ORACLE ACTION SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.ORACLE_ACTION_SENSOR = CN()
+_C.TASK.ORACLE_ACTION_SENSOR.TYPE = "OracleActionSensor"
+_C.TASK.ORACLE_ACTION_SENSOR.GOAL_RADIUS = 0.25
+_C.TASK.ORACLE_ACTION_SENSOR.ORACLE_TYPE = "random"
+_C.TASK.ORACLE_ACTION_SENSOR.NUM_TARGETS = 500
+# -----------------------------------------------------------------------------
+# # COLLISION SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.COLLISION_SENSOR = CN()
+_C.TASK.COLLISION_SENSOR.TYPE = "CollisionSensor"
+# -----------------------------------------------------------------------------
+# # OPSR MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.OPSR = CN()
+_C.TASK.OPSR.TYPE = "OPSR"
+_C.TASK.OPSR.GEODESIC_DIST_THRESH = 2.0  # Meters
+_C.TASK.OPSR.ANGULAR_DIST_THRESH = 20.0  # Degrees
+# -----------------------------------------------------------------------------
+# # AREA COVERED MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.AREA_COVERED = CN()
+_C.TASK.AREA_COVERED.TYPE = "AreaCovered"
+# -----------------------------------------------------------------------------
+# # INC AREA COVERED MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.INC_AREA_COVERED = CN()
+_C.TASK.INC_AREA_COVERED.TYPE = "IncAreaCovered"
+# -----------------------------------------------------------------------------
+# # TopDownMapPose MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.TOP_DOWN_MAP_POSE = CN()
+_C.TASK.TOP_DOWN_MAP_POSE.TYPE = "TopDownMapPose"
+_C.TASK.TOP_DOWN_MAP_POSE.MAX_EPISODE_STEPS = _C.ENVIRONMENT.MAX_EPISODE_STEPS
+_C.TASK.TOP_DOWN_MAP_POSE.MAP_PADDING = 3
+_C.TASK.TOP_DOWN_MAP_POSE.NUM_TOPDOWN_MAP_SAMPLE_POINTS = 20000
+_C.TASK.TOP_DOWN_MAP_POSE.MAP_RESOLUTION = 1250
+_C.TASK.TOP_DOWN_MAP_POSE.DRAW_SOURCE_AND_REFERENCES = True
+_C.TASK.TOP_DOWN_MAP_POSE.DRAW_BORDER = True
+_C.TASK.TOP_DOWN_MAP_POSE.DRAW_SHORTEST_PATH = True
+_C.TASK.TOP_DOWN_MAP_POSE.FOG_OF_WAR = CN()
+_C.TASK.TOP_DOWN_MAP_POSE.FOG_OF_WAR.DRAW = True
+_C.TASK.TOP_DOWN_MAP_POSE.FOG_OF_WAR.VISIBILITY_DIST = 5.0
+_C.TASK.TOP_DOWN_MAP_POSE.FOG_OF_WAR.FOV = 90
+# -----------------------------------------------------------------------------
+# # COLLISIONS MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.COLLISIONS = CN()
+_C.TASK.COLLISIONS.TYPE = "Collisions"
+# -----------------------------------------------------------------------------
+# # OBJECTS COVERED GEOMETRIC MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.OBJECTS_COVERED_GEOMETRIC = CN()
+_C.TASK.OBJECTS_COVERED_GEOMETRIC.TYPE = "ObjectsCoveredGeometric"
+# -----------------------------------------------------------------------------
+# # NOVELTY_REWARD MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.NOVELTY_REWARD = CN()
+_C.TASK.NOVELTY_REWARD.TYPE = "NoveltyReward"
+_C.TASK.NOVELTY_REWARD.GRID_SIZE = 0.5  # In meters
+# -----------------------------------------------------------------------------
+# # COVERAGE_NOVELTY_REWARD MEASUREMENT
+# -----------------------------------------------------------------------------
+_C.TASK.COVERAGE_NOVELTY_REWARD = CN()
+_C.TASK.COVERAGE_NOVELTY_REWARD.TYPE = "CoverageNoveltyReward"
+# -----------------------------------------------------------------------------
+# SIMULATOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR = CN()
+_C.SIMULATOR.TYPE = "Sim-v1"
+_C.SIMULATOR.ACTION_SPACE_CONFIG = "v2"
+_C.SIMULATOR.FORWARD_STEP_SIZE = 0.25  # in metres
+_C.SIMULATOR.SCENE = "data/scene_datasets/habitat-test-scenes/" "van-gogh-room.glb"
+_C.SIMULATOR.SEED = _C.SEED
+_C.SIMULATOR.TURN_ANGLE = 10  # angle to rotate left or right in degrees
+_C.SIMULATOR.TILT_ANGLE = 15  # angle to tilt the camera up or down in degrees
+_C.SIMULATOR.DEFAULT_AGENT_ID = 0
+_C.SIMULATOR.ENABLE_ODOMETRY_NOISE = False
+_C.SIMULATOR.ODOMETER_NOISE_SCALING = 0.0
+# -----------------------------------------------------------------------------
+# # SENSORS
+# -----------------------------------------------------------------------------
+SENSOR = CN()
+SENSOR.HEIGHT = 84
+SENSOR.WIDTH = 84
+SENSOR.HFOV = 90  # horizontal field of view in degrees
+SENSOR.POSITION = [0, 1.25, 0]
+# -----------------------------------------------------------------------------
+# # RGB SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.RGB_SENSOR = SENSOR.clone()
+_C.SIMULATOR.RGB_SENSOR.TYPE = "HabitatSimRGBSensor"
+# -----------------------------------------------------------------------------
+# DEPTH SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.DEPTH_SENSOR = SENSOR.clone()
+_C.SIMULATOR.DEPTH_SENSOR.TYPE = "HabitatSimDepthSensor"
+_C.SIMULATOR.DEPTH_SENSOR.MIN_DEPTH = 0
+_C.SIMULATOR.DEPTH_SENSOR.MAX_DEPTH = 10
+_C.SIMULATOR.DEPTH_SENSOR.NORMALIZE_DEPTH = True
+# -----------------------------------------------------------------------------
+# SEMANTIC SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.SEMANTIC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.SEMANTIC_SENSOR.TYPE = "HabitatSimSemanticSensor"
+# -----------------------------------------------------------------------------
+# # FINE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.FINE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.FINE_OCC_SENSOR.TYPE = "HabitatSimFineOccSensor"
+# -----------------------------------------------------------------------------
+# # COARSE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.COARSE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.COARSE_OCC_SENSOR.TYPE = "HabitatSimCoarseOccSensor"
+# -----------------------------------------------------------------------------
+# # HIGHRES COARSE-OCCUPANCY SENSOR
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.HIGHRES_COARSE_OCC_SENSOR = SENSOR.clone()
+_C.SIMULATOR.HIGHRES_COARSE_OCC_SENSOR.TYPE = "HabitatSimHighResCoarseOccSensor"
+# -----------------------------------------------------------------------------
+# # OCCUPANCY MAPS
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.OCCUPANCY_MAPS = CN()
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT = SENSOR.HEIGHT
+_C.SIMULATOR.OCCUPANCY_MAPS.WIDTH = SENSOR.WIDTH
+_C.SIMULATOR.OCCUPANCY_MAPS.MAP_SCALE = 0.1
+_C.SIMULATOR.OCCUPANCY_MAPS.MAP_SIZE = 800
+_C.SIMULATOR.OCCUPANCY_MAPS.MAX_DEPTH = 3
+_C.SIMULATOR.OCCUPANCY_MAPS.SMALL_MAP_RANGE = 20
+_C.SIMULATOR.OCCUPANCY_MAPS.LARGE_MAP_RANGE = 100
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT_LOWER = 0.2
+_C.SIMULATOR.OCCUPANCY_MAPS.HEIGHT_UPPER = 1.5
+_C.SIMULATOR.OCCUPANCY_MAPS.GET_PROJ_LOC_MAP = False
+_C.SIMULATOR.OCCUPANCY_MAPS.GET_HIGHRES_LOC_MAP = False
+_C.SIMULATOR.OCCUPANCY_MAPS.USE_GT_OCC_MAP = False
+_C.SIMULATOR.OCCUPANCY_MAPS.MEASURE_NOISE_FREE_AREA = False
+_C.SIMULATOR.OCCUPANCY_MAPS.COVERAGE_NOVELTY_POOLING = "mean"
+# -----------------------------------------------------------------------------
+# # OBJECT ANNOTATIONS
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.OBJECT_ANNOTATIONS = CN()
+_C.SIMULATOR.OBJECT_ANNOTATIONS.IS_AVAILABLE = False
+_C.SIMULATOR.OBJECT_ANNOTATIONS.PATH = "./"
+# -----------------------------------------------------------------------------
+# # POSE RGB SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.POSE_RGB_SENSOR = SENSOR.clone()
+_C.TASK.POSE_RGB_SENSOR.TYPE = "PoseEstimationRGBSensor"
+_C.TASK.POSE_RGB_SENSOR.NREF = 20
+# -----------------------------------------------------------------------------
+# # POSE DEPTH SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.POSE_DEPTH_SENSOR = _C.SIMULATOR.DEPTH_SENSOR.clone()
+_C.TASK.POSE_DEPTH_SENSOR.TYPE = "PoseEstimationDepthSensor"
+_C.TASK.POSE_DEPTH_SENSOR.NREF = 20
+# -----------------------------------------------------------------------------
+# # POSE MASK SENSOR
+# -----------------------------------------------------------------------------
+_C.TASK.POSE_MASK_SENSOR = CN()
+_C.TASK.POSE_MASK_SENSOR.TYPE = "PoseEstimationMaskSensor"
+_C.TASK.POSE_MASK_SENSOR.NREF = 20
+# -----------------------------------------------------------------------------
+# AGENT
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.AGENT_0 = CN()
+_C.SIMULATOR.AGENT_0.HEIGHT = 1.5
+_C.SIMULATOR.AGENT_0.RADIUS = 0.1
+_C.SIMULATOR.AGENT_0.MASS = 32.0
+_C.SIMULATOR.AGENT_0.LINEAR_ACCELERATION = 20.0
+_C.SIMULATOR.AGENT_0.ANGULAR_ACCELERATION = 4 * 3.14
+_C.SIMULATOR.AGENT_0.LINEAR_FRICTION = 0.5
+_C.SIMULATOR.AGENT_0.ANGULAR_FRICTION = 1.0
+_C.SIMULATOR.AGENT_0.COEFFICIENT_OF_RESTITUTION = 0.0
+_C.SIMULATOR.AGENT_0.SENSORS = ["RGB_SENSOR"]
+_C.SIMULATOR.AGENT_0.IS_SET_START_STATE = False
+_C.SIMULATOR.AGENT_0.START_POSITION = [0, 0, 0]
+_C.SIMULATOR.AGENT_0.START_ROTATION = [0, 0, 0, 1]
+_C.SIMULATOR.AGENTS = ["AGENT_0"]
+# -----------------------------------------------------------------------------
+# SIMULATOR HABITAT_SIM_V0
+# -----------------------------------------------------------------------------
+_C.SIMULATOR.HABITAT_SIM_V0 = CN()
+_C.SIMULATOR.HABITAT_SIM_V0.GPU_DEVICE_ID = 0
+# -----------------------------------------------------------------------------
+# DATASET
+# -----------------------------------------------------------------------------
+_C.DATASET = CN()
+_C.DATASET.TYPE = "PoseEstimation-v1"
+_C.DATASET.SPLIT = "train"
+_C.DATASET.SCENES_DIR = "data/scene_datasets"
+_C.DATASET.NUM_EPISODE_SAMPLE = -1
+_C.DATASET.CONTENT_SCENES = ["*"]
+_C.DATASET.DATA_PATH = "data/datasets/pose_estimation/mp3d/v1/{split}/{split}.json.gz"
+_C.DATASET.SHUFFLE_DATASET = True
+
+
+# -----------------------------------------------------------------------------
+
+
+def get_config_pose(
+    config_paths: Optional[Union[List[str], str]] = None, opts: Optional[list] = None,
+) -> CN:
+    r"""Create a unified config with default values overwritten by values from
+    `config_paths` and overwritten by options from `opts`.
+    Args:
+        config_paths: List of config paths or string that contains comma
+        separated list of config paths.
+        opts: Config options (keys, values) in a list (e.g., passed from
+        command line into the config. For example, `opts = ['FOO.BAR',
+        0.5]`. Argument can be used for parameter sweeping or quick tests.
+    """
+    config = _C.clone()
+    if config_paths:
+        if isinstance(config_paths, str):
+            if CONFIG_FILE_SEPARATOR in config_paths:
+                config_paths = config_paths.split(CONFIG_FILE_SEPARATOR)
+            else:
+                config_paths = [config_paths]
+
+        for config_path in config_paths:
+            config.merge_from_file(config_path)
+
+    if opts:
+        config.merge_from_list(opts)
+
+    config.freeze()
+    return config
diff --git habitat/core/benchmark.py habitat/core/benchmark.py
index 296509c..bcd4573 100644
--- habitat/core/benchmark.py
+++ habitat/core/benchmark.py
@@ -9,10 +9,12 @@ the ``config_env`` parameter in constructor. The evaluation is task agnostic
 and is implemented through metrics defined for ``habitat.EmbodiedTask``.
 """
 
+import pdb
 from collections import defaultdict
 from typing import Dict, Optional
 
 from habitat.config.default import get_config
+from habitat.config.default_pose import get_config_pose
 from habitat.core.agent import Agent
 from habitat.core.env import Env
 
@@ -26,7 +28,8 @@ class Benchmark:
     """
 
     def __init__(self, config_paths: Optional[str] = None) -> None:
-        config_env = get_config(config_paths)
+        # config_env = get_config(config_paths)
+        config_env = get_config_pose(config_paths)
         self._env = Env(config=config_env)
 
     def evaluate(
@@ -47,9 +50,7 @@ class Benchmark:
         else:
             assert num_episodes <= len(self._env.episodes), (
                 "num_episodes({}) is larger than number of episodes "
-                "in environment ({})".format(
-                    num_episodes, len(self._env.episodes)
-                )
+                "in environment ({})".format(num_episodes, len(self._env.episodes))
             )
 
         assert num_episodes > 0, "num_episodes should be greater than 0"
diff --git habitat/core/dataset.py habitat/core/dataset.py
index b0bc35a..b98a686 100644
--- habitat/core/dataset.py
+++ habitat/core/dataset.py
@@ -39,12 +39,8 @@ class Episode:
 
     episode_id: str = attr.ib(default=None, validator=not_none_validator)
     scene_id: str = attr.ib(default=None, validator=not_none_validator)
-    start_position: List[float] = attr.ib(
-        default=None, validator=not_none_validator
-    )
-    start_rotation: List[float] = attr.ib(
-        default=None, validator=not_none_validator
-    )
+    start_position: List[float] = attr.ib(default=None, validator=not_none_validator)
+    start_rotation: List[float] = attr.ib(default=None, validator=not_none_validator)
     info: Optional[Dict[str, str]] = None
 
 
@@ -59,6 +55,7 @@ class Dataset(Generic[T]):
     """
 
     episodes: List[T]
+    shuffle_dataset: bool = True
 
     @property
     def scene_ids(self) -> List[str]:
@@ -76,9 +73,7 @@ class Dataset(Generic[T]):
         Returns:
             list of episodes for the ``scene_id``.
         """
-        return list(
-            filter(lambda x: x.scene_id == scene_id, iter(self.episodes))
-        )
+        return list(filter(lambda x: x.scene_id == scene_id, iter(self.episodes)))
 
     def get_episodes(self, indexes: List[int]) -> List[T]:
         r"""
@@ -98,8 +93,25 @@ class Dataset(Generic[T]):
             iterator for episodes
         """
         # TODO: support shuffling between epoch and  scene switching
+        if self.shuffle_dataset:
+            self.shuffle_episodes(50)
+            print("=====> Dataset: Shuffling episodes!")
+        if len(self.episodes) < 15:
+            print(
+                "Set of episodes used: {}".format(
+                    [ep.episode_id for ep in self.episodes]
+                )
+            )
         return cycle(self.episodes)
 
+    def shuffle_episodes(self, shuffle_interval):
+        ranges = np.arange(0, len(self.episodes), shuffle_interval)
+        np.random.shuffle(ranges)
+        new_episodes = []
+        for r in ranges:
+            new_episodes += self.episodes[r : r + shuffle_interval]
+        self.episodes = new_episodes
+
     def to_json(self) -> str:
         class DatasetJSONEncoder(json.JSONEncoder):
             def default(self, object):
@@ -108,9 +120,7 @@ class Dataset(Generic[T]):
         result = DatasetJSONEncoder().encode(self)
         return result
 
-    def from_json(
-        self, json_str: str, scenes_dir: Optional[str] = None
-    ) -> None:
+    def from_json(self, json_str: str, scenes_dir: Optional[str] = None) -> None:
         r"""
         Creates dataset from ``json_str``. Directory containing relevant 
         graphical assets of scenes is passed through ``scenes_dir``.
@@ -122,9 +132,7 @@ class Dataset(Generic[T]):
         """
         raise NotImplementedError
 
-    def filter_episodes(
-        self, filter_fn: Callable[[Episode], bool]
-    ) -> "Dataset":
+    def filter_episodes(self, filter_fn: Callable[[Episode], bool]) -> "Dataset":
         r"""
         Returns a new dataset with only the filtered episodes from the 
         original dataset.
@@ -196,9 +204,7 @@ class Dataset(Generic[T]):
         if allow_uneven_splits:
             stride = int(np.ceil(len(self.episodes) * 1.0 / num_splits))
             split_lengths = [stride] * (num_splits - 1)
-            split_lengths.append(
-                (len(self.episodes) - stride * (num_splits - 1))
-            )
+            split_lengths.append((len(self.episodes) - stride * (num_splits - 1)))
         else:
             if episodes_per_split is not None:
                 stride = episodes_per_split
@@ -208,9 +214,7 @@ class Dataset(Generic[T]):
 
         num_episodes = sum(split_lengths)
 
-        rand_items = np.random.choice(
-            len(self.episodes), num_episodes, replace=False
-        )
+        rand_items = np.random.choice(len(self.episodes), num_episodes, replace=False)
         if collate_scene_ids:
             scene_ids = {}
             for rand_ind in rand_items:
@@ -247,9 +251,5 @@ class Dataset(Generic[T]):
         if num_episodes == -1:
             return
         if num_episodes < -1:
-            raise ValueError(
-                f"Invalid number for episodes to sample: {num_episodes}"
-            )
-        self.episodes = np.random.choice(
-            self.episodes, num_episodes, replace=False
-        )
+            raise ValueError(f"Invalid number for episodes to sample: {num_episodes}")
+        self.episodes = np.random.choice(self.episodes, num_episodes, replace=False)
diff --git habitat/core/embodied_task.py habitat/core/embodied_task.py
index d9e063b..83e1b03 100644
--- habitat/core/embodied_task.py
+++ habitat/core/embodied_task.py
@@ -62,9 +62,7 @@ class Metrics(dict):
     """
 
     def __init__(self, measures: Dict[str, Measure]) -> None:
-        data = [
-            (uuid, measure.get_metric()) for uuid, measure in measures.items()
-        ]
+        data = [(uuid, measure.get_metric()) for uuid, measure in measures.items()]
         super().__init__(data)
 
 
diff --git habitat/core/env.py habitat/core/env.py
index 7141050..1b9baed 100644
--- habitat/core/env.py
+++ habitat/core/env.py
@@ -56,12 +56,9 @@ class Env:
     _episode_start_time: Optional[float]
     _episode_over: bool
 
-    def __init__(
-        self, config: Config, dataset: Optional[Dataset] = None
-    ) -> None:
+    def __init__(self, config: Config, dataset: Optional[Dataset] = None) -> None:
         assert config.is_frozen(), (
-            "Freeze the config before creating the "
-            "environment, use config.freeze()."
+            "Freeze the config before creating the " "environment, use config.freeze()."
         )
         self._config = config
         self._dataset = dataset
@@ -99,9 +96,7 @@ class Env:
             }
         )
         self.action_space = self._sim.action_space
-        self._max_episode_seconds = (
-            self._config.ENVIRONMENT.MAX_EPISODE_SECONDS
-        )
+        self._max_episode_seconds = self._config.ENVIRONMENT.MAX_EPISODE_SECONDS
         self._max_episode_steps = self._config.ENVIRONMENT.MAX_EPISODE_STEPS
         self._elapsed_steps = 0
         self._episode_start_time: Optional[float] = None
@@ -130,9 +125,7 @@ class Env:
 
     @episodes.setter
     def episodes(self, episodes: List[Type[Episode]]) -> None:
-        assert (
-            len(episodes) > 0
-        ), "Environment doesn't accept empty episodes list."
+        assert len(episodes) > 0, "Environment doesn't accept empty episodes list."
         self._episodes = episodes
 
     @property
@@ -190,6 +183,8 @@ class Env:
         assert len(self.episodes) > 0, "Episodes list is empty"
 
         self.current_episode = next(self._episode_iterator)
+        episode_id = self.current_episode.episode_id
+        scene_id = self.current_episode.scene_id
         self.reconfigure(self._config)
 
         observations = self._sim.reset()
@@ -278,9 +273,7 @@ class RLEnv(gym.Env):
 
     _env: Env
 
-    def __init__(
-        self, config: Config, dataset: Optional[Dataset] = None
-    ) -> None:
+    def __init__(self, config: Config, dataset: Optional[Dataset] = None) -> None:
         self._env = Env(config, dataset)
         self.observation_space = self._env.observation_space
         self.action_space = self._env.action_space
diff --git habitat/core/registry.py habitat/core/registry.py
index bf224e5..b85c181 100644
--- habitat/core/registry.py
+++ habitat/core/registry.py
@@ -38,9 +38,7 @@ class Registry(metaclass=Singleton):
             if assert_type is not None:
                 assert issubclass(
                     to_register, assert_type
-                ), "{} must be a subclass of {}".format(
-                    to_register, assert_type
-                )
+                ), "{} must be a subclass of {}".format(to_register, assert_type)
 
             cls.mapping[_type][
                 to_register.__name__ if name is None else name
@@ -80,14 +78,10 @@ class Registry(metaclass=Singleton):
         """
         from habitat.core.embodied_task import EmbodiedTask
 
-        return cls._register_impl(
-            "task", to_register, name, assert_type=EmbodiedTask
-        )
+        return cls._register_impl("task", to_register, name, assert_type=EmbodiedTask)
 
     @classmethod
-    def register_simulator(
-        cls, to_register=None, *, name: Optional[str] = None
-    ):
+    def register_simulator(cls, to_register=None, *, name: Optional[str] = None):
         r"""Register a simulator to registry with key 'name'
 
         Args:
@@ -113,9 +107,7 @@ class Registry(metaclass=Singleton):
         """
         from habitat.core.simulator import Simulator
 
-        return cls._register_impl(
-            "sim", to_register, name, assert_type=Simulator
-        )
+        return cls._register_impl("sim", to_register, name, assert_type=Simulator)
 
     @classmethod
     def register_sensor(cls, to_register=None, *, name: Optional[str] = None):
@@ -128,9 +120,7 @@ class Registry(metaclass=Singleton):
         """
         from habitat.core.simulator import Sensor
 
-        return cls._register_impl(
-            "sensor", to_register, name, assert_type=Sensor
-        )
+        return cls._register_impl("sensor", to_register, name, assert_type=Sensor)
 
     @classmethod
     def register_measure(cls, to_register=None, *, name: Optional[str] = None):
@@ -143,9 +133,7 @@ class Registry(metaclass=Singleton):
         """
         from habitat.core.embodied_task import Measure
 
-        return cls._register_impl(
-            "measure", to_register, name, assert_type=Measure
-        )
+        return cls._register_impl("measure", to_register, name, assert_type=Measure)
 
     @classmethod
     def register_dataset(cls, to_register=None, *, name: Optional[str] = None):
@@ -158,9 +146,7 @@ class Registry(metaclass=Singleton):
         """
         from habitat.core.dataset import Dataset
 
-        return cls._register_impl(
-            "dataset", to_register, name, assert_type=Dataset
-        )
+        return cls._register_impl("dataset", to_register, name, assert_type=Dataset)
 
     @classmethod
     def register_action_space_configuration(
diff --git habitat/core/simulator.py habitat/core/simulator.py
index 7effea5..212fbc1 100644
--- habitat/core/simulator.py
+++ habitat/core/simulator.py
@@ -27,10 +27,10 @@ class ActionSpaceConfiguration(abc.ABC):
 
 
 class _DefaultSimulatorActions(Enum):
-    STOP = 0
-    MOVE_FORWARD = 1
-    TURN_LEFT = 2
-    TURN_RIGHT = 3
+    MOVE_FORWARD = 0
+    TURN_LEFT = 1
+    TURN_RIGHT = 2
+    STOP = 3
     LOOK_UP = 4
     LOOK_DOWN = 5
 
@@ -69,9 +69,7 @@ class SimulatorActionsSingleton(metaclass=Singleton):
             SimulatorActions.extend_action_space("MY_ACTION")
             print(SimulatorActions.MY_ACTION)
         """
-        assert (
-            name not in self._known_actions
-        ), "Cannot register an action name twice"
+        assert name not in self._known_actions, "Cannot register an action name twice"
         self._known_actions[name] = len(self._known_actions)
 
         return self._known_actions[name]
@@ -171,9 +169,7 @@ class Observations(dict):
         sensors: list of sensors whose observations are fetched and packaged.
     """
 
-    def __init__(
-        self, sensors: Dict[str, Sensor], *args: Any, **kwargs: Any
-    ) -> None:
+    def __init__(self, sensors: Dict[str, Sensor], *args: Any, **kwargs: Any) -> None:
         data = [
             (uuid, sensor.get_observation(*args, **kwargs))
             for uuid, sensor in sensors.items()
@@ -232,6 +228,74 @@ class SemanticSensor(Sensor):
         raise NotImplementedError
 
 
+class FineOccSensor(Sensor):
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
+        super().__init__(*args, **kwargs)
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "fine_occupancy"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any) -> SensorTypes:
+        return SensorTypes.COLOR
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any) -> Space:
+        raise NotImplementedError
+
+    def get_observation(self, *args: Any, **kwargs: Any) -> Any:
+        raise NotImplementedError
+
+
+class CoarseOccSensor(Sensor):
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
+        super().__init__(*args, **kwargs)
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "coarse_occupancy"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any) -> SensorTypes:
+        return SensorTypes.COLOR
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any) -> Space:
+        raise NotImplementedError
+
+    def get_observation(self, *args: Any, **kwargs: Any) -> Any:
+        raise NotImplementedError
+
+
+class HighResCoarseOccSensor(Sensor):
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
+        super().__init__(*args, **kwargs)
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "highres_coarse_occupancy"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any) -> SensorTypes:
+        return SensorTypes.COLOR
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any) -> Space:
+        raise NotImplementedError
+
+    def get_observation(self, *args: Any, **kwargs: Any) -> Any:
+        raise NotImplementedError
+
+
+class ProjOccSensor(Sensor):
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
+        super().__init__(*args, **kwargs)
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "proj_occupancy"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any) -> SensorTypes:
+        return SensorTypes.COLOR
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any) -> Space:
+        raise NotImplementedError
+
+    def get_observation(self, *args: Any, **kwargs: Any) -> Any:
+        raise NotImplementedError
+
+
 class SensorSuite:
     r"""Represents a set of sensors, with each sensor being identified
     through a unique id.
@@ -271,9 +335,7 @@ class AgentState:
     position: List[float]
     rotation: Optional[List[float]]
 
-    def __init__(
-        self, position: List[float], rotation: Optional[List[float]]
-    ) -> None:
+    def __init__(self, position: List[float], rotation: Optional[List[float]]) -> None:
         self.position = position
         self.rotation = rotation
 
diff --git habitat/core/utils.py habitat/core/utils.py
index e2fa28b..132279c 100644
--- habitat/core/utils.py
+++ habitat/core/utils.py
@@ -26,19 +26,14 @@ def tile_images(images: List[np.ndarray]) -> np.ndarray:
     new_width = int(np.ceil(float(n_images) / new_height))
     # pad with empty images to complete the rectangle
     np_images = np.array(
-        images
-        + [images[0] * 0 for _ in range(n_images, new_height * new_width)]
+        images + [images[0] * 0 for _ in range(n_images, new_height * new_width)]
     )
     # img_HWhwc
-    out_image = np_images.reshape(
-        new_height, new_width, height, width, n_channels
-    )
+    out_image = np_images.reshape(new_height, new_width, height, width, n_channels)
     # img_HhWwc
     out_image = out_image.transpose(0, 2, 1, 3, 4)
     # img_Hh_Ww_c
-    out_image = out_image.reshape(
-        new_height * height, new_width * width, n_channels
-    )
+    out_image = out_image.reshape(new_height * height, new_width * width, n_channels)
     return out_image
 
 
@@ -52,7 +47,5 @@ class Singleton(type):
 
     def __call__(cls, *args, **kwargs):
         if cls not in cls._instances:
-            cls._instances[cls] = super(Singleton, cls).__call__(
-                *args, **kwargs
-            )
+            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)
         return cls._instances[cls]
diff --git habitat/core/vector_env.py habitat/core/vector_env.py
index 29a4c23..7312d77 100644
--- habitat/core/vector_env.py
+++ habitat/core/vector_env.py
@@ -112,14 +112,10 @@ class VectorEnv:
 
         for write_fn in self._connection_write_fns:
             write_fn((OBSERVATION_SPACE_COMMAND, None))
-        self.observation_spaces = [
-            read_fn() for read_fn in self._connection_read_fns
-        ]
+        self.observation_spaces = [read_fn() for read_fn in self._connection_read_fns]
         for write_fn in self._connection_write_fns:
             write_fn((ACTION_SPACE_COMMAND, None))
-        self.action_spaces = [
-            read_fn() for read_fn in self._connection_read_fns
-        ]
+        self.action_spaces = [read_fn() for read_fn in self._connection_read_fns]
         self._paused = []
 
     @property
@@ -150,9 +146,7 @@ class VectorEnv:
             while command != CLOSE_COMMAND:
                 if command == STEP_COMMAND:
                     # different step methods for habitat.RLEnv and habitat.Env
-                    if isinstance(env, habitat.RLEnv) or isinstance(
-                        env, gym.Env
-                    ):
+                    if isinstance(env, habitat.RLEnv) or isinstance(env, gym.Env):
                         # habitat.RLEnv
                         observations, reward, done, info = env.step(data)
                         if auto_reset_done and done:
@@ -374,10 +368,7 @@ class VectorEnv:
         self._paused = []
 
     def call_at(
-        self,
-        index: int,
-        function_name: str,
-        function_args: Optional[List[Any]] = None,
+        self, index: int, function_name: str, function_args: Optional[List[Any]] = None,
     ) -> Any:
         r"""Calls a function (which is passed by name) on the selected env and
         returns the result.
@@ -399,9 +390,7 @@ class VectorEnv:
         return result
 
     def call(
-        self,
-        function_names: List[str],
-        function_args_list: Optional[List[Any]] = None,
+        self, function_names: List[str], function_args_list: Optional[List[Any]] = None,
     ) -> List[Any]:
         r"""Calls a list of functions (which are passed by name) on the
         corresponding env (by index).
@@ -420,9 +409,7 @@ class VectorEnv:
             function_args_list = [None] * len(function_names)
         assert len(function_names) == len(function_args_list)
         func_args = zip(function_names, function_args_list)
-        for write_fn, func_args_on in zip(
-            self._connection_write_fns, func_args
-        ):
+        for write_fn, func_args_on in zip(self._connection_write_fns, func_args):
             write_fn((CALL_COMMAND, func_args_on))
         results = []
         for read_fn in self._connection_read_fns:
@@ -430,9 +417,7 @@ class VectorEnv:
         self._is_waiting = False
         return results
 
-    def render(
-        self, mode: str = "human", *args, **kwargs
-    ) -> Union[np.ndarray, None]:
+    def render(self, mode: str = "human", *args, **kwargs) -> Union[np.ndarray, None]:
         r"""Render observations from all environments in a tiled image.
         """
         for write_fn in self._connection_write_fns:
diff --git habitat/datasets/eqa/mp3d_eqa_dataset.py habitat/datasets/eqa/mp3d_eqa_dataset.py
index 4194fb5..7f63a6d 100644
--- habitat/datasets/eqa/mp3d_eqa_dataset.py
+++ habitat/datasets/eqa/mp3d_eqa_dataset.py
@@ -54,9 +54,7 @@ class Matterport3dDatasetV1(Dataset):
 
         self.sample_episodes(config.NUM_EPISODE_SAMPLE)
 
-    def from_json(
-        self, json_str: str, scenes_dir: Optional[str] = None
-    ) -> None:
+    def from_json(self, json_str: str, scenes_dir: Optional[str] = None) -> None:
         deserialized = json.loads(json_str)
         self.__dict__.update(deserialized)
         for ep_index, episode in enumerate(deserialized["episodes"]):
diff --git habitat/datasets/exp_nav/.gitignore habitat/datasets/exp_nav/.gitignore
new file mode 100644
index 0000000..bee8a64
--- /dev/null
+++ habitat/datasets/exp_nav/.gitignore
@@ -0,0 +1 @@
+__pycache__
diff --git habitat/datasets/exp_nav/__init__.py habitat/datasets/exp_nav/__init__.py
new file mode 100644
index 0000000..240697e
--- /dev/null
+++ habitat/datasets/exp_nav/__init__.py
@@ -0,0 +1,5 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
diff --git habitat/datasets/exp_nav/exp_nav_dataset.py habitat/datasets/exp_nav/exp_nav_dataset.py
new file mode 100644
index 0000000..6712a4f
--- /dev/null
+++ habitat/datasets/exp_nav/exp_nav_dataset.py
@@ -0,0 +1,129 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+import os
+import pdb
+import gzip
+import json
+import numpy as np
+from typing import List, Optional
+
+from habitat.config import Config
+from habitat.core.dataset import Dataset
+from habitat.core.registry import registry
+from habitat.tasks.exp_nav.exp_nav_task import (
+    ExploreNavigationEpisode,
+    ExploreNavigationGoal,
+    ShortestPathPoint,
+)
+
+ALL_SCENES_MASK = "*"
+CONTENT_SCENES_PATH_FIELD = "content_scenes_path"
+DEFAULT_SCENE_PATH_PREFIX = "data/scene_datasets/"
+
+
+@registry.register_dataset(name="ExpNav-v1")
+class ExpNavDatasetV1(Dataset):
+    r"""Class inherited from Dataset that loads Explore Navigation dataset.
+    """
+
+    episodes: List[ExploreNavigationEpisode]
+    content_scenes_path: str = "{data_path}/content/{scene}.json.gz"
+    shuffle_dataset: bool = True
+
+    @staticmethod
+    def check_config_paths_exist(config: Config) -> bool:
+        return os.path.exists(
+            config.DATA_PATH.format(split=config.SPLIT)
+        ) and os.path.exists(config.SCENES_DIR)
+
+    @staticmethod
+    def get_scenes_to_load(config: Config) -> List[str]:
+        r"""Return list of scene ids for which dataset has separate files with
+        episodes.
+        """
+        assert ExpNavDatasetV1.check_config_paths_exist(config)
+        dataset_dir = os.path.dirname(config.DATA_PATH.format(split=config.SPLIT))
+
+        cfg = config.clone()
+        cfg.defrost()
+        cfg.CONTENT_SCENES = []
+        dataset = ExpNavDatasetV1(cfg)
+        return ExpNavDatasetV1._get_scenes_from_folder(
+            content_scenes_path=dataset.content_scenes_path, dataset_dir=dataset_dir,
+        )
+
+    @staticmethod
+    def _get_scenes_from_folder(content_scenes_path, dataset_dir):
+        scenes = []
+        content_dir = content_scenes_path.split("{scene}")[0]
+        scene_dataset_ext = content_scenes_path.split("{scene}")[1]
+        content_dir = content_dir.format(data_path=dataset_dir)
+        if not os.path.exists(content_dir):
+            return scenes
+
+        for filename in os.listdir(content_dir):
+            if filename.endswith(scene_dataset_ext):
+                scene = filename[: -len(scene_dataset_ext)]
+                scenes.append(scene)
+        scenes.sort()
+        return scenes
+
+    def __init__(self, config: Optional[Config] = None) -> None:
+        self.episodes = []
+
+        if config is None:
+            return
+
+        self.shuffle_dataset = config.SHUFFLE_DATASET
+
+        datasetfile_path = config.DATA_PATH.format(split=config.SPLIT)
+        with gzip.open(datasetfile_path, "rt") as f:
+            self.from_json(f.read(), scenes_dir=config.SCENES_DIR)
+
+        # Read separate file for each scene
+        dataset_dir = os.path.dirname(datasetfile_path)
+        scenes = config.CONTENT_SCENES
+        if ALL_SCENES_MASK in scenes:
+            scenes = ExpNavDatasetV1._get_scenes_from_folder(
+                content_scenes_path=self.content_scenes_path, dataset_dir=dataset_dir,
+            )
+
+        for scene in scenes:
+            scene_filename = self.content_scenes_path.format(
+                data_path=dataset_dir, scene=scene
+            )
+            with gzip.open(scene_filename, "rt") as f:
+                self.from_json(f.read(), scenes_dir=config.SCENES_DIR)
+
+        self.sample_episodes(config.NUM_EPISODE_SAMPLE)
+
+    def from_json(self, json_str: str, scenes_dir: Optional[str] = None) -> None:
+        deserialized = json.loads(json_str)
+        if CONTENT_SCENES_PATH_FIELD in deserialized:
+            self.content_scenes_path = deserialized[CONTENT_SCENES_PATH_FIELD]
+
+        for episode in deserialized["episodes"]:
+            episode = ExploreNavigationEpisode(**episode)
+
+            if scenes_dir is not None:
+                if episode.scene_id.startswith(DEFAULT_SCENE_PATH_PREFIX):
+                    episode.scene_id = episode.scene_id[
+                        len(DEFAULT_SCENE_PATH_PREFIX) :
+                    ]
+
+                episode.scene_id = os.path.join(scenes_dir, episode.scene_id)
+
+            for g_index, goal in enumerate(episode.goals):
+                episode.goals[g_index] = ExploreNavigationGoal(**goal)
+            if episode.shortest_paths is not None:
+                for path in episode.shortest_paths:
+                    for p_index, point in enumerate(path):
+                        path[p_index] = ShortestPathPoint(**point)
+
+            self.episodes.append(episode)
+
+        print(f"======> Effective number of episodes: {len(self.episodes)}")
diff --git habitat/datasets/pointnav/pointnav_dataset.py habitat/datasets/pointnav/pointnav_dataset.py
index 70caad5..9ba2a31 100644
--- habitat/datasets/pointnav/pointnav_dataset.py
+++ habitat/datasets/pointnav/pointnav_dataset.py
@@ -7,6 +7,7 @@
 import gzip
 import json
 import os
+import pdb
 from typing import List, Optional
 
 from habitat.config import Config
@@ -30,6 +31,7 @@ class PointNavDatasetV1(Dataset):
 
     episodes: List[NavigationEpisode]
     content_scenes_path: str = "{data_path}/content/{scene}.json.gz"
+    shuffle_dataset: bool = True
 
     @staticmethod
     def check_config_paths_exist(config: Config) -> bool:
@@ -43,17 +45,14 @@ class PointNavDatasetV1(Dataset):
         episodes.
         """
         assert PointNavDatasetV1.check_config_paths_exist(config)
-        dataset_dir = os.path.dirname(
-            config.DATA_PATH.format(split=config.SPLIT)
-        )
+        dataset_dir = os.path.dirname(config.DATA_PATH.format(split=config.SPLIT))
 
         cfg = config.clone()
         cfg.defrost()
         cfg.CONTENT_SCENES = []
         dataset = PointNavDatasetV1(cfg)
         return PointNavDatasetV1._get_scenes_from_folder(
-            content_scenes_path=dataset.content_scenes_path,
-            dataset_dir=dataset_dir,
+            content_scenes_path=dataset.content_scenes_path, dataset_dir=dataset_dir,
         )
 
     @staticmethod
@@ -78,6 +77,8 @@ class PointNavDatasetV1(Dataset):
         if config is None:
             return
 
+        self.shuffle_dataset = config.SHUFFLE_DATASET
+
         datasetfile_path = config.DATA_PATH.format(split=config.SPLIT)
         with gzip.open(datasetfile_path, "rt") as f:
             self.from_json(f.read(), scenes_dir=config.SCENES_DIR)
@@ -87,8 +88,7 @@ class PointNavDatasetV1(Dataset):
         scenes = config.CONTENT_SCENES
         if ALL_SCENES_MASK in scenes:
             scenes = PointNavDatasetV1._get_scenes_from_folder(
-                content_scenes_path=self.content_scenes_path,
-                dataset_dir=dataset_dir,
+                content_scenes_path=self.content_scenes_path, dataset_dir=dataset_dir,
             )
 
         for scene in scenes:
@@ -100,9 +100,7 @@ class PointNavDatasetV1(Dataset):
 
         self.sample_episodes(config.NUM_EPISODE_SAMPLE)
 
-    def from_json(
-        self, json_str: str, scenes_dir: Optional[str] = None
-    ) -> None:
+    def from_json(self, json_str: str, scenes_dir: Optional[str] = None) -> None:
         deserialized = json.loads(json_str)
         if CONTENT_SCENES_PATH_FIELD in deserialized:
             self.content_scenes_path = deserialized[CONTENT_SCENES_PATH_FIELD]
diff --git habitat/datasets/pointnav/pointnav_generator.py habitat/datasets/pointnav/pointnav_generator.py
index 047876a..740446a 100644
--- habitat/datasets/pointnav/pointnav_generator.py
+++ habitat/datasets/pointnav/pointnav_generator.py
@@ -33,9 +33,7 @@ def _ratio_sample_rate(ratio: float, ratio_threshold: float) -> float:
     return 20 * (ratio - 0.98) ** 2
 
 
-def is_compatible_episode(
-    s, t, sim, near_dist, far_dist, geodesic_to_euclid_ratio
-):
+def is_compatible_episode(s, t, sim, near_dist, far_dist, geodesic_to_euclid_ratio):
     euclid_dist = np.power(np.power(np.array(s) - np.array(t), 2).sum(0), 0.5)
     if np.abs(s[1] - t[1]) > 0.5:  # check height difference to assure s and
         #  t are from same floor
@@ -47,8 +45,7 @@ def is_compatible_episode(
         return False, 0
     distances_ratio = d_separation / euclid_dist
     if distances_ratio < geodesic_to_euclid_ratio and (
-        np.random.rand()
-        > _ratio_sample_rate(distances_ratio, geodesic_to_euclid_ratio)
+        np.random.rand() > _ratio_sample_rate(distances_ratio, geodesic_to_euclid_ratio)
     ):
         return False, 0
     if sim.island_radius(s) < ISLAND_RADIUS_LIMIT:
diff --git habitat/datasets/pose_estimation/__init__.py habitat/datasets/pose_estimation/__init__.py
new file mode 100644
index 0000000..240697e
--- /dev/null
+++ habitat/datasets/pose_estimation/__init__.py
@@ -0,0 +1,5 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
diff --git habitat/datasets/pose_estimation/pose_estimation_dataset.py habitat/datasets/pose_estimation/pose_estimation_dataset.py
new file mode 100644
index 0000000..0141b3a
--- /dev/null
+++ habitat/datasets/pose_estimation/pose_estimation_dataset.py
@@ -0,0 +1,134 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+import gzip
+import json
+import os
+from itertools import cycle
+from typing import List, Optional
+
+from habitat.config import Config
+from habitat.core.dataset import Dataset
+from habitat.core.registry import registry
+from habitat.tasks.pose_estimation.pose_estimation_task import (
+    PoseEstimationEpisode,
+    ShortestPathPoint,
+)
+
+ALL_SCENES_MASK = "*"
+CONTENT_SCENES_PATH_FIELD = "content_scenes_path"
+DEFAULT_SCENE_PATH_PREFIX = "data/scene_datasets/"
+
+
+@registry.register_dataset(name="PoseEstimation-v1")
+class PoseEstimationDatasetV1(Dataset):
+    r"""Class inherited from Dataset that loads Pose Estimation dataset.
+    """
+
+    episodes: List[PoseEstimationEpisode]
+    content_scenes_path: str = "{data_path}/content/{scene}.json.gz"
+    shuffle_dataset: bool = True
+
+    @staticmethod
+    def check_config_paths_exist(config: Config) -> bool:
+        return os.path.exists(
+            config.DATA_PATH.format(split=config.SPLIT)
+        ) and os.path.exists(config.SCENES_DIR)
+
+    @staticmethod
+    def get_scenes_to_load(config: Config) -> List[str]:
+        r"""Return list of scene ids for which dataset has separate files with
+        episodes.
+        """
+        assert PoseEstimationDatasetV1.check_config_paths_exist(config)
+        dataset_dir = os.path.dirname(config.DATA_PATH.format(split=config.SPLIT))
+
+        cfg = config.clone()
+        cfg.defrost()
+        cfg.CONTENT_SCENES = []
+        dataset = PoseEstimationDatasetV1(cfg)
+        return PoseEstimationDatasetV1._get_scenes_from_folder(
+            content_scenes_path=dataset.content_scenes_path, dataset_dir=dataset_dir,
+        )
+
+    @staticmethod
+    def _get_scenes_from_folder(content_scenes_path, dataset_dir):
+        scenes = []
+        content_dir = content_scenes_path.split("{scene}")[0]
+        scene_dataset_ext = content_scenes_path.split("{scene}")[1]
+        content_dir = content_dir.format(data_path=dataset_dir)
+        if not os.path.exists(content_dir):
+            return scenes
+
+        for filename in os.listdir(content_dir):
+            if filename.endswith(scene_dataset_ext):
+                scene = filename[: -len(scene_dataset_ext)]
+                scenes.append(scene)
+        scenes.sort()
+        return scenes
+
+    def __init__(self, config: Optional[Config] = None) -> None:
+        self.episodes = []
+
+        if config is None:
+            return
+
+        self.shuffle_dataset = config.SHUFFLE_DATASET
+
+        datasetfile_path = config.DATA_PATH.format(split=config.SPLIT)
+        with gzip.open(datasetfile_path, "rt") as f:
+            self.from_json(f.read(), scenes_dir=config.SCENES_DIR)
+
+        # Read separate file for each scene
+        dataset_dir = os.path.dirname(datasetfile_path)
+        scenes = config.CONTENT_SCENES
+        if ALL_SCENES_MASK in scenes:
+            scenes = PoseEstimationDatasetV1._get_scenes_from_folder(
+                content_scenes_path=self.content_scenes_path, dataset_dir=dataset_dir,
+            )
+
+        for scene in scenes:
+            scene_filename = self.content_scenes_path.format(
+                data_path=dataset_dir, scene=scene
+            )
+            with gzip.open(scene_filename, "rt") as f:
+                self.from_json(f.read(), scenes_dir=config.SCENES_DIR)
+
+        self.sample_episodes(config.NUM_EPISODE_SAMPLE)
+
+    def get_episode_iterator(self):
+        r"""
+        Creates and returns an iterator that iterates through self.episodes
+        in the desirable way specified.
+        Returns:
+            iterator for episodes
+        """
+        # TODO: support shuffling between epoch and scene switching
+        if self.shuffle_dataset:
+            self.shuffle_episodes(50)
+        return cycle(self.episodes)
+
+    def from_json(self, json_str: str, scenes_dir: Optional[str] = None) -> None:
+        deserialized = json.loads(json_str)
+        if CONTENT_SCENES_PATH_FIELD in deserialized:
+            self.content_scenes_path = deserialized[CONTENT_SCENES_PATH_FIELD]
+
+        for episode in deserialized["episodes"]:
+            episode = PoseEstimationEpisode(**episode)
+
+            if scenes_dir is not None:
+                if episode.scene_id.startswith(DEFAULT_SCENE_PATH_PREFIX):
+                    episode.scene_id = episode.scene_id[
+                        len(DEFAULT_SCENE_PATH_PREFIX) :
+                    ]
+
+                episode.scene_id = os.path.join(scenes_dir, episode.scene_id)
+
+            # Ignore edge case episodes where no references were sampled
+            if len(episode.pose_ref_positions) == 0:
+                continue
+
+            self.episodes.append(episode)
diff --git habitat/datasets/registration.py habitat/datasets/registration.py
index 265c43e..0a2a9e0 100644
--- habitat/datasets/registration.py
+++ habitat/datasets/registration.py
@@ -7,6 +7,10 @@
 from habitat.core.registry import registry
 from habitat.datasets.eqa.mp3d_eqa_dataset import Matterport3dDatasetV1
 from habitat.datasets.pointnav.pointnav_dataset import PointNavDatasetV1
+from habitat.datasets.pose_estimation.pose_estimation_dataset import (
+    PoseEstimationDatasetV1,
+)
+from habitat.datasets.exp_nav.exp_nav_dataset import ExpNavDatasetV1
 
 
 def make_dataset(id_dataset, **kwargs):
diff --git habitat/datasets/utils.py habitat/datasets/utils.py
index 9684d53..e485452 100644
--- habitat/datasets/utils.py
+++ habitat/datasets/utils.py
@@ -34,9 +34,7 @@ def get_action_shortest_path(
         state = sim.get_agent_state()
         shortest_path.append(
             ShortestPathPoint(
-                state.position.tolist(),
-                quaternion_to_list(state.rotation),
-                action,
+                state.position.tolist(), quaternion_to_list(state.rotation), action,
             )
         )
         sim.step(action)
diff --git habitat/sims/habitat_simulator/action_spaces.py habitat/sims/habitat_simulator/action_spaces.py
index 4cee1cb..dcb6240 100644
--- habitat/sims/habitat_simulator/action_spaces.py
+++ habitat/sims/habitat_simulator/action_spaces.py
@@ -18,38 +18,47 @@ class HabitatSimV0ActionSpaceConfiguration(ActionSpaceConfiguration):
             SimulatorActions.STOP: habitat_sim.ActionSpec("stop"),
             SimulatorActions.MOVE_FORWARD: habitat_sim.ActionSpec(
                 "move_forward",
-                habitat_sim.ActuationSpec(
-                    amount=self.config.FORWARD_STEP_SIZE
-                ),
+                habitat_sim.ActuationSpec(amount=self.config.FORWARD_STEP_SIZE),
             ),
             SimulatorActions.TURN_LEFT: habitat_sim.ActionSpec(
-                "turn_left",
-                habitat_sim.ActuationSpec(amount=self.config.TURN_ANGLE),
+                "turn_left", habitat_sim.ActuationSpec(amount=self.config.TURN_ANGLE),
             ),
             SimulatorActions.TURN_RIGHT: habitat_sim.ActionSpec(
-                "turn_right",
-                habitat_sim.ActuationSpec(amount=self.config.TURN_ANGLE),
+                "turn_right", habitat_sim.ActuationSpec(amount=self.config.TURN_ANGLE),
             ),
         }
 
 
 @registry.register_action_space_configuration(name="v1")
-class HabitatSimV1ActionSpaceConfiguration(
-    HabitatSimV0ActionSpaceConfiguration
-):
+class HabitatSimV1ActionSpaceConfiguration(HabitatSimV0ActionSpaceConfiguration):
     def get(self):
         config = super().get()
         new_config = {
             SimulatorActions.LOOK_UP: habitat_sim.ActionSpec(
-                "look_up",
-                habitat_sim.ActuationSpec(amount=self.config.TILT_ANGLE),
+                "look_up", habitat_sim.ActuationSpec(amount=self.config.TILT_ANGLE),
             ),
             SimulatorActions.LOOK_DOWN: habitat_sim.ActionSpec(
-                "look_down",
-                habitat_sim.ActuationSpec(amount=self.config.TILT_ANGLE),
+                "look_down", habitat_sim.ActuationSpec(amount=self.config.TILT_ANGLE),
             ),
         }
 
         config.update(new_config)
 
         return config
+
+
+@registry.register_action_space_configuration(name="v2")
+class HabitatSimV2ActionSpaceConfiguration(ActionSpaceConfiguration):
+    def get(self):
+        return {
+            SimulatorActions.MOVE_FORWARD: habitat_sim.ActionSpec(
+                "move_forward",
+                habitat_sim.ActuationSpec(amount=self.config.FORWARD_STEP_SIZE),
+            ),
+            SimulatorActions.TURN_LEFT: habitat_sim.ActionSpec(
+                "turn_left", habitat_sim.ActuationSpec(amount=self.config.TURN_ANGLE),
+            ),
+            SimulatorActions.TURN_RIGHT: habitat_sim.ActionSpec(
+                "turn_right", habitat_sim.ActuationSpec(amount=self.config.TURN_ANGLE),
+            ),
+        }
diff --git habitat/sims/habitat_simulator/habitat_simulator.py habitat/sims/habitat_simulator/habitat_simulator.py
index d77e13f..0f7ee9b 100644
--- habitat/sims/habitat_simulator/habitat_simulator.py
+++ habitat/sims/habitat_simulator/habitat_simulator.py
@@ -5,11 +5,20 @@
 # LICENSE file in the root directory of this source tree.
 
 from enum import Enum
-from typing import Any, List, Optional
-
+from typing import Any, List, Optional, Tuple
+
+import os
+import cv2
+import gzip
+import json
+import math
+import quaternion  # noqa # pylint: disable=unused-import
 import numpy as np
 from gym import Space, spaces
 
+import matplotlib.pyplot as plt
+from mpl_toolkits.mplot3d import Axes3D
+
 import habitat_sim
 from habitat.core.logging import logger
 from habitat.core.registry import registry
@@ -20,12 +29,27 @@ from habitat.core.simulator import (
     Observations,
     RGBSensor,
     SemanticSensor,
+    FineOccSensor,
+    CoarseOccSensor,
+    HighResCoarseOccSensor,
+    ProjOccSensor,
     Sensor,
     SensorSuite,
     ShortestPathPoint,
     Simulator,
     SimulatorActions,
 )
+from habitat.tasks.utils import (
+    cartesian_to_polar,
+    quaternion_rotate_vector,
+    compute_egocentric_delta,
+    compute_heading_from_quaternion,
+    compute_quaternion_from_heading,
+    compute_updated_pose,
+    truncated_normal_noise,
+)
+
+from habitat.utils.visualizations import maps
 
 RGBSENSOR_DIMENSION = 3
 
@@ -130,6 +154,106 @@ class HabitatSimSemanticSensor(SemanticSensor):
         return obs
 
 
+@registry.register_sensor
+class HabitatSimFineOccSensor(FineOccSensor):
+    sim_sensor_type: habitat_sim.SensorType
+
+    def __init__(self, config):
+        self.sim_sensor_type = habitat_sim.SensorType.COLOR
+        super().__init__(config=config)
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0,
+            high=255,
+            shape=(self.config.HEIGHT, self.config.WIDTH, RGBSENSOR_DIMENSION),
+            dtype=np.uint8,
+        )
+
+    def get_observation(self, sim_obs):
+        obs = sim_obs.get(self.uuid, None)
+        check_sim_obs(obs, self)
+
+        # remove alpha channel
+        obs = obs[:, :, :RGBSENSOR_DIMENSION]
+        return obs
+
+
+@registry.register_sensor
+class HabitatSimCoarseOccSensor(CoarseOccSensor):
+    sim_sensor_type: habitat_sim.SensorType
+
+    def __init__(self, config):
+        self.sim_sensor_type = habitat_sim.SensorType.COLOR
+        super().__init__(config=config)
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0,
+            high=255,
+            shape=(self.config.HEIGHT, self.config.WIDTH, RGBSENSOR_DIMENSION),
+            dtype=np.uint8,
+        )
+
+    def get_observation(self, sim_obs):
+        obs = sim_obs.get(self.uuid, None)
+        check_sim_obs(obs, self)
+
+        # remove alpha channel
+        obs = obs[:, :, :RGBSENSOR_DIMENSION]
+        return obs
+
+
+@registry.register_sensor
+class HabitatSimHighResCoarseOccSensor(HighResCoarseOccSensor):
+    sim_sensor_type: habitat_sim.SensorType
+
+    def __init__(self, config):
+        self.sim_sensor_type = habitat_sim.SensorType.COLOR
+        super().__init__(config=config)
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0,
+            high=255,
+            shape=(self.config.HEIGHT, self.config.WIDTH, RGBSENSOR_DIMENSION),
+            dtype=np.uint8,
+        )
+
+    def get_observation(self, sim_obs):
+        obs = sim_obs.get(self.uuid, None)
+        check_sim_obs(obs, self)
+
+        # remove alpha channel
+        obs = obs[:, :, :RGBSENSOR_DIMENSION]
+        return obs
+
+
+@registry.register_sensor
+class HabitatSimProjOccSensor(ProjOccSensor):
+    sim_sensor_type: habitat_sim.SensorType
+
+    def __init__(self, config):
+        self.sim_sensor_type = habitat_sim.SensorType.COLOR
+        super().__init__(config=config)
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0,
+            high=255,
+            shape=(self.config.HEIGHT, self.config.WIDTH, RGBSENSOR_DIMENSION),
+            dtype=np.uint8,
+        )
+
+    def get_observation(self, sim_obs):
+        obs = sim_obs.get(self.uuid, None)
+        check_sim_obs(obs, self)
+
+        # remove alpha channel
+        obs = obs[:, :, :RGBSENSOR_DIMENSION]
+        return obs
+
+
 @registry.register_simulator(name="Sim-v0")
 class HabitatSim(Simulator):
     r"""Simulator wrapper over habitat-sim
@@ -161,8 +285,12 @@ class HabitatSim(Simulator):
         self._action_space = spaces.Discrete(
             len(self.sim_config.agents[0].action_space)
         )
-
         self._is_episode_active = False
+        # Handling noisy odometer scenario
+        self._estimated_position = None
+        self._estimated_rotation = None
+        self._enable_odometer_noise = self.config.ENABLE_ODOMETRY_NOISE
+        self._odometer_noise_eta = self.config.ODOMETER_NOISE_SCALING
 
     def create_sim_config(
         self, _sensor_suite: SensorSuite
@@ -171,17 +299,13 @@ class HabitatSim(Simulator):
         sim_config.scene.id = self.config.SCENE
         sim_config.gpu_device_id = self.config.HABITAT_SIM_V0.GPU_DEVICE_ID
         agent_config = habitat_sim.AgentConfiguration()
-        overwrite_config(
-            config_from=self._get_agent_config(), config_to=agent_config
-        )
+        overwrite_config(config_from=self._get_agent_config(), config_to=agent_config)
 
         sensor_specifications = []
         for sensor in _sensor_suite.sensors.values():
             sim_sensor_cfg = habitat_sim.SensorSpec()
             sim_sensor_cfg.uuid = sensor.uuid
-            sim_sensor_cfg.resolution = list(
-                sensor.observation_space.shape[:2]
-            )
+            sim_sensor_cfg.resolution = list(sensor.observation_space.shape[:2])
             sim_sensor_cfg.parameters["hfov"] = str(sensor.config.HFOV)
             sim_sensor_cfg.position = sensor.config.POSITION
             # TODO(maksymets): Add configure method to Sensor API to avoid
@@ -214,9 +338,7 @@ class HabitatSim(Simulator):
             agent_cfg = self._get_agent_config(agent_id)
             if agent_cfg.IS_SET_START_STATE:
                 self.set_agent_state(
-                    agent_cfg.START_POSITION,
-                    agent_cfg.START_ROTATION,
-                    agent_id,
+                    agent_cfg.START_POSITION, agent_cfg.START_ROTATION, agent_id,
                 )
                 is_updated = True
 
@@ -227,6 +349,14 @@ class HabitatSim(Simulator):
         if self._update_agents_state():
             sim_obs = self._sim.get_sensor_observations()
 
+        # If noisy odometer is enabled, maintain an
+        # estimated position and rotation for the agent.
+        if self._enable_odometer_noise:
+            agent_state = self.get_agent_state()
+            # Initialize with the ground-truth position, rotation.
+            self._estimated_position = agent_state.position
+            self._estimated_rotation = agent_state.rotation
+
         self._prev_sim_obs = sim_obs
         self._is_episode_active = True
         return self._sensor_suite.get_observations(sim_obs)
@@ -237,14 +367,48 @@ class HabitatSim(Simulator):
             "STOP action called previously"
         )
 
+        agent_state = self.get_agent_state()
+        position_before_step = agent_state.position
+        rotation_before_step = agent_state.rotation
+
         if action == self.index_stop_action:
             self._is_episode_active = False
             sim_obs = self._sim.get_sensor_observations()
         else:
             sim_obs = self._sim.step(action)
-
         self._prev_sim_obs = sim_obs
 
+        agent_state = self.get_agent_state()
+        position_after_step = agent_state.position
+        rotation_after_step = agent_state.rotation
+
+        # Compute the estimated position, rotation.
+        if self._enable_odometer_noise and action != self.index_stop_action:
+            # Measure ground-truth delta in egocentric coordinates.
+            delta_rpt_gt = compute_egocentric_delta(
+                position_before_step,
+                rotation_before_step,
+                position_after_step,
+                rotation_after_step,
+            )
+            delta_y_gt = position_after_step[1] - position_before_step[1]
+            # Add noise to the ground-truth delta.
+            eta = self._odometer_noise_eta
+            D_rho, D_phi, D_theta = delta_rpt_gt
+            D_rho_n = D_rho + truncated_normal_noise(eta, 2 * eta) * D_rho
+            D_phi_n = D_phi
+            D_theta_n = D_theta + truncated_normal_noise(eta, 2 * eta) * D_theta
+            delta_rpt_n = np.array((D_rho_n, D_phi_n, D_theta_n))
+            delta_y_n = delta_y_gt
+            # Update noisy pose estimates
+            old_position = self._estimated_position
+            old_rotation = self._estimated_rotation
+            (new_position, new_rotation) = compute_updated_pose(
+                old_position, old_rotation, delta_rpt_n, delta_y_n
+            )
+            self._estimated_position = new_position
+            self._estimated_rotation = new_rotation
+
         observations = self._sensor_suite.get_observations(sim_obs)
         return observations
 
@@ -437,14 +601,28 @@ class HabitatSim(Simulator):
             observations = self._sensor_suite.get_observations(sim_obs)
             if not keep_agent_at_new_pose:
                 self.set_agent_state(
-                    current_state.position,
-                    current_state.rotation,
-                    reset_sensors=False,
+                    current_state.position, current_state.rotation, reset_sensors=False,
                 )
             return observations
         else:
             return None
 
+    def get_specific_sensor_observations_at(
+        self, position: List[float], rotation: List[float], sensor_uuid: str,
+    ) -> Optional[Observations]:
+
+        current_state = self.get_agent_state()
+        success = self.set_agent_state(position, rotation, reset_sensors=False)
+
+        if success:
+            specific_sim_obs = self._sim.get_specific_sensor_observations(sensor_uuid)
+            self.set_agent_state(
+                current_state.position, current_state.rotation, reset_sensors=False,
+            )
+            return specific_sim_obs
+        else:
+            return None
+
     # TODO (maksymets): Remove check after simulator became stable
     def _check_agent_position(self, position, agent_id=0) -> bool:
         if not np.allclose(position, self.get_agent_state(agent_id).position):
@@ -474,3 +652,848 @@ class HabitatSim(Simulator):
             result in an action (step) being taken.
         """
         return self._prev_sim_obs.get("collided", False)
+
+    def get_environment_extents(self) -> Tuple[float, float, float, float]:
+        """Returns the minimum and maximum X, Z coordinates navigable on
+        the current floor.
+        """
+        num_samples = 20000
+        start_height = self.get_agent_state().position[1]
+        min_x, max_x = (math.inf, -math.inf)
+        min_z, max_z = (math.inf, -math.inf)
+        for _ in range(num_samples):
+            point = self.sample_navigable_point()
+            # Check if on same level as original
+            if np.abs(start_height - point[1]) > 0.5:
+                continue
+            min_x = min(point[0], min_x)
+            max_x = max(point[0], max_x)
+            min_z = min(point[2], min_z)
+            max_z = max(point[2], max_z)
+
+        return (min_x, min_z, max_x, max_z)
+
+
+@registry.register_simulator(name="Sim-v1")
+class HabitatSimOcc(HabitatSim):
+    r"""Simulator wrapper over HabitatSim that additionally builds an
+    occupancy map of the environment as the agent is moving.
+
+    Args:
+        config: configuration for initializing the simulator.
+
+    Acknowledgement: large parts of the occupancy generation code were
+    borrowed from https://github.com/taochenshh/exp4nav with some
+    modifications for faster processing.
+    """
+
+    def __init__(self, config: Config) -> None:
+        super().__init__(config)
+        self.initialize_map(config)
+
+    def initialize_map(self, config):
+        r"""Initializes the map configurations and useful variables for map
+        computation.
+        """
+        occ_cfg = config.OCCUPANCY_MAPS
+        # ======================= Store map configurations ====================
+        occ_info = {
+            "map_scale": occ_cfg.MAP_SCALE,
+            "map_size": occ_cfg.MAP_SIZE,
+            "max_depth": occ_cfg.MAX_DEPTH,
+            "small_map_range": occ_cfg.SMALL_MAP_RANGE,
+            "large_map_range": occ_cfg.LARGE_MAP_RANGE,
+            "small_map_size": config.FINE_OCC_SENSOR.WIDTH,
+            "large_map_size": config.COARSE_OCC_SENSOR.WIDTH,
+            "height_threshold": (occ_cfg.HEIGHT_LOWER, occ_cfg.HEIGHT_UPPER),
+            "get_proj_loc_map": occ_cfg.GET_PROJ_LOC_MAP,
+            "use_gt_occ_map": occ_cfg.USE_GT_OCC_MAP,
+            # NOTE: This assumes that there is only one agent
+            "agent_height": config.AGENT_0.HEIGHT,
+            "Lx_min": None,
+            "Lx_max": None,
+            "Lz_min": None,
+            "Lz_max": None,
+            # Coverage novelty reward
+            "coverage_novelty_pooling": config.OCCUPANCY_MAPS.COVERAGE_NOVELTY_POOLING,
+        }
+        # High-resolution map options.
+        occ_info["get_highres_loc_map"] = occ_cfg.GET_HIGHRES_LOC_MAP
+        if occ_info["get_highres_loc_map"]:
+            occ_info["highres_large_map_size"] = config.HIGHRES_COARSE_OCC_SENSOR.WIDTH
+        # Measure noise-free area covered or noisy area covered?
+        if self._enable_odometer_noise:
+            occ_info["measure_noise_free_area"] = occ_cfg.MEASURE_NOISE_FREE_AREA
+        else:
+            occ_info["measure_noise_free_area"] = False
+        # Camera intrinsics.
+        hfov = math.radians(self._sensor_suite.sensors["depth"].config.HFOV)
+        v1 = 1.0 / np.tan(hfov / 2.0)
+        v2 = 1.0 / np.tan(hfov / 2.0)  # Assumes both FoVs are same.
+        intrinsic_matrix = np.array(
+            [
+                [v1, 0.0, 0.0, 0.0],
+                [0.0, v2, 0.0, 0.0],
+                [0.0, 0.0, 1.0, 0.0],
+                [0.0, 0.0, 0.0, 1.0],
+            ]
+        )
+        occ_info["intrinsic_matrix"] = intrinsic_matrix
+        occ_info["inverse_intrinsic_matrix"] = np.linalg.inv(intrinsic_matrix)
+        self.occupancy_info = occ_info
+        # ======================= Object annotations ==========================
+        self.has_object_annotations = config.OBJECT_ANNOTATIONS.IS_AVAILABLE
+        self.object_annotations_dir = config.OBJECT_ANNOTATIONS.PATH
+        # ========== Memory to be allocated at the start of an episode ========
+        self.grids_mat = None
+        self.count_grids_mat = None
+        self.noise_free_grids_mat = None
+        self.gt_grids_mat = None
+        self.proj_grids_mat = None
+        # ========================== GT topdown map ===========================
+        self._gt_top_down_map = None
+        # Maintain a cache to avoid redundant computation and to store
+        # useful statistics.
+        self._cache = {}
+        W = config.DEPTH_SENSOR.WIDTH
+        # Cache meshgrid for depth projection.
+        # [1, -1] for y as array indexing is y-down while world is y-up.
+        xs, ys = np.meshgrid(np.linspace(-1, 1, W), np.linspace(1, -1, W))
+        self._cache["xs"] = xs
+        self._cache["ys"] = ys
+
+    def create_grid_memory(self):
+        r"""Pre-assigns memory for global grids which are used to aggregate
+        the per-frame occupancy maps.
+        """
+        grid_size = self.occupancy_info["map_scale"]
+        min_x, min_z, max_x, max_z = self.get_environment_extents()
+        # Compute map size conditioned on environment extents.
+        # Add a 5m buffer to account for noise in extent estimates.
+        Lx_min, Lx_max, Lz_min, Lz_max = min_x - 5, max_x + 5, min_z - 5, max_z + 5
+        is_same_environment = (
+            (Lx_min == self.occupancy_info["Lx_min"])
+            and (Lx_max == self.occupancy_info["Lx_max"])
+            and (Lz_min == self.occupancy_info["Lz_min"])
+            and (Lz_max == self.occupancy_info["Lz_max"])
+        )
+        # Only if the environment changes, create new arrays.
+        if not is_same_environment:
+            # Update extents data
+            self.occupancy_info["Lx_min"] = Lx_min
+            self.occupancy_info["Lx_max"] = Lx_max
+            self.occupancy_info["Lz_min"] = Lz_min
+            self.occupancy_info["Lz_max"] = Lz_max
+            grid_num = (
+                int((Lx_max - Lx_min) / grid_size),
+                int((Lz_max - Lz_min) / grid_size),
+            )
+            # Create new arrays
+            self.grids_mat = np.zeros(grid_num, np.uint8)
+            self.count_grids_mat = np.zeros(grid_num, dtype=np.float32)
+            if self.occupancy_info["measure_noise_free_area"]:
+                self.noise_free_grids_mat = np.zeros(grid_num, np.uint8)
+            if self.occupancy_info["use_gt_occ_map"]:
+                self.gt_grids_mat = np.zeros(grid_num, np.uint8)
+            """
+            The local projection has 3 channels.
+            One each for occupied, free and unknown.
+            """
+            if self.occupancy_info["get_proj_loc_map"]:
+                self.proj_grids_mat = np.zeros((*grid_num, 3), np.uint8)
+
+    def reset(self):
+        sim_obs = self._sim.reset()
+        if self._update_agents_state():
+            sim_obs = self._sim.get_sensor_observations()
+        agent_state = self.get_agent_state()
+
+        # If noisy odometer is enabled, maintain an
+        # estimated position and rotation for the agent.
+        if self._enable_odometer_noise:
+            # Initialize with the ground-truth position, rotation
+            self._estimated_position = agent_state.position
+            self._estimated_rotation = agent_state.rotation
+        # Create map memory and reset stats
+        self.create_grid_memory()
+        self.reset_occupancy_stats()
+        # Obtain ground-truth environment layout
+        self.gt_map_creation_height = agent_state.position[1]
+        if self.occupancy_info["use_gt_occ_map"]:
+            self._gt_top_down_map = self.get_original_map()
+        # Update map based on current observations
+        sim_obs = self._update_map_observations(sim_obs)
+        # Load object annotations if available
+        if self.has_object_annotations:
+            scene_id = self._current_scene.split("/")[-1]
+            annot_path = f"{self.object_annotations_dir}/{scene_id}.json.gz"
+            with gzip.open(annot_path, "rt") as fp:
+                self.object_annotations = json.load(fp)
+
+        self._prev_sim_obs = sim_obs
+        self._is_episode_active = True
+        return self._sensor_suite.get_observations(sim_obs)
+
+    def step(self, action):
+        assert self._is_episode_active, (
+            "episode is not active, environment not RESET or "
+            "STOP action called previously"
+        )
+
+        agent_state = self.get_agent_state()
+        position_before_step = agent_state.position
+        rotation_before_step = agent_state.rotation
+
+        if action == self.index_stop_action:
+            self._is_episode_active = False
+            sim_obs = self._sim.get_sensor_observations()
+        else:
+            sim_obs = self._sim.step(action)
+
+        agent_state = self.get_agent_state()
+        position_after_step = agent_state.position
+        rotation_after_step = agent_state.rotation
+
+        # Compute the estimated position, rotation.
+        if self._enable_odometer_noise and action != self.index_stop_action:
+            # Measure ground-truth delta in egocentric coordinates.
+            delta_rpt_gt = compute_egocentric_delta(
+                position_before_step,
+                rotation_before_step,
+                position_after_step,
+                rotation_after_step,
+            )
+            delta_y_gt = position_after_step[1] - position_before_step[1]
+            # Add noise to the ground-truth delta.
+            eta = self._odometer_noise_eta
+            D_rho, D_phi, D_theta = delta_rpt_gt
+            D_rho_n = D_rho + truncated_normal_noise(eta, 2 * eta) * D_rho
+            D_phi_n = D_phi
+            D_theta_n = D_theta + truncated_normal_noise(eta, 2 * eta) * D_theta
+            delta_rpt_n = np.array((D_rho_n, D_phi_n, D_theta_n))
+            delta_y_n = delta_y_gt
+            # Update noisy pose estimates
+            old_position = self._estimated_position
+            old_rotation = self._estimated_rotation
+            (new_position, new_rotation) = compute_updated_pose(
+                old_position, old_rotation, delta_rpt_n, delta_y_n
+            )
+            self._estimated_position = new_position
+            self._estimated_rotation = new_rotation
+        # Update map based on current observations
+        sim_obs = self._update_map_observations(sim_obs)
+
+        self._prev_sim_obs = sim_obs
+        return self._sensor_suite.get_observations(sim_obs)
+
+    def convert_to_pointcloud(
+        self,
+        rgb: np.array,
+        depth: np.array,
+        agent_position: np.array,
+        agent_rotation: np.quaternion,
+    ) -> Tuple[np.array, Optional[np.array]]:
+        """Converts depth input into a sequence of points corresponding to
+        the 3D projection of camera points by using both intrinsic and
+        extrinsic parameters.
+
+        Args:
+            rgb - uint8 RGB images
+            depth - normalized depth inputs with values lying in [0.0, 1.0]
+            agent_position - pre-computed agent position for efficiency
+            agent_rotation - pre-computed agent rotation for efficiency
+        Returns:
+            xyz_world - a sequence of (x, y, z) real-world coordinates,
+                        may contain noise depending on the settings.
+            xyz_world_nf - a sequence of (x, y, z) real-world coordinates,
+                            strictly noise-free (Optional).
+        """
+        # =============== Unnormalize depth input if applicable ===============
+        depth_sensor = self._sensor_suite.sensors["depth"]
+        min_depth_value = depth_sensor.config.MIN_DEPTH
+        max_depth_value = depth_sensor.config.MAX_DEPTH
+        if depth_sensor.config.NORMALIZE_DEPTH:
+            depth_float = depth.astype(np.float32) * max_depth_value + min_depth_value
+        else:
+            depth_float = depth.astype(np.float32)
+        depth_float = depth_float[..., 0]
+        # ========== Convert to camera coordinates using intrinsics ===========
+        W = depth.shape[1]
+        xs = np.copy(self._cache["xs"]).reshape(-1)
+        ys = np.copy(self._cache["ys"]).reshape(-1)
+        depth_float = depth_float.reshape(-1)
+        # Filter out invalid depths.
+        valid_depths = (depth_float != 0.0) & (
+            depth_float <= self.occupancy_info["max_depth"]
+        )
+        xs = xs[valid_depths]
+        ys = ys[valid_depths]
+        depth_float = depth_float[valid_depths]
+        # Project to 3D coordinates.
+        # Negate depth as the camera looks along -Z.
+        xys = np.vstack(
+            (
+                xs * depth_float,
+                ys * depth_float,
+                -depth_float,
+                np.ones(depth_float.shape),
+            )
+        )
+        inv_K = self.occupancy_info["inverse_intrinsic_matrix"]
+        xyz_cam = np.matmul(inv_K, xys)
+        ## Uncomment for visualizing point-clouds in camera coordinates.
+        # colors = rgb.reshape(-1, 3)
+        # colors = colors[valid_depths, :]
+        # cv2.imshow('RGB', rgb[:, :, ::-1])
+        # cv2.imshow('Depth', depth)
+        # cv2.waitKey(0)
+        # fig = plt.figure()
+        # ax = fig.gca(projection='3d')
+        # colors = colors.astype(np.float32)/255.0
+        # ax.scatter(xyz_cam[0, :], xyz_cam[1, :], xyz_cam[2, :], c = colors)
+        # ax.set_xlabel('X axis')
+        # ax.set_ylabel('Y axis')
+        # ax.set_zlabel('Z axis')
+        # ax.view_init(elev=0.0, azim=-90.0)
+        # plt.show()
+        # =========== Convert to world coordinates using extrinsics ===========
+        T_world = np.eye(4)
+        T_world[:3, :3] = quaternion.as_rotation_matrix(agent_rotation)
+        T_world[:3, 3] = agent_position
+        xyz_world = np.matmul(T_world, xyz_cam).T
+        # Convert to non-homogeneous coordinates
+        xyz_world = xyz_world[:, :3] / xyz_world[:, 3][:, np.newaxis]
+        # ============ Compute noise-free point-cloud if required =============
+        xyz_world_nf = None
+        if self.occupancy_info["measure_noise_free_area"]:
+            agent_state = self.get_agent_state()
+            agent_position = agent_state.position
+            agent_rotation = agent_state.rotation
+            T_world = np.eye(4)
+            T_world[:3, :3] = quaternion.as_rotation_matrix(agent_rotation)
+            T_world[:3, 3] = agent_position
+            xyz_world_nf = np.matmul(T_world, xyz_cam).T
+            # Convert to non-homogeneous coordinates
+            xyz_world_nf = xyz_world_nf[:, :3] / xyz_world_nf[:, 3][:, np.newaxis]
+        return xyz_world, xyz_world_nf
+
+    def reconfigure(self, config: Config) -> None:
+        super().reconfigure(config)
+        self.initialize_map(config)
+
+    def get_observations_at(
+        self,
+        position: List[float],
+        rotation: List[float],
+        keep_agent_at_new_pose: bool = False,
+    ) -> Optional[Observations]:
+
+        current_state = self.get_agent_state()
+
+        success = self.set_agent_state(position, rotation, reset_sensors=False)
+        if success:
+            sim_obs = self._sim.get_sensor_observations()
+            if keep_agent_at_new_pose:
+                sim_obs = self._update_map_observations(sim_obs)
+            else:
+                # Difference being that the global map will not be updated
+                # using the current observation.
+                (
+                    fine_occupancy,
+                    coarse_occupancy,
+                    highres_coarse_occupancy,
+                ) = self.get_local_maps()
+                sim_obs["coarse_occupancy"] = coarse_occupancy
+                sim_obs["fine_occupancy"] = fine_occupancy
+                sim_obs["highres_coarse_occupancy"] = highres_coarse_occupancy
+                if self.occupancy_info["get_proj_loc_map"]:
+                    proj_occupancy = self.get_proj_loc_map()
+                    sim_obs["proj_occupancy"] = proj_occupancy
+            self._prev_sim_obs = sim_obs
+            # Process observations using sensor_suite.
+            observations = self._sensor_suite.get_observations(sim_obs)
+            if not keep_agent_at_new_pose:
+                self.set_agent_state(
+                    current_state.position, current_state.rotation, reset_sensors=False,
+                )
+            return observations
+        else:
+            return None
+
+    def get_original_map(self) -> np.array:
+        r"""Returns the top-down environment layout in the current global
+        map scale.
+        """
+        x_min = self.occupancy_info["Lx_min"]
+        x_max = self.occupancy_info["Lx_max"]
+        z_min = self.occupancy_info["Lz_min"]
+        z_max = self.occupancy_info["Lz_max"]
+        top_down_map = maps.get_topdown_map_v2(
+            self, (x_min, x_max, z_min, z_max), self.occupancy_info["map_scale"], 20000,
+        )
+        return top_down_map
+
+    def reset_occupancy_stats(self):
+        r"""Resets occupancy maps, area estimates.
+        """
+        self.occupancy_info["seen_area"] = 0
+        self.occupancy_info["inc_area"] = 0
+        self.grids_mat.fill(0)
+        self.count_grids_mat.fill(0)
+        if self.occupancy_info["measure_noise_free_area"]:
+            self.noise_free_grids_mat.fill(0)
+        if self.occupancy_info["use_gt_occ_map"]:
+            self.gt_grids_mat.fill(0)
+        if self.occupancy_info["get_proj_loc_map"]:
+            self.proj_grids_mat.fill(0)
+
+    def get_seen_area(
+        self,
+        rgb: np.array,
+        depth: np.array,
+        out_mat: np.array,
+        count_out_mat: np.array,
+        gt_out_mat: Optional[np.array],
+        proj_out_mat: Optional[np.array],
+        noise_free_out_mat: Optional[np.array],
+    ) -> int:
+        r"""Given new RGBD observations, it updates the global occupancy map
+        and computes total area seen after the update.
+
+        Args:
+            rgb - uint8 RGB images.
+            depth - normalized depth inputs with values lying in [0.0, 1.0].
+            *out_mat - global map to aggregate current inputs in.
+        Returns:
+            Area seen in the environment after aggregating current inputs.
+            Area is measured in gridcells. Multiply by map_scale**2 to
+            get area in m^2.
+        """
+        agent_state = self.get_agent_state()
+        if self._enable_odometer_noise:
+            agent_position = self._estimated_position
+            agent_rotation = self._estimated_rotation
+        else:
+            agent_position = agent_state.position
+            agent_rotation = agent_state.rotation
+        # ====================== Compute the pointcloud =======================
+        XYZ_ego, XYZ_ego_nf = self.convert_to_pointcloud(
+            rgb, depth, agent_position, agent_rotation
+        )
+        # Normalizing the point cloud so that ground plane is Y=0
+        if self._enable_odometer_noise:
+            current_agent_y = self._estimated_position[1]
+        else:
+            current_agent_y = agent_state.position[1]
+        ground_plane_y = current_agent_y - self.occupancy_info["agent_height"]
+        XYZ_ego[:, 1] -= ground_plane_y
+        # Measure pointcloud without ground-truth pose instead of estimated.
+        if self.occupancy_info["measure_noise_free_area"]:
+            ground_plane_y = (
+                agent_state.position[1] - self.occupancy_info["agent_height"]
+            )
+            XYZ_ego_nf[:, 1] -= ground_plane_y
+        # ================== Compute local occupancy map ======================
+        grids_mat = np.zeros(self.grids_mat.shape, dtype=np.uint8)
+        Lx_min = self.occupancy_info["Lx_min"]
+        Lz_min = self.occupancy_info["Lz_min"]
+        grid_size = self.occupancy_info["map_scale"]
+        height_thresh = self.occupancy_info["height_threshold"]
+        points = XYZ_ego
+        # Compute grid coordinates of points in pointcloud.
+        grid_locs = (points[:, [0, 2]] - np.array([[Lx_min, Lz_min]])) / grid_size
+        grid_locs = np.floor(grid_locs).astype(int)
+        # Classify points in occupancy map as free/occupied/unknown
+        # using height-based thresholds on the point-cloud.
+        high_filter_idx = points[:, 1] < height_thresh[1]
+        low_filter_idx = points[:, 1] > height_thresh[0]
+        obstacle_idx = np.logical_and(low_filter_idx, high_filter_idx)
+        # Assign known space as all free initially.
+        self.safe_assign(
+            grids_mat, grid_locs[high_filter_idx, 0], grid_locs[high_filter_idx, 1], 2,
+        )
+        kernel = np.ones((3, 3), np.uint8)
+        grids_mat = cv2.morphologyEx(grids_mat, cv2.MORPH_CLOSE, kernel)
+        # Assign occupied space based on presence of obstacles.
+        obs_mat = np.zeros(self.grids_mat.shape, dtype=np.uint8)
+        self.safe_assign(
+            obs_mat, grid_locs[obstacle_idx, 0], grid_locs[obstacle_idx, 1], 1
+        )
+        kernel = np.ones((3, 3), np.uint8)
+        obs_mat = cv2.morphologyEx(obs_mat, cv2.MORPH_CLOSE, kernel)
+        # ================== Update global occupancy map ======================
+        visible_mask = grids_mat == 2
+        occupied_mask = obs_mat == 1
+        np.putmask(out_mat, visible_mask, 2)
+        np.putmask(out_mat, occupied_mask, 1)
+        # Update counts to each grid location
+        seen_mask = (visible_mask | occupied_mask).astype(np.float32)
+        count_out_mat += seen_mask
+        inv_count_out_mat = np.ma.array(
+            1 / np.sqrt(np.clip(count_out_mat, 1.0, math.inf)), mask=1 - seen_mask
+        )
+        # Pick out counts for locations seen in this frame
+        if self.occupancy_info["coverage_novelty_pooling"] == "mean":
+            seen_count_reward = inv_count_out_mat.mean().item()
+        elif self.occupancy_info["coverage_novelty_pooling"] == "median":
+            seen_count_reward = np.ma.median(inv_count_out).item()
+        elif self.occupancy_info["coverage_novelty_pooling"] == "max":
+            seen_count_reward = inv_count_out_mat.max().item()
+        self.occupancy_info["seen_count_reward"] = seen_count_reward
+        # If ground-truth navigability is required (and not height-based),
+        # obtain the navigability values for valid locations in out_mat from
+        # use self._gt_top_down_map.
+        if self.occupancy_info["use_gt_occ_map"]:
+            gt_visible_mask = visible_mask | occupied_mask
+            # Dilate the visible mask
+            dkernel = np.ones((9, 9), np.uint8)
+            gt_visible_mask = cv2.dilate(
+                gt_visible_mask.astype(np.uint8), dkernel, iterations=2
+            )
+            gt_visible_mask = gt_visible_mask != 0
+            gt_occupied_mask = gt_visible_mask & (self._gt_top_down_map == 0)
+            np.putmask(gt_out_mat, gt_visible_mask, 2)
+            np.putmask(gt_out_mat, gt_occupied_mask, 1)
+        # If noise-free measurement for area-seen is required, then compute a
+        # global map that uses the ground-truth pose values.
+        if self.occupancy_info["measure_noise_free_area"]:
+            # --------------- Compute local occupancy map ---------------------
+            nf_grids_mat = np.zeros(self.grids_mat.shape, dtype=np.uint8)
+            points_nf = XYZ_ego_nf
+            # Compute grid coordinates of points in pointcloud.
+            grid_locs_nf = (
+                points_nf[:, [0, 2]] - np.array([[Lx_min, Lz_min]])
+            ) / grid_size
+            grid_locs_nf = np.floor(grid_locs_nf).astype(int)
+            # Classify points in occupancy map as free/occupied/unknown
+            # using height-based thresholds on the point-cloud.
+            high_filter_idx_nf = points_nf[:, 1] < height_thresh[1]
+            low_filter_idx_nf = points_nf[:, 1] > height_thresh[0]
+            obstacle_idx_nf = np.logical_and(low_filter_idx_nf, high_filter_idx_nf)
+            # Assign known space as all free initially.
+            self.safe_assign(
+                nf_grids_mat,
+                grid_locs_nf[high_filter_idx_nf, 0],
+                grid_locs_nf[high_filter_idx_nf, 1],
+                2,
+            )
+            kernel = np.ones((3, 3), np.uint8)
+            nf_grids_mat = cv2.morphologyEx(nf_grids_mat, cv2.MORPH_CLOSE, kernel)
+            # Assign occupied space based on presence of obstacles.
+            nf_obs_mat = np.zeros(self.grids_mat.shape, dtype=np.uint8)
+            self.safe_assign(
+                nf_obs_mat,
+                grid_locs_nf[obstacle_idx_nf, 0],
+                grid_locs_nf[obstacle_idx_nf, 1],
+                1,
+            )
+            kernel = np.ones((3, 3), np.uint8)
+            nf_obs_mat = cv2.morphologyEx(nf_obs_mat, cv2.MORPH_CLOSE, kernel)
+            np.putmask(nf_grids_mat, nf_obs_mat == 1, 1)
+            # ---------------- Update global occupancy map --------------------
+            visible_mask_nf = nf_grids_mat == 2
+            occupied_mask_nf = nf_grids_mat == 1
+            np.putmask(noise_free_out_mat, visible_mask_nf, 2)
+            np.putmask(noise_free_out_mat, occupied_mask_nf, 1)
+        # ================== Measure area seen (m^2) in the map =====================
+        if self.occupancy_info["measure_noise_free_area"]:
+            seen_area = (
+                float(np.count_nonzero(noise_free_out_mat > 0)) * (grid_size) ** 2
+            )
+        else:
+            seen_area = float(np.count_nonzero(out_mat > 0)) * (grid_size) ** 2
+        # ================= Compute local depth projection ====================
+        if self.occupancy_info["get_proj_loc_map"]:
+            proj_out_mat.fill(0)
+            # Set everything to unknown initially.
+            proj_out_mat[..., 2] = 1
+            # Set obstacles.
+            np.putmask(proj_out_mat[..., 0], grids_mat == 1, 1)
+            np.putmask(proj_out_mat[..., 2], grids_mat == 1, 0)
+            # Set free space.
+            free_space_mask = (grids_mat != 1) & (grids_mat == 2)
+            np.putmask(proj_out_mat[..., 1], free_space_mask, 1)
+            np.putmask(proj_out_mat[..., 2], free_space_mask, 0)
+        return seen_area
+
+    def safe_assign(self, im_map, x_idx, y_idx, value):
+        try:
+            im_map[x_idx, y_idx] = value
+        except IndexError:
+            valid_idx1 = np.logical_and(x_idx >= 0, x_idx < im_map.shape[0])
+            valid_idx2 = np.logical_and(y_idx >= 0, y_idx < im_map.shape[1])
+            valid_idx = np.logical_and(valid_idx1, valid_idx2)
+            im_map[x_idx[valid_idx], y_idx[valid_idx]] = value
+
+    def get_camera_grid_pos(self) -> Tuple[np.array, np.array]:
+        """Returns the agent's current position in both the real world
+        (X, Z, theta from -Z to X) and the grid world (Xg, Zg) coordinates.
+        """
+        if self._enable_odometer_noise:
+            position = self._estimated_position
+            rotation = self._estimated_rotation
+        else:
+            agent_state = self.get_agent_state()
+            position = agent_state.position
+            rotation = agent_state.rotation
+        X, Z = position[0], position[2]
+        grid_size = self.occupancy_info["map_scale"]
+        Lx_min = self.occupancy_info["Lx_min"]
+        Lx_max = self.occupancy_info["Lx_max"]
+        Lz_min = self.occupancy_info["Lz_min"]
+        Lz_max = self.occupancy_info["Lz_max"]
+        # Clamp positions within range.
+        X = min(max(X, Lx_min), Lx_max)
+        Z = min(max(Z, Lz_min), Lz_max)
+        # Compute grid world positions.
+        Xg = (X - Lx_min) / grid_size
+        Zg = (Z - Lz_min) / grid_size
+        # Real world rotation.
+        direction_vector = np.array([0, 0, -1])
+        heading_vector = quaternion_rotate_vector(rotation.inverse(), direction_vector)
+        phi = cartesian_to_polar(-heading_vector[2], heading_vector[0])[1]
+        phi = -phi  # (rotation from -Z to X)
+        return np.array((X, Z, phi)), np.array((Xg, Zg))
+
+    def get_local_maps(self) -> Tuple[np.array, np.array, np.array]:
+        r"""Generates egocentric crops of the global occupancy map.
+        Returns:
+            The occupancy images display free, occupied and unknown space.
+            The color conventions are:
+                free-space - (0, 255, 0)
+                occupied-space - (0, 0, 255)
+                unknown-space - (255, 255, 255)
+            The outputs are:
+                fine_ego_map_color - (H, W, 3) occupancy image
+                coarse_ego_map_color - (H, W, 3) occupancy image
+                highres_coarse_ego_map_color - (H, W, 3) occupancy image
+        """
+        # ================ The global occupancy map ===========================
+        if self.occupancy_info["use_gt_occ_map"]:
+            top_down_map = self.gt_grids_mat.copy()  # (map_size, map_size)
+        else:
+            top_down_map = self.grids_mat.copy()  # (map_size, map_size)
+        # =========== Obtain local crop around the agent ======================
+        # Agent's world and map positions.
+        xzt_world, xz_map = self.get_camera_grid_pos()
+        xz_map = (int(xz_map[0]), int(xz_map[1]))
+        # Crop out only the essential parts of the global map.
+        # This saves computation cost for the subsequent operations.
+        # *_range - #grid-cells of the map on either sides of the center
+        large_map_range = self.occupancy_info["large_map_range"]
+        small_map_range = self.occupancy_info["small_map_range"]
+        # *_size - output image size
+        large_map_size = self.occupancy_info["large_map_size"]
+        small_map_size = self.occupancy_info["small_map_size"]
+        min_range = int(1.5 * large_map_range)
+        x_start = max(0, xz_map[0] - min_range)
+        x_end = min(top_down_map.shape[0], xz_map[0] + min_range)
+        y_start = max(0, xz_map[1] - min_range)
+        y_end = min(top_down_map.shape[1], xz_map[1] + min_range)
+        ego_map = top_down_map[x_start:x_end, y_start:y_end]
+        # Pad the cropped map to account for out-of-bound indices
+        top_pad = max(min_range - xz_map[0], 0)
+        left_pad = max(min_range - xz_map[1], 0)
+        bottom_pad = max(min_range - top_down_map.shape[0] + xz_map[0] + 1, 0)
+        right_pad = max(min_range - top_down_map.shape[1] + xz_map[1] + 1, 0)
+        ego_map = np.pad(
+            ego_map,
+            ((top_pad, bottom_pad), (left_pad, right_pad)),
+            "constant",
+            constant_values=((0, 0), (0, 0)),
+        )
+        # The global map is currently addressed as follows:
+        # rows are -X to X top to bottom, cols are -Z to Z left to right
+        # To get -Z top and X right, we need transpose the map.
+        ego_map = ego_map.transpose(1, 0)
+        # Rotate the global map to obtain egocentric top-down view.
+        half_size = ego_map.shape[0] // 2
+        center = (half_size, half_size)
+        rot_angle = math.degrees(xzt_world[2])
+        M = cv2.getRotationMatrix2D(center, rot_angle, 1.0)
+        ego_map = cv2.warpAffine(
+            ego_map,
+            M,
+            (ego_map.shape[1], ego_map.shape[0]),
+            flags=cv2.INTER_NEAREST,
+            borderMode=cv2.BORDER_CONSTANT,
+            borderValue=(255,),
+        )
+        # =========== Obtain final maps at different resolutions ==============
+        # Obtain the fine occupancy map.
+        start = int(half_size - small_map_range)
+        end = int(half_size + small_map_range)
+        fine_ego_map = ego_map[start:end, start:end]
+        fine_ego_map = cv2.resize(
+            fine_ego_map,
+            (small_map_size, small_map_size),
+            interpolation=cv2.INTER_NEAREST,
+        )
+        fine_ego_map = np.clip(fine_ego_map, 0, 2)
+        # Obtain the coarse occupancy map.
+        start = int(half_size - large_map_range)
+        end = int(half_size + large_map_range)
+        coarse_ego_map_orig = ego_map[start:end, start:end]
+        coarse_ego_map = cv2.resize(
+            coarse_ego_map_orig,
+            (large_map_size, large_map_size),
+            interpolation=cv2.INTER_NEAREST,
+        )
+        coarse_ego_map = np.clip(coarse_ego_map, 0, 2)
+        # Obtain a high-resolution coarse occupancy map.
+        # This is primarily useful as an input to an A* path-planner.
+        if self.occupancy_info["get_highres_loc_map"]:
+            map_size = self.occupancy_info["highres_large_map_size"]
+            highres_coarse_ego_map = cv2.resize(
+                coarse_ego_map_orig,
+                (map_size, map_size),
+                interpolation=cv2.INTER_NEAREST,
+            )
+            highres_coarse_ego_map = np.clip(highres_coarse_ego_map, 0, 2)
+        # Convert to RGB maps.
+        # Fine occupancy map.
+        map_shape = (*fine_ego_map.shape, 3)
+        fine_ego_map_color = np.zeros(map_shape, dtype=np.uint8)
+        fine_ego_map_color[fine_ego_map == 0] = np.array([255, 255, 255])
+        fine_ego_map_color[fine_ego_map == 1] = np.array([0, 0, 255])
+        fine_ego_map_color[fine_ego_map == 2] = np.array([0, 255, 0])
+        # Coarse occupancy map.
+        map_shape = (*coarse_ego_map.shape, 3)
+        coarse_ego_map_color = np.zeros(map_shape, dtype=np.uint8)
+        coarse_ego_map_color[coarse_ego_map == 0] = np.array([255, 255, 255])
+        coarse_ego_map_color[coarse_ego_map == 1] = np.array([0, 0, 255])
+        coarse_ego_map_color[coarse_ego_map == 2] = np.array([0, 255, 0])
+        # High-resolution coarse occupancy map.
+        if self.occupancy_info["get_highres_loc_map"]:
+            map_shape = (*highres_coarse_ego_map.shape, 3)
+            highres_coarse_ego_map_color = np.zeros(map_shape, dtype=np.uint8)
+            highres_coarse_ego_map_color[highres_coarse_ego_map == 0] = np.array(
+                [255, 255, 255]
+            )
+            highres_coarse_ego_map_color[highres_coarse_ego_map == 1] = np.array(
+                [0, 0, 255]
+            )
+            highres_coarse_ego_map_color[highres_coarse_ego_map == 2] = np.array(
+                [0, 255, 0]
+            )
+        else:
+            highres_coarse_ego_map_color = None
+
+        return fine_ego_map_color, coarse_ego_map_color, highres_coarse_ego_map_color
+
+    def get_proj_loc_map(self):
+        """Generates a fine egocentric projection of depth map.
+        Returns:
+            The occupancy map is binary and indicates free, occupied and
+            unknown spaces. Channel 0 - occupied space, channel 1 - free space,
+            channel 2 - unknown space.
+            The outputs are:
+                fine_ego_map - (H, W, 3) occupancy map
+        """
+        # ================ The global occupancy map ===========================
+        top_down_map = self.proj_grids_mat.copy()  # (map_size, map_size)
+        # =========== Obtain local crop around the agent ======================
+        # Agent's world and map positions.
+        xzt_world, xz_map = self.get_camera_grid_pos()
+        xz_map = (int(xz_map[0]), int(xz_map[1]))
+        # Crop out only the essential parts of the global map.
+        # This saves computation cost for the subsequent opeartions.
+        # *_range - #grid-cells of the map on either sides of the center
+        small_map_range = self.occupancy_info["small_map_range"]
+        # *_size - output image size
+        small_map_size = self.occupancy_info["small_map_size"]
+        min_range = int(1.5 * small_map_range)
+        x_start = max(0, xz_map[0] - min_range)
+        x_end = min(top_down_map.shape[0], xz_map[0] + min_range)
+        y_start = max(0, xz_map[1] - min_range)
+        y_end = min(top_down_map.shape[1], xz_map[1] + min_range)
+        ego_map = top_down_map[x_start:x_end, y_start:y_end]
+        # Pad the cropped map to account for out-of-bound indices
+        top_pad = max(min_range - xz_map[0], 0)
+        left_pad = max(min_range - xz_map[1], 0)
+        bottom_pad = max(min_range - top_down_map.shape[0] + xz_map[0] + 1, 0)
+        right_pad = max(min_range - top_down_map.shape[1] + xz_map[1] + 1, 0)
+        ego_map = np.pad(
+            ego_map,
+            ((top_pad, bottom_pad), (left_pad, right_pad), (0, 0)),
+            "constant",
+            constant_values=0,
+        )
+        # The global map is currently addressed as follows:
+        # rows are -X to X top to bottom, cols are -Z to Z left to right
+        # To get -Z top and X right, we need transpose the map.
+        ego_map = ego_map.transpose(1, 0, 2)
+        # Rotate the global map to obtain egocentric top-down view.
+        half_size = ego_map.shape[0] // 2
+        center = (half_size, half_size)
+        rot_angle = math.degrees(xzt_world[2])
+        M = cv2.getRotationMatrix2D(center, rot_angle, 1.0)
+        ego_map = cv2.warpAffine(
+            ego_map,
+            M,
+            (ego_map.shape[1], ego_map.shape[0]),
+            flags=cv2.INTER_NEAREST,
+            borderMode=cv2.BORDER_CONSTANT,
+            borderValue=(0,),
+        )
+        # =========== Obtain final maps at different resolutions ==============
+        # Obtain the fine occupancy map
+        start = int(half_size - small_map_range)
+        end = int(half_size + small_map_range)
+        fine_ego_map = ego_map[start:end, start:end]
+        fine_ego_map = cv2.resize(
+            fine_ego_map,
+            (small_map_size, small_map_size),
+            interpolation=cv2.INTER_NEAREST,
+        )
+        # Note: There is no conversion to RGB here.
+        fine_ego_map = np.clip(fine_ego_map, 0, 1)  # (H, W, 1)
+
+        return fine_ego_map
+
+    def _update_map_observations(self, sim_obs):
+        r"""Given the default simulator observations, update it by adding the
+        occupancy maps.
+
+        Args:
+            sim_obs - a dictionary containing observations from self._sim.
+        Returns:
+            sim_obs with occupancy maps added as keys to it.
+        """
+        sensors = self._sensor_suite.sensors
+        proc_rgb = sensors["rgb"].get_observation(sim_obs)
+        proc_depth = sensors["depth"].get_observation(sim_obs)
+        # If the agent went to a new floor, update the GT map
+        if self.occupancy_info["use_gt_occ_map"]:
+            agent_height = self.get_agent_state().position[1]
+            if abs(agent_height - self.gt_map_creation_height) >= 0.5:
+                self._gt_top_down_map = self.get_original_map()
+                self.gt_map_creation_height = agent_height
+        # Update the map with new observations
+        seen_area = self.get_seen_area(
+            proc_rgb,
+            proc_depth,
+            self.grids_mat,
+            self.count_grids_mat,
+            self.gt_grids_mat,
+            self.proj_grids_mat,
+            self.noise_free_grids_mat,
+        )
+        inc_area = seen_area - self.occupancy_info["seen_area"]
+        # Crop out new egocentric maps
+        (
+            fine_occupancy,
+            coarse_occupancy,
+            highres_coarse_occupancy,
+        ) = self.get_local_maps()
+        # Update stats, observations
+        self.occupancy_info["seen_area"] = seen_area
+        self.occupancy_info["inc_area"] = inc_area
+        sim_obs["coarse_occupancy"] = coarse_occupancy
+        sim_obs["fine_occupancy"] = fine_occupancy
+        if self.occupancy_info["get_highres_loc_map"]:
+            sim_obs["highres_coarse_occupancy"] = highres_coarse_occupancy
+        if self.occupancy_info["get_proj_loc_map"]:
+            proj_occupancy = self.get_proj_loc_map()
+            sim_obs["proj_occupancy"] = proj_occupancy
+        return sim_obs
diff --git habitat/sims/registration.py habitat/sims/registration.py
index d756906..6ea5e07 100644
--- habitat/sims/registration.py
+++ habitat/sims/registration.py
@@ -13,9 +13,7 @@ from habitat.sims.habitat_simulator import _try_register_habitat_sim
 def make_sim(id_sim, **kwargs):
     logger.info("initializing sim {}".format(id_sim))
     _sim = registry.get_simulator(id_sim)
-    assert _sim is not None, "Could not find simulator with name {}".format(
-        id_sim
-    )
+    assert _sim is not None, "Could not find simulator with name {}".format(id_sim)
     return _sim(**kwargs)
 
 
diff --git habitat/tasks/eqa/eqa_task.py habitat/tasks/eqa/eqa_task.py
index 6f76569..6b636b9 100644
--- habitat/tasks/eqa/eqa_task.py
+++ habitat/tasks/eqa/eqa_task.py
@@ -43,9 +43,7 @@ class EQAEpisode(NavigationEpisode):
         question: question related to goal object.
     """
 
-    question: QuestionData = attr.ib(
-        default=None, validator=not_none_validator
-    )
+    question: QuestionData = attr.ib(default=None, validator=not_none_validator)
 
 
 class QuestionSensor(Sensor):
@@ -56,10 +54,7 @@ class QuestionSensor(Sensor):
         self.observation_space = spaces.Discrete(0)
 
     def _get_observation(
-        self,
-        observations: Dict[str, Observations],
-        episode: EQAEpisode,
-        **kwargs
+        self, observations: Dict[str, Observations], episode: EQAEpisode, **kwargs
     ):
         return episode.question.question_text
 
@@ -75,10 +70,7 @@ class AnswerSensor(Sensor):
         self.observation_space = spaces.Discrete(0)
 
     def _get_observation(
-        self,
-        observations: Dict[str, Observations],
-        episode: EQAEpisode,
-        **kwargs
+        self, observations: Dict[str, Observations], episode: EQAEpisode, **kwargs
     ):
         return episode.question.answer_text
 
diff --git habitat/tasks/exp_nav/.gitignore habitat/tasks/exp_nav/.gitignore
new file mode 100644
index 0000000..bee8a64
--- /dev/null
+++ habitat/tasks/exp_nav/.gitignore
@@ -0,0 +1 @@
+__pycache__
diff --git habitat/tasks/exp_nav/__init__.py habitat/tasks/exp_nav/__init__.py
new file mode 100644
index 0000000..e8c3661
--- /dev/null
+++ habitat/tasks/exp_nav/__init__.py
@@ -0,0 +1,9 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from habitat.tasks.exp_nav.exp_nav_task import ExploreNavigationTask
+
+__all__ = ["ExploreNavigationTask"]
diff --git habitat/tasks/exp_nav/exp_nav_task.py habitat/tasks/exp_nav/exp_nav_task.py
new file mode 100644
index 0000000..d0a1cc6
--- /dev/null
+++ habitat/tasks/exp_nav/exp_nav_task.py
@@ -0,0 +1,708 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, List, Optional, Type
+
+import pdb
+import attr
+import cv2
+import math
+import numpy as np
+from gym import spaces
+
+from habitat.config import Config
+from habitat.core.dataset import Dataset, Episode
+from habitat.core.embodied_task import EmbodiedTask, Measure, Measurements
+from habitat.core.registry import registry
+from habitat.core.simulator import (
+    Sensor,
+    SensorSuite,
+    SensorTypes,
+    ShortestPathPoint,
+    Simulator,
+    SimulatorActions,
+)
+from habitat.core.utils import not_none_validator
+from habitat.tasks.utils import (
+    cartesian_to_polar,
+    quaternion_rotate_vector,
+    compute_heading_from_quaternion,
+)
+from habitat.utils.visualizations import fog_of_war, maps
+from habitat.utils.visualizations.utils import topdown_to_image
+from habitat.tasks.exp_nav.shortest_path_follower import ShortestPathFollower
+from habitat.tasks.nav.nav_task import NavigationGoal, NavigationEpisode, NavigationTask
+
+MAP_THICKNESS_SCALAR: int = 1250
+RGBSENSOR_DIMENSION: int = 3
+
+
+def merge_sim_episode_config(sim_config: Config, episode: Type[Episode]) -> Any:
+    sim_config.defrost()
+    sim_config.SCENE = episode.scene_id
+    sim_config.freeze()
+    if episode.start_position is not None and episode.start_rotation is not None:
+        agent_name = sim_config.AGENTS[sim_config.DEFAULT_AGENT_ID]
+        agent_cfg = getattr(sim_config, agent_name)
+        agent_cfg.defrost()
+        agent_cfg.START_POSITION = episode.start_position
+        agent_cfg.START_ROTATION = episode.start_rotation
+        agent_cfg.IS_SET_START_STATE = True
+        agent_cfg.freeze()
+    return sim_config
+
+
+@attr.s(auto_attribs=True, kw_only=True)
+class ExploreNavigationGoal(NavigationGoal):
+    r"""A navigation goal that can be specified by position and
+    rotation (optional).
+    """
+
+    rotation: List[float] = attr.ib(default=None, validator=not_none_validator)
+
+
+@attr.s(auto_attribs=True, kw_only=True)
+class ExploreNavigationEpisode(NavigationEpisode):
+    r"""Class for episode specification that includes initial position and
+    rotation of agent, scene name, goal and optional shortest paths. An
+    episode is a description of one task instance for the agent.
+
+    Args:
+        episode_id: id of episode in the dataset, usually episode number
+        scene_id: id of scene in scene dataset
+        start_position: numpy ndarray containing 3 entries for (x, y, z)
+        start_rotation: numpy ndarray with 4 entries for (x, y, z, w)
+            elements of unit quaternion (versor) representing agent 3D
+            orientation. ref: https://en.wikipedia.org/wiki/Versor
+        start_nav_position: numpy ndarray containing 3 entries for (x, y, z)
+        start_nav_rotation: numpy ndarray with 4 entries for (x, y, z, w)
+            elements of unit quaternion (versor) representing agent 3D
+            orientation.
+        goals: list of goals specifications
+        start_room: room id
+        shortest_paths: list containing shortest paths to goals
+    """
+
+    start_nav_position: List[float] = attr.ib(
+        default=None, validator=not_none_validator
+    )
+    start_nav_rotation: List[float] = attr.ib(
+        default=None, validator=not_none_validator
+    )
+    goals: List[ExploreNavigationGoal] = attr.ib(
+        default=None, validator=not_none_validator
+    )
+
+
+@registry.register_sensor
+class ImageGoalSensorExploreNavigation(Sensor):
+    r"""Sensor for ImageGoal observations which are used in the ExploreNavigation task.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the ImageGoal sensor.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self.T_exp = config.T_EXP
+        self.T_nav = config.T_NAV
+        super().__init__(config=config)
+        self.current_episode_id = None
+        self.current_target = None
+        self._num_steps = None
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "image_goal_exp_nav"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0,
+            high=255,
+            shape=(self.config.HEIGHT, self.config.WIDTH, RGBSENSOR_DIMENSION),
+            dtype=np.uint8,
+        )
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            self.current_episode_id = episode_id
+            self._num_steps = 0.0
+
+        if self._num_steps >= self.T_exp:
+            tgt_obs = self._sim.get_observations_at(
+                episode.goals[0].position, episode.goals[0].rotation
+            )
+            tgt_im = tgt_obs["rgb"]
+        else:
+            tgt_im = np.zeros(
+                (self.config.HEIGHT, self.config.WIDTH, RGBSENSOR_DIMENSION),
+                dtype=np.uint8,
+            )
+
+        self._num_steps += 1
+        return tgt_im
+
+
+@registry.register_sensor
+class GridGoalSensorExploreNavigation(Sensor):
+    r"""Sensor for PointNav coordinates in the map grid space.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the ImageGoal sensor.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self.T_exp = config.T_EXP
+        self.T_nav = config.T_NAV
+        super().__init__(config=config)
+        self.current_episode_id = None
+        self.current_target = None
+        self._num_steps = None
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "grid_goal_exp_nav"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=-100000000.0, high=100000000.0, shape=(2,), dtype=np.float32,
+        )
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            self.current_episode_id = episode_id
+            self._num_steps = 0.0
+
+        if self._num_steps >= self.T_exp:
+            agent_position = self._sim.get_agent_state().position
+            agent_rotation = self._sim.get_agent_state().rotation
+            tgt_position = np.array(episode.goals[0].position)
+            tgt_grid_pos = self._get_egocentric_grid_loc(
+                agent_position, agent_rotation, tgt_position
+            )
+        else:
+            tgt_grid_pos = np.zeros((2,))
+
+        self._num_steps += 1
+        return tgt_grid_pos
+
+    def _get_egocentric_grid_loc(
+        self,
+        agent_position: np.array,
+        agent_rotation: np.quaternion,
+        tgt_position: np.array,
+    ) -> np.array:
+        """Provides the target location in terms of (x, y) coordinates in the
+        current egocentric occupancy map. The convention for the egocentric
+        occupancy map is that the agent is at the center of the map, with
+        forward direction being upward and the agent's right being the map's
+        rightward direction.
+        Args:
+            agent_position - current agent position in simulator (X, Y, Z)
+            agent_rotation - current agent rotation as a quaternion
+            tgt_position - navigation goal position in simulator (X, Y, Z)
+        """
+        # Changing coordinate system from -Z as forward, X as rightward
+        # to X as forward, Y as rightward.
+        tgt_x = -tgt_position[2]
+        tgt_y = tgt_position[0]
+        curr_x = -agent_position[2]
+        curr_y = agent_position[0]
+        curr_t = compute_heading_from_quaternion(agent_rotation)
+        # Target in egocentric polar coordinates.
+        r_ct = math.sqrt((tgt_x - curr_x) ** 2 + (tgt_y - curr_y) ** 2)
+        p_ct = math.atan2(tgt_y - curr_y, tgt_x - curr_x) - curr_t
+        # Convert to map grid coordinates with X rightward, Y downward.
+        grid_size = self._sim.config.OCCUPANCY_MAPS.MAP_SCALE
+        W = self._sim.config.HIGHRES_COARSE_OCC_SENSOR.WIDTH
+        H = self._sim.config.HIGHRES_COARSE_OCC_SENSOR.HEIGHT
+        large_map_range = self._sim.config.OCCUPANCY_MAPS.LARGE_MAP_RANGE
+        Wby2 = W // 2
+        Hby2 = H // 2
+        disp_y = -r_ct * math.cos(p_ct) / grid_size
+        disp_x = r_ct * math.sin(p_ct) / grid_size
+        # Map coordinates to occupancy image coordinates.
+        # Accounts for conversion from global-map cropping to image size, and
+        # shifts origin to top-left corner.
+        disp_y = Hby2 + H * disp_y / (2 * large_map_range + 1)
+        disp_x = Wby2 + W * disp_x / (2 * large_map_range + 1)
+        # Clip the values to be within the occupancy image extents. This might
+        # be a problem in large environments where the large_map_range does not
+        # cover the target.
+        grid_y = np.clip(disp_y, 0, H - 1)
+        grid_x = np.clip(disp_x, 0, W - 1)
+
+        return np.array([grid_x, grid_y])
+
+
+@registry.register_sensor
+class SPActionSensorExploreNavigation(Sensor):
+    r"""Sensor that returns the shortest path action to the navigation target.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the sensor. 
+
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        super().__init__(config=config)
+
+        self.current_episode_id = None
+        self.target_position = None
+        goal_radius = config.GOAL_RADIUS
+        self.goal_radius = goal_radius
+        self.follower = ShortestPathFollower(sim, goal_radius, False)
+        self.follower.mode = "geodesic_path"
+        self.T_exp = config.T_EXP
+        self.T_nav = config.T_NAV
+        self._step_count = None
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "sp_action_sensor_exp_nav"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(low=0, high=10, shape=(1,), dtype=np.int32,)
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            # A new episode has started
+            self.current_episode_id = episode_id
+            self.target_position = episode.goals[0].position
+            self._step_count = 0
+
+        if self._step_count >= self.T_exp:
+            current_target = self.target_position
+            agent_position = self._sim.get_agent_state().position
+            oracle_action = self.follower.get_next_action(current_target)
+        else:
+            # Some constant action
+            oracle_action = SimulatorActions.MOVE_FORWARD
+
+        self._step_count += 1
+
+        return np.array([oracle_action], dtype=np.int32)
+
+
+@registry.register_measure
+class TopDownMapExpNav(Measure):
+    r"""Top Down Map measure. During the exploration phase, this serves as
+    an exploration top-down view without the target marked. During the
+    navigation phase, this serves as a regular navigation map with both
+    source and target marked.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._config = config
+        self._grid_delta = config.MAP_PADDING
+        self.T_exp = config.T_EXP
+        self.T_nav = config.T_NAV
+        self._step_count = None
+        self._map_resolution = (config.MAP_RESOLUTION, config.MAP_RESOLUTION)
+        self._num_samples = config.NUM_TOPDOWN_MAP_SAMPLE_POINTS
+        self._ind_x_min = None
+        self._ind_x_max = None
+        self._ind_y_min = None
+        self._ind_y_max = None
+        self._previous_xy_location = None
+        self._coordinate_min = maps.COORDINATE_MIN
+        self._coordinate_max = maps.COORDINATE_MAX
+        self._top_down_map = None
+        self._shortest_path_points = None
+        self._cell_scale = (
+            self._coordinate_max - self._coordinate_min
+        ) / self._map_resolution[0]
+        self.line_thickness = int(
+            np.round(self._map_resolution[0] * 2 / MAP_THICKNESS_SCALAR)
+        )
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "top_down_map_exp_nav"
+
+    def _check_valid_nav_point(self, point: List[float]):
+        self._sim.is_navigable(point)
+
+    def get_original_map(self):
+        top_down_map = maps.get_topdown_map(
+            self._sim,
+            self._map_resolution,
+            self._num_samples,
+            self._config.DRAW_BORDER,
+        )
+
+        range_x = np.where(np.any(top_down_map, axis=1))[0]
+        range_y = np.where(np.any(top_down_map, axis=0))[0]
+
+        self._ind_x_min = range_x[0]
+        self._ind_x_max = range_x[-1]
+        self._ind_y_min = range_y[0]
+        self._ind_y_max = range_y[-1]
+
+        if self._config.FOG_OF_WAR.DRAW:
+            self._fog_of_war_mask = np.zeros_like(top_down_map)
+
+        return top_down_map
+
+    def draw_source_and_target(self, episode):
+        if self._step_count < self.T_exp:
+            # mark source point
+            s_x, s_y = maps.to_grid(
+                episode.start_position[0],
+                episode.start_position[2],
+                self._coordinate_min,
+                self._coordinate_max,
+                self._map_resolution,
+            )
+        else:
+            # mark source point
+            s_x, s_y = maps.to_grid(
+                episode.start_nav_position[0],
+                episode.start_nav_position[2],
+                self._coordinate_min,
+                self._coordinate_max,
+                self._map_resolution,
+            )
+
+        point_padding = 2 * int(np.ceil(self._map_resolution[0] / MAP_THICKNESS_SCALAR))
+        self._top_down_map[
+            s_x - point_padding : s_x + point_padding + 1,
+            s_y - point_padding : s_y + point_padding + 1,
+        ] = maps.MAP_SOURCE_POINT_INDICATOR
+
+        if self._step_count >= self.T_exp:
+            # mark target point
+            t_x, t_y = maps.to_grid(
+                episode.goals[0].position[0],
+                episode.goals[0].position[2],
+                self._coordinate_min,
+                self._coordinate_max,
+                self._map_resolution,
+            )
+            self._top_down_map[
+                t_x - point_padding : t_x + point_padding + 1,
+                t_y - point_padding : t_y + point_padding + 1,
+            ] = maps.MAP_TARGET_POINT_INDICATOR
+
+    def reset_metric(self, episode):
+        self._step_count = 0
+        self._metric = None
+        self._top_down_map = self.get_original_map()
+        agent_position = self._sim.get_agent_state().position
+        a_x, a_y = maps.to_grid(
+            agent_position[0],
+            agent_position[2],
+            self._coordinate_min,
+            self._coordinate_max,
+            self._map_resolution,
+        )
+        self._previous_xy_location = (a_y, a_x)
+
+        self.update_fog_of_war_mask(np.array([a_x, a_y]))
+
+        # draw source and target points last to avoid overlap
+        if self._config.DRAW_SOURCE_AND_TARGET:
+            self.draw_source_and_target(episode)
+
+    def _clip_map(self, _map):
+        return _map[
+            self._ind_x_min - self._grid_delta : self._ind_x_max + self._grid_delta,
+            self._ind_y_min - self._grid_delta : self._ind_y_max + self._grid_delta,
+        ]
+
+    def update_metric(self, episode, action):
+        if self._step_count == self.T_exp:
+            self._top_down_map = self.get_original_map()
+            agent_position = self._sim.get_agent_state().position
+            a_x, a_y = maps.to_grid(
+                agent_position[0],
+                agent_position[2],
+                self._coordinate_min,
+                self._coordinate_max,
+                self._map_resolution,
+            )
+
+            self._previous_xy_location = (a_y, a_x)
+            if self._config.DRAW_SHORTEST_PATH:
+                # draw shortest path
+                self._shortest_path_points = self._sim.get_straight_shortest_path_points(
+                    agent_position, episode.goals[0].position
+                )
+                self._shortest_path_points = [
+                    maps.to_grid(
+                        p[0],
+                        p[2],
+                        self._coordinate_min,
+                        self._coordinate_max,
+                        self._map_resolution,
+                    )[::-1]
+                    for p in self._shortest_path_points
+                ]
+                maps.draw_path(
+                    self._top_down_map,
+                    self._shortest_path_points,
+                    maps.MAP_SHORTEST_PATH_COLOR,
+                    self.line_thickness,
+                )
+
+            self.update_fog_of_war_mask(np.array([a_x, a_y]))
+
+            # draw source and target points last to avoid overlap
+            if self._config.DRAW_SOURCE_AND_TARGET:
+                self.draw_source_and_target(episode)
+
+        self._step_count += 1
+
+        house_map, map_agent_x, map_agent_y = self.update_map(
+            self._sim.get_agent_state().position
+        )
+
+        # Rather than return the whole map which may have large empty regions,
+        # only return the occupied part (plus some padding).
+        clipped_house_map = self._clip_map(house_map)
+
+        clipped_fog_of_war_map = None
+        if self._config.FOG_OF_WAR.DRAW:
+            clipped_fog_of_war_map = self._clip_map(self._fog_of_war_mask)
+
+        self._metric = {
+            "map": clipped_house_map,
+            "fog_of_war_mask": clipped_fog_of_war_map,
+            "agent_map_coord": (
+                map_agent_x - (self._ind_x_min - self._grid_delta),
+                map_agent_y - (self._ind_y_min - self._grid_delta),
+            ),
+            "agent_angle": self.get_polar_angle(),
+        }
+
+        self._metric = topdown_to_image(self._metric)
+
+    def get_polar_angle(self):
+        agent_state = self._sim.get_agent_state()
+        # quaternion is in x, y, z, w format
+        ref_rotation = agent_state.rotation
+
+        heading_vector = quaternion_rotate_vector(
+            ref_rotation.inverse(), np.array([0, 0, -1])
+        )
+
+        phi = cartesian_to_polar(-heading_vector[2], heading_vector[0])[1]
+        x_y_flip = -np.pi / 2
+        return np.array(phi) + x_y_flip
+
+    def update_map(self, agent_position):
+        a_x, a_y = maps.to_grid(
+            agent_position[0],
+            agent_position[2],
+            self._coordinate_min,
+            self._coordinate_max,
+            self._map_resolution,
+        )
+        # Don't draw over the source point
+        if self._top_down_map[a_x, a_y] != maps.MAP_SOURCE_POINT_INDICATOR:
+            color = 10 + min(
+                self._step_count * 245 // self._config.MAX_EPISODE_STEPS, 245
+            )
+
+            thickness = int(
+                np.round(self._map_resolution[0] * 2 / MAP_THICKNESS_SCALAR)
+            )
+            cv2.line(
+                self._top_down_map,
+                self._previous_xy_location,
+                (a_y, a_x),
+                color,
+                thickness=thickness,
+            )
+
+        self.update_fog_of_war_mask(np.array([a_x, a_y]))
+
+        self._previous_xy_location = (a_y, a_x)
+        return self._top_down_map, a_x, a_y
+
+    def update_fog_of_war_mask(self, agent_position):
+        if self._config.FOG_OF_WAR.DRAW:
+            self._fog_of_war_mask = fog_of_war.reveal_fog_of_war(
+                self._top_down_map,
+                self._fog_of_war_mask,
+                agent_position,
+                self.get_polar_angle(),
+                fov=self._config.FOG_OF_WAR.FOV,
+                max_line_len=self._config.FOG_OF_WAR.VISIBILITY_DIST
+                * max(self._map_resolution)
+                / (self._coordinate_max - self._coordinate_min),
+            )
+
+
+@registry.register_measure
+class SPLExpNav(Measure):
+    r"""SPL (Success weighted by Path Length)
+
+    ref: On Evaluation of Embodied Agents - Anderson et. al
+    https://arxiv.org/pdf/1807.06757.pdf
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._previous_position = None
+        self._start_end_episode_distance = None
+        self._agent_episode_distance = None
+        self._sim = sim
+        self.T_exp = config.T_EXP
+        self.T_nav = config.T_NAV
+        self._step_count = None
+        self._config = config
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "spl_exp_nav"
+
+    def reset_metric(self, episode):
+        self._step_count = 0
+        # self._previous_position = self._sim.get_agent_state().position.tolist()
+        self._start_end_episode_distance = episode.info["geodesic_distance"]
+        self._agent_episode_distance = 0.0
+        self._metric = None
+
+    def _euclidean_distance(self, position_a, position_b):
+        return np.linalg.norm(np.array(position_b) - np.array(position_a), ord=2)
+
+    def update_metric(self, episode, action):
+
+        # The metric is computed only after the exploration phase ends.
+        if self._step_count > self.T_exp + 1:
+            ep_success = 0
+            current_position = self._sim.get_agent_state().position.tolist()
+            distance_to_target = self._sim.geodesic_distance(
+                current_position, episode.goals[0].position
+            )
+            if (
+                action == self._sim.index_stop_action
+                and distance_to_target < self._config.SUCCESS_DISTANCE
+            ):
+                ep_success = 1
+            self._agent_episode_distance += self._euclidean_distance(
+                current_position, self._previous_position
+            )
+            self._previous_position = current_position
+            self._metric = ep_success * (
+                self._start_end_episode_distance
+                / max(self._start_end_episode_distance, self._agent_episode_distance)
+            )
+        else:
+            self._metric = 0.0
+
+        self._step_count += 1
+
+        if self._step_count == self.T_exp + 1:
+            self._previous_position = self._sim.get_agent_state().position.tolist()
+
+
+@registry.register_measure
+class SuccessExpNav(Measure):
+    r"""Success Rate
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self.T_exp = config.T_EXP
+        self.T_nav = config.T_NAV
+        self._step_count = None
+        self._config = config
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "success_exp_nav"
+
+    def reset_metric(self, episode):
+        self._step_count = 0
+        self._metric = None
+
+    def update_metric(self, episode, action):
+        # The metric is computed only after the exploration phase ends.
+        if self._step_count > self.T_exp + 1:
+            ep_success = 0.0
+            current_position = self._sim.get_agent_state().position.tolist()
+            distance_to_target = self._sim.geodesic_distance(
+                current_position, episode.goals[0].position
+            )
+            if (
+                action == self._sim.index_stop_action
+                and distance_to_target < self._config.SUCCESS_DISTANCE
+            ):
+                ep_success = 1.0
+            self._metric = ep_success
+        else:
+            self._metric = 0.0
+
+        self._step_count += 1
+
+
+@registry.register_measure
+class NavigationErrorExpNav(Measure):
+    r"""Navigation Error - geodesic distance to target at the current time
+    step.
+
+    ref: On Evaluation of Embodied Agents - Anderson et. al
+    https://arxiv.org/pdf/1807.06757.pdf
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self.T_exp = config.T_EXP
+        self.T_nav = config.T_NAV
+        self._step_count = None
+        self._config = config
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "nav_error_exp_nav"
+
+    def reset_metric(self, episode):
+        self._step_count = 0
+        self._metric = None
+
+    def update_metric(self, episode, action):
+        # The metric is computed only after the exploration phase ends.
+        if self._step_count > self.T_exp + 1:
+            current_position = self._sim.get_agent_state().position.tolist()
+            distance_to_target = self._sim.geodesic_distance(
+                current_position, episode.goals[0].position
+            )
+            self._metric = distance_to_target
+        else:
+            self._metric = math.inf
+
+        self._step_count += 1
+
+
+@registry.register_task(name="ExpNav-v0")
+class ExploreNavigationTask(NavigationTask):
+    def __init__(
+        self, task_config: Config, sim: Simulator, dataset: Optional[Dataset] = None,
+    ) -> None:
+
+        super().__init__(task_config=task_config, sim=sim, dataset=dataset)
+
+    def overwrite_sim_config(self, sim_config: Any, episode: Type[Episode]) -> Any:
+        return merge_sim_episode_config(sim_config, episode)
diff --git habitat/tasks/exp_nav/shortest_path_follower.py habitat/tasks/exp_nav/shortest_path_follower.py
new file mode 100644
index 0000000..8af05f7
--- /dev/null
+++ habitat/tasks/exp_nav/shortest_path_follower.py
@@ -0,0 +1,215 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Union
+
+import math
+import numpy as np
+
+import habitat_sim
+from habitat.core.simulator import SimulatorActions
+from habitat.sims.habitat_simulator.habitat_simulator import HabitatSim
+from habitat.utils.geometry_utils import (
+    angle_between_quaternions,
+    quaternion_from_two_vectors,
+)
+from habitat.tasks.utils import cartesian_to_polar, quaternion_rotate_vector
+
+EPSILON = 1e-6
+
+
+def action_to_one_hot(action: int) -> np.array:
+    one_hot = np.zeros(len(SimulatorActions), dtype=np.float32)
+    one_hot[action] = 1
+    return one_hot
+
+
+class ShortestPathFollower:
+    r"""Utility class for extracting the action on the shortest path to the
+        goal.
+    Args:
+        sim: HabitatSim instance.
+        goal_radius: Distance between the agent and the goal for it to be
+            considered successful.
+        return_one_hot: If true, returns a one-hot encoding of the action
+            (useful for training ML agents). If false, returns the
+            SimulatorAction.
+    """
+
+    def __init__(
+        self, sim: HabitatSim, goal_radius: float, return_one_hot: bool = True
+    ):
+        assert (
+            getattr(sim, "geodesic_distance", None) is not None
+        ), "{} must have a method called geodesic_distance".format(type(sim).__name__)
+
+        self._sim = sim
+        self._max_delta = self._sim.config.FORWARD_STEP_SIZE - EPSILON
+        self._goal_radius = goal_radius
+        self._step_size = self._sim.config.FORWARD_STEP_SIZE
+
+        self._mode = (
+            "geodesic_path"
+            if getattr(sim, "get_straight_shortest_path_points", None) is not None
+            else "greedy"
+        )
+        self._return_one_hot = return_one_hot
+
+    def _get_return_value(self, action) -> Union[int, np.array]:
+        if self._return_one_hot:
+            return action_to_one_hot(action)
+        else:
+            return action
+
+    def get_next_action(self, goal_pos: np.array) -> Union[int, np.array]:
+        """Returns the next action along the shortest path.
+        """
+        if (
+            np.linalg.norm(goal_pos - self._sim.get_agent_state().position)
+            <= self._goal_radius
+        ):
+            return self._get_return_value(SimulatorActions.STOP)
+
+        max_grad_dir = self._est_max_grad_dir(goal_pos)
+        if max_grad_dir is None:
+            return self._get_return_value(SimulatorActions.MOVE_FORWARD)
+        return self._step_along_grad(max_grad_dir)
+
+    def _quaternion_to_heading(self, quaternion):
+        # quaternion - np.quaternion unit quaternion
+        # Real world rotation
+        direction_vector = np.array([0, 0, -1])
+        heading_vector = quaternion_rotate_vector(
+            quaternion.inverse(), direction_vector
+        )
+
+        phi = cartesian_to_polar(-heading_vector[2], heading_vector[0])[1]
+        return phi
+
+    def _step_along_grad(self, grad_dir: np.quaternion) -> Union[int, np.array]:
+        current_state = self._sim.get_agent_state()
+        alpha = angle_between_quaternions(grad_dir, current_state.rotation)
+        if alpha <= np.deg2rad(self._sim.config.TURN_ANGLE) + EPSILON:
+            return self._get_return_value(SimulatorActions.MOVE_FORWARD)
+        else:
+            # These angles represent the rightward rotation from forward direction
+            grad_angle = -self._quaternion_to_heading(grad_dir)
+            curr_angle = -self._quaternion_to_heading(current_state.rotation)
+
+            diff_angle = grad_angle - curr_angle
+            diff_angle = math.atan2(math.sin(diff_angle), math.cos(diff_angle))
+
+            if diff_angle > 0:
+                best_turn = SimulatorActions.TURN_RIGHT
+            else:
+                best_turn = SimulatorActions.TURN_LEFT
+
+            # sim_action = SimulatorActions.TURN_LEFT
+            # self._sim.step(sim_action)
+            # best_turn = (
+            #    SimulatorActions.TURN_LEFT
+            #    if (
+            #        angle_between_quaternions(
+            #            grad_dir, self._sim.get_agent_state().rotation
+            #        )
+            #        < alpha
+            #    )
+            #    else SimulatorActions.TURN_RIGHT
+            # )
+            # self._reset_agent_state(current_state)
+            return self._get_return_value(best_turn)
+
+    def _reset_agent_state(self, state: habitat_sim.AgentState) -> None:
+        self._sim.set_agent_state(state.position, state.rotation, reset_sensors=False)
+
+    def _geo_dist(self, goal_pos: np.array) -> float:
+        return self._sim.geodesic_distance(
+            self._sim.get_agent_state().position, goal_pos
+        )
+
+    def _est_max_grad_dir(self, goal_pos: np.array) -> np.array:
+
+        current_state = self._sim.get_agent_state()
+        current_pos = current_state.position
+
+        if self.mode == "geodesic_path":
+            points = self._sim.get_straight_shortest_path_points(
+                self._sim.get_agent_state().position, goal_pos
+            )
+            # Add a little offset as things get weird if
+            # points[1] - points[0] is anti-parallel with forward
+            if len(points) < 2:
+                return None
+            max_grad_dir = quaternion_from_two_vectors(
+                self._sim.forward_vector,
+                points[1]
+                - points[0]
+                + EPSILON * np.cross(self._sim.up_vector, self._sim.forward_vector),
+            )
+            max_grad_dir.x = 0
+            max_grad_dir = np.normalized(max_grad_dir)
+        else:
+            current_rotation = self._sim.get_agent_state().rotation
+            current_dist = self._geo_dist(goal_pos)
+
+            best_geodesic_delta = -2 * self._max_delta
+            best_rotation = current_rotation
+            for _ in range(0, 360, self._sim.config.TURN_ANGLE):
+                sim_action = SimulatorActions.MOVE_FORWARD
+                self._sim.step(sim_action)
+                new_delta = current_dist - self._geo_dist(goal_pos)
+
+                if new_delta > best_geodesic_delta:
+                    best_rotation = self._sim.get_agent_state().rotation
+                    best_geodesic_delta = new_delta
+
+                # If the best delta is within (1 - cos(TURN_ANGLE))% of the
+                # best delta (the step size), then we almost certainly have
+                # found the max grad dir and should just exit
+                if np.isclose(
+                    best_geodesic_delta,
+                    self._max_delta,
+                    rtol=1 - np.cos(np.deg2rad(self._sim.config.TURN_ANGLE)),
+                ):
+                    break
+
+                self._sim.set_agent_state(
+                    current_pos,
+                    self._sim.get_agent_state().rotation,
+                    reset_sensors=False,
+                )
+
+                sim_action = SimulatorActions.TURN_LEFT
+                self._sim.step(sim_action)
+
+            self._reset_agent_state(current_state)
+
+            max_grad_dir = best_rotation
+
+        return max_grad_dir
+
+    @property
+    def mode(self):
+        return self._mode
+
+    @mode.setter
+    def mode(self, new_mode: str):
+        r"""Sets the mode for how the greedy follower determines the best next
+            step.
+        Args:
+            new_mode: geodesic_path indicates using the simulator's shortest
+                path algorithm to find points on the map to navigate between.
+                greedy indicates trying to move forward at all possible
+                orientations and selecting the one which reduces the geodesic
+                distance the most.
+        """
+        assert new_mode in {"geodesic_path", "greedy"}
+        if new_mode == "geodesic_path":
+            assert (
+                getattr(self._sim, "get_straight_shortest_path_points", None)
+                is not None
+            )
+        self._mode = new_mode
diff --git habitat/tasks/nav/nav_task.py habitat/tasks/nav/nav_task.py
index 27b1451..9ccd8cc 100644
--- habitat/tasks/nav/nav_task.py
+++ habitat/tasks/nav/nav_task.py
@@ -6,8 +6,10 @@
 
 from typing import Any, List, Optional, Type
 
+import pdb
 import attr
 import cv2
+import math
 import numpy as np
 from gym import spaces
 
@@ -25,20 +27,16 @@ from habitat.core.simulator import (
 from habitat.core.utils import not_none_validator
 from habitat.tasks.utils import cartesian_to_polar, quaternion_rotate_vector
 from habitat.utils.visualizations import fog_of_war, maps
+from habitat.tasks.nav.shortest_path_follower import ShortestPathFollower
 
 MAP_THICKNESS_SCALAR: int = 1250
 
 
-def merge_sim_episode_config(
-    sim_config: Config, episode: Type[Episode]
-) -> Any:
+def merge_sim_episode_config(sim_config: Config, episode: Type[Episode]) -> Any:
     sim_config.defrost()
     sim_config.SCENE = episode.scene_id
     sim_config.freeze()
-    if (
-        episode.start_position is not None
-        and episode.start_rotation is not None
-    ):
+    if episode.start_position is not None and episode.start_rotation is not None:
         agent_name = sim_config.AGENTS[sim_config.DEFAULT_AGENT_ID]
         agent_cfg = getattr(sim_config, agent_name)
         agent_cfg.defrost()
@@ -98,9 +96,7 @@ class NavigationEpisode(Episode):
         shortest_paths: list containing shortest paths to goals
     """
 
-    goals: List[NavigationGoal] = attr.ib(
-        default=None, validator=not_none_validator
-    )
+    goals: List[NavigationGoal] = attr.ib(default=None, validator=not_none_validator)
     start_room: Optional[str] = None
     shortest_paths: Optional[List[ShortestPathPoint]] = None
 
@@ -155,8 +151,7 @@ class PointGoalSensor(Sensor):
         rotation_world_agent = agent_state.rotation
 
         direction_vector = (
-            np.array(episode.goals[0].position, dtype=np.float32)
-            - ref_position
+            np.array(episode.goals[0].position, dtype=np.float32) - ref_position
         )
         direction_vector_agent = quaternion_rotate_vector(
             rotation_world_agent.inverse(), direction_vector
@@ -224,8 +219,7 @@ class StaticPointGoalSensor(Sensor):
             rotation_world_agent = agent_state.rotation
 
             direction_vector = (
-                np.array(episode.goals[0].position, dtype=np.float32)
-                - ref_position
+                np.array(episode.goals[0].position, dtype=np.float32) - ref_position
             )
             direction_vector_agent = quaternion_rotate_vector(
                 rotation_world_agent.inverse(), direction_vector
@@ -235,9 +229,7 @@ class StaticPointGoalSensor(Sensor):
                 rho, phi = cartesian_to_polar(
                     -direction_vector_agent[2], direction_vector_agent[0]
                 )
-                direction_vector_agent = np.array(
-                    [rho, -phi], dtype=np.float32
-                )
+                direction_vector_agent = np.array([rho, -phi], dtype=np.float32)
 
             self._initial_vector = direction_vector_agent
         return self._initial_vector
@@ -291,9 +283,7 @@ class ProximitySensor(Sensor):
 
     def __init__(self, sim, config):
         self._sim = sim
-        self._max_detection_radius = getattr(
-            config, "MAX_DETECTION_RADIUS", 2.0
-        )
+        self._max_detection_radius = getattr(config, "MAX_DETECTION_RADIUS", 2.0)
         super().__init__(config=config)
 
     def _get_uuid(self, *args: Any, **kwargs: Any):
@@ -304,10 +294,7 @@ class ProximitySensor(Sensor):
 
     def _get_observation_space(self, *args: Any, **kwargs: Any):
         return spaces.Box(
-            low=0.0,
-            high=self._max_detection_radius,
-            shape=(1,),
-            dtype=np.float,
+            low=0.0, high=self._max_detection_radius, shape=(1,), dtype=np.float,
         )
 
     def get_observation(self, observations, episode):
@@ -318,6 +305,45 @@ class ProximitySensor(Sensor):
         )
 
 
+@registry.register_sensor
+class SPActionSensor(Sensor):
+    r"""Sensor that returns the shortest path action to the navigation target.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the sensor.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        super().__init__(config=config)
+
+        self.current_episode_id = None
+        self.target_position = None
+        goal_radius = config.GOAL_RADIUS
+        self.goal_radius = goal_radius
+        self.follower = ShortestPathFollower(sim, goal_radius, False)
+        self.follower.mode = "geodesic_path"
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "sp_action_sensor"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(low=0, high=10, shape=(1,), dtype=np.int32,)
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            # A new episode has started
+            self.current_episode_id = episode_id
+            self.target_position = episode.goals[0].position
+        oracle_action = self.follower.get_next_action(self.targetposition)
+        return np.array([oracle_action], dtype=np.int32)
+
+
 @registry.register_measure
 class SPL(Measure):
     r"""SPL (Success weighted by Path Length)
@@ -345,9 +371,7 @@ class SPL(Measure):
         self._metric = None
 
     def _euclidean_distance(self, position_a, position_b):
-        return np.linalg.norm(
-            np.array(position_b) - np.array(position_a), ord=2
-        )
+        return np.linalg.norm(np.array(position_b) - np.array(position_a), ord=2)
 
     def update_metric(self, episode, action):
         ep_success = 0
@@ -371,9 +395,7 @@ class SPL(Measure):
 
         self._metric = ep_success * (
             self._start_end_episode_distance
-            / max(
-                self._start_end_episode_distance, self._agent_episode_distance
-            )
+            / max(self._start_end_episode_distance, self._agent_episode_distance)
         )
 
 
@@ -465,9 +487,7 @@ class TopDownMap(Measure):
             self._coordinate_max,
             self._map_resolution,
         )
-        point_padding = 2 * int(
-            np.ceil(self._map_resolution[0] / MAP_THICKNESS_SCALAR)
-        )
+        point_padding = 2 * int(np.ceil(self._map_resolution[0] / MAP_THICKNESS_SCALAR))
         self._top_down_map[
             s_x - point_padding : s_x + point_padding + 1,
             s_y - point_padding : s_y + point_padding + 1,
@@ -529,12 +549,8 @@ class TopDownMap(Measure):
 
     def _clip_map(self, _map):
         return _map[
-            self._ind_x_min
-            - self._grid_delta : self._ind_x_max
-            + self._grid_delta,
-            self._ind_y_min
-            - self._grid_delta : self._ind_y_max
-            + self._grid_delta,
+            self._ind_x_min - self._grid_delta : self._ind_x_max + self._grid_delta,
+            self._ind_y_min - self._grid_delta : self._ind_y_max + self._grid_delta,
         ]
 
     def update_metric(self, episode, action):
@@ -621,19 +637,16 @@ class TopDownMap(Measure):
 @registry.register_task(name="Nav-v0")
 class NavigationTask(EmbodiedTask):
     def __init__(
-        self,
-        task_config: Config,
-        sim: Simulator,
-        dataset: Optional[Dataset] = None,
+        self, task_config: Config, sim: Simulator, dataset: Optional[Dataset] = None,
     ) -> None:
 
         task_measurements = []
         for measurement_name in task_config.MEASUREMENTS:
             measurement_cfg = getattr(task_config, measurement_name)
             measure_type = registry.get_measure(measurement_cfg.TYPE)
-            assert (
-                measure_type is not None
-            ), "invalid measurement type {}".format(measurement_cfg.TYPE)
+            assert measure_type is not None, "invalid measurement type {}".format(
+                measurement_cfg.TYPE
+            )
             task_measurements.append(measure_type(sim, measurement_cfg))
         self.measurements = Measurements(task_measurements)
 
@@ -649,7 +662,5 @@ class NavigationTask(EmbodiedTask):
         self.sensor_suite = SensorSuite(task_sensors)
         super().__init__(config=task_config, sim=sim, dataset=dataset)
 
-    def overwrite_sim_config(
-        self, sim_config: Any, episode: Type[Episode]
-    ) -> Any:
+    def overwrite_sim_config(self, sim_config: Any, episode: Type[Episode]) -> Any:
         return merge_sim_episode_config(sim_config, episode)
diff --git habitat/tasks/nav/shortest_path_follower.py habitat/tasks/nav/shortest_path_follower.py
index 2d7b2f1..4a9a0e9 100644
--- habitat/tasks/nav/shortest_path_follower.py
+++ habitat/tasks/nav/shortest_path_follower.py
@@ -8,6 +8,7 @@ from typing import Union
 
 import numpy as np
 
+import math
 import habitat_sim
 from habitat.core.simulator import SimulatorActions
 from habitat.sims.habitat_simulator.habitat_simulator import HabitatSim
@@ -15,6 +16,11 @@ from habitat.utils.geometry_utils import (
     angle_between_quaternions,
     quaternion_from_two_vectors,
 )
+from habitat.tasks.utils import (
+    cartesian_to_polar,
+    quaternion_rotate_vector,
+    compute_quaternion_from_heading,
+)
 
 EPSILON = 1e-6
 
@@ -42,9 +48,7 @@ class ShortestPathFollower:
     ):
         assert (
             getattr(sim, "geodesic_distance", None) is not None
-        ), "{} must have a method called geodesic_distance".format(
-            type(sim).__name__
-        )
+        ), "{} must have a method called geodesic_distance".format(type(sim).__name__)
 
         self._sim = sim
         self._max_delta = self._sim.config.FORWARD_STEP_SIZE - EPSILON
@@ -53,8 +57,7 @@ class ShortestPathFollower:
 
         self._mode = (
             "geodesic_path"
-            if getattr(sim, "get_straight_shortest_path_points", None)
-            is not None
+            if getattr(sim, "get_straight_shortest_path_points", None) is not None
             else "greedy"
         )
         self._return_one_hot = return_one_hot
@@ -79,33 +82,27 @@ class ShortestPathFollower:
             return self._get_return_value(SimulatorActions.MOVE_FORWARD)
         return self._step_along_grad(max_grad_dir)
 
-    def _step_along_grad(
-        self, grad_dir: np.quaternion
-    ) -> Union[int, np.array]:
+    def _step_along_grad(self, grad_dir: np.quaternion) -> Union[int, np.array]:
         current_state = self._sim.get_agent_state()
         alpha = angle_between_quaternions(grad_dir, current_state.rotation)
         if alpha <= np.deg2rad(self._sim.config.TURN_ANGLE) + EPSILON:
             return self._get_return_value(SimulatorActions.MOVE_FORWARD)
         else:
-            sim_action = SimulatorActions.TURN_LEFT
-            self._sim.step(sim_action)
-            best_turn = (
-                SimulatorActions.TURN_LEFT
-                if (
-                    angle_between_quaternions(
-                        grad_dir, self._sim.get_agent_state().rotation
-                    )
-                    < alpha
-                )
-                else SimulatorActions.TURN_RIGHT
-            )
-            self._reset_agent_state(current_state)
+            # These angles represent the rightward rotation from
+            # forward direction.
+            curr_dir = current_state.rotation
+            grad_angle = compute_quaternion_from_heading(grad_dir)
+            curr_angle = compute_quaternion_from_heading(curr_dir)
+            diff_angle = grad_angle - curr_angle
+            diff_angle = math.atan2(math.sin(diff_angle), math.cos(diff_angle))
+            if diff_angle > 0:
+                best_turn = SimulatorActions.TURN_RIGHT
+            else:
+                best_turn = SimulatorActions.TURN_LEFT
             return self._get_return_value(best_turn)
 
     def _reset_agent_state(self, state: habitat_sim.AgentState) -> None:
-        self._sim.set_agent_state(
-            state.position, state.rotation, reset_sensors=False
-        )
+        self._sim.set_agent_state(state.position, state.rotation, reset_sensors=False)
 
     def _geo_dist(self, goal_pos: np.array) -> float:
         return self._sim.geodesic_distance(
@@ -129,8 +126,7 @@ class ShortestPathFollower:
                 self._sim.forward_vector,
                 points[1]
                 - points[0]
-                + EPSILON
-                * np.cross(self._sim.up_vector, self._sim.forward_vector),
+                + EPSILON * np.cross(self._sim.up_vector, self._sim.forward_vector),
             )
             max_grad_dir.x = 0
             max_grad_dir = np.normalized(max_grad_dir)
diff --git habitat/tasks/pose_estimation/__init__.py habitat/tasks/pose_estimation/__init__.py
new file mode 100644
index 0000000..2b99fc3
--- /dev/null
+++ habitat/tasks/pose_estimation/__init__.py
@@ -0,0 +1,9 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from habitat.tasks.pose_estimation.pose_estimation_task import PoseEstimationTask
+
+__all__ = ["PoseEstimationTask"]
diff --git habitat/tasks/pose_estimation/pose_estimation_task.py habitat/tasks/pose_estimation/pose_estimation_task.py
new file mode 100644
index 0000000..2dace03
--- /dev/null
+++ habitat/tasks/pose_estimation/pose_estimation_task.py
@@ -0,0 +1,1175 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Any, List, Optional, Type
+
+import pdb
+import attr
+import cv2
+import copy
+import math
+import quaternion
+import numpy as np
+from gym import spaces
+
+from habitat.config import Config
+from habitat.core.dataset import Dataset, Episode
+from habitat.core.embodied_task import EmbodiedTask, Measure, Measurements
+from habitat.core.registry import registry
+from habitat.core.simulator import (
+    Sensor,
+    SensorSuite,
+    SensorTypes,
+    ShortestPathPoint,
+    Simulator,
+)
+from habitat.core.utils import not_none_validator
+from habitat.tasks.utils import (
+    cartesian_to_polar,
+    quaternion_rotate_vector,
+    compute_heading_from_quaternion,
+)
+from habitat.utils.geometry_utils import (
+    angle_between_quaternions,
+    quaternion_to_list,
+)
+from habitat.utils.visualizations import fog_of_war, maps
+from habitat.utils.visualizations.utils import topdown_to_image
+from habitat.tasks.nav.nav_task import TopDownMap
+from habitat_sim.utils import quat_from_coeffs
+from habitat.tasks.pose_estimation.shortest_path_follower import ShortestPathFollower
+
+MAP_THICKNESS_SCALAR: int = 1250
+RGBSENSOR_DIMENSION = 3
+
+
+def merge_sim_episode_config(sim_config: Config, episode: Type[Episode]) -> Any:
+    sim_config.defrost()
+    sim_config.SCENE = episode.scene_id
+    sim_config.freeze()
+    if episode.start_position is not None and episode.start_rotation is not None:
+        agent_name = sim_config.AGENTS[sim_config.DEFAULT_AGENT_ID]
+        agent_cfg = getattr(sim_config, agent_name)
+        agent_cfg.defrost()
+        agent_cfg.START_POSITION = episode.start_position
+        agent_cfg.START_ROTATION = episode.start_rotation
+        agent_cfg.IS_SET_START_STATE = True
+        agent_cfg.freeze()
+    return sim_config
+
+
+@attr.s(auto_attribs=True, kw_only=True)
+class PoseEstimationEpisode(Episode):
+    r"""Class for episode specification that includes initial position and
+    rotation of agent, scene name, pose reference details. An
+    episode is a description of one task instance for the agent.
+
+    Args:
+        episode_id: id of episode in the dataset, usually episode number
+        scene_id: id of scene in scene dataset
+        start_position: numpy ndarray containing 3 entries for (x, y, z)
+        start_rotation: numpy ndarray with 4 entries for (x, y, z, w)
+            elements of unit quaternion (versor) representing agent 3D
+            orientation. ref: https://en.wikipedia.org/wiki/Versor
+        pose_ref_positions: numpy ndarray containing 3 entries for (x, y, z)
+            for each pose reference - (nRef, 3)
+        pose_ref_rotations: numpy ndarray with 4 entries for (x, y, z, w)
+            for each pose reference - (nRef, 4)
+    """
+    pose_ref_positions: np.array = attr.ib(default=None, validator=not_none_validator)
+    pose_ref_rotations: np.array = attr.ib(default=None, validator=not_none_validator)
+
+
+@registry.register_sensor
+class PoseEstimationRGBSensor(Sensor):
+    r"""Sensor for PoseEstimation observations.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the PoseEstimation sensor.
+
+    Attributes:
+        _nRef: number of pose references
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._nRef = getattr(config, "NREF")
+        super().__init__(config=config)
+
+        self.current_episode_id = None
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "pose_estimation_rgb"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.COLOR
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=0,
+            high=255,
+            shape=(
+                self._nRef,
+                self.config.HEIGHT,
+                self.config.WIDTH,
+                RGBSENSOR_DIMENSION,
+            ),
+            dtype=np.uint8,
+        )
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        # Render the pose references only at the start of each episode.
+        if self.current_episode_id != episode_id:
+            self.current_episode_id = episode_id
+            ref_positions = episode.pose_ref_positions
+            ref_rotations = episode.pose_ref_rotations
+            ref_rgb = []
+            for position, rotation in zip(ref_positions, ref_rotations):
+                # Get data only from the RGB sensor
+                obs = self._sim.get_specific_sensor_observations_at(
+                    position, rotation, "rgb"
+                )
+                # remove alpha channel
+                obs = obs[:, :, :RGBSENSOR_DIMENSION]
+                ref_rgb.append(obs)
+            # Add dummy images to compensate for fewer than nRef references.
+            if len(ref_rgb) < self._nRef:
+                dummy_image = np.zeros_like(ref_rgb[0])
+                for i in range(len(ref_rgb), self._nRef):
+                    ref_rgb.append(dummy_image)
+            self._pose_ref_rgb = np.stack(ref_rgb, axis=0)
+            return np.copy(self._pose_ref_rgb)
+        else:
+            return None
+
+
+@registry.register_sensor
+class PoseEstimationDepthSensor(Sensor):
+    r"""Sensor for PoseEstimation observations.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the PoseEstimation sensor.
+
+    Attributes:
+        _nRef: number of pose references
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._nRef = getattr(config, "NREF")
+
+        if config.NORMALIZE_DEPTH:
+            self.min_depth_value = 0
+            self.max_depth_value = 1
+        else:
+            self.min_depth_value = config.MIN_DEPTH
+            self.max_depth_value = config.MAX_DEPTH
+
+        super().__init__(config=config)
+
+        self.current_episode_id = None
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "pose_estimation_depth"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.DEPTH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=self.min_depth_value,
+            high=self.max_depth_value,
+            shape=(self._nRef, self.config.HEIGHT, self.config.WIDTH, 1),
+            dtype=np.float32,
+        )
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        # Render the pose references only at the start of each episode.
+        if self.current_episode_id != episode_id:
+            self.current_episode_id = episode_id
+            ref_positions = episode.pose_ref_positions
+            ref_rotations = episode.pose_ref_rotations
+
+            ref_depth = []
+            for position, rotation in zip(ref_positions, ref_rotations):
+                # Get data only from the Depth sensor
+                obs = self._sim.get_specific_sensor_observations_at(
+                    position, rotation, "depth"
+                )
+                # Process data similar to HabitatSimDepthSensor
+                obs = np.clip(obs, self.config.MIN_DEPTH, self.config.MAX_DEPTH)
+                if self.config.NORMALIZE_DEPTH:
+                    # normalize depth observation to [0, 1]
+                    obs = (obs - self.config.MIN_DEPTH) / self.config.MAX_DEPTH
+                obs = np.expand_dims(obs, axis=2)
+                ref_depth.append(obs)
+            # Add dummy images to compensate for fewer than nRef references.
+            if len(ref_depth) < self._nRef:
+                dummy_image = np.zeros_like(ref_depth[0])
+                for i in range(len(ref_depth), self._nRef):
+                    ref_depth.append(dummy_image)
+            self._pose_ref_depth = np.stack(ref_depth, axis=0)
+            return np.copy(self._pose_ref_depth)
+        else:
+            return None
+
+
+@registry.register_sensor
+class PoseEstimationRegressSensor(Sensor):
+    r"""Sensor for PoseEstimation observations. Returns the full GT pose
+        of references w.r.t agent's starting point. Useful for evaluation
+        and computing rewards.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the PoseEstimation sensor.
+
+    Attributes:
+        _nRef: number of pose references
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._nRef = getattr(config, "NREF")
+        super().__init__(config=config)
+
+        self.current_episode_id = None
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "pose_estimation_reg"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.POSITION
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=-1000000.0, high=1000000.0, shape=(self._nRef, 4), dtype=np.float32,
+        )
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        # Render the pose references only at the start of each episode.
+        if self.current_episode_id != episode_id:
+            self.current_episode_id = episode_id
+            ref_positions = episode.pose_ref_positions
+            ref_rotations = episode.pose_ref_rotations
+            start_position = episode.start_position
+            start_rotation = episode.start_rotation
+            start_quat = quat_from_coeffs(start_rotation)
+            # Measures the angle from forward to right directions.
+            start_heading = compute_heading_from_quaternion(start_quat)
+            xs, ys = -start_position[2], start_position[0]
+            ref_reg = []
+            for position, rotation in zip(ref_positions, ref_rotations):
+                rotation = quat_from_coeffs(rotation)
+                # Measures the angle from forward to right directions.
+                ref_heading = compute_heading_from_quaternion(rotation)
+                xr, yr = -position[2], position[0]
+                # Compute vector from start to ref assuming start is
+                # facing forward @ (0, 0)
+                rad_sr = np.sqrt((xr - xs) ** 2 + (yr - ys) ** 2)
+                phi_sr = np.arctan2(yr - ys, xr - xs) - start_heading
+                theta_sr = ref_heading - start_heading
+                # Normalize theta_sr
+                theta_sr = np.arctan2(np.sin(theta_sr), np.cos(theta_sr))
+                ref_reg.append((rad_sr, phi_sr, theta_sr, 0.0))
+            if len(ref_reg) < self._nRef:
+                for i in range(len(ref_reg), self._nRef):
+                    ref_reg.append((0.0, 0.0, 0.0, 0.0))
+            self._pose_ref_reg = np.array(ref_reg)
+            return np.copy(self._pose_ref_reg)
+        else:
+            return None
+
+
+@registry.register_sensor
+class PoseEstimationMaskSensor(Sensor):
+    r"""Sensor for PoseEstimation observations. Returns the mask
+    indicating which references are valid.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the PoseEstimation sensor.
+
+    Attributes:
+        _nRef: number of pose references
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._nRef = getattr(config, "NREF")
+        super().__init__(config=config)
+
+        self.current_episode_id = None
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "pose_estimation_mask"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.MEASUREMENT
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(
+            low=-1000000.0, high=1000000.0, shape=(self._nRef,), dtype=np.float32,
+        )
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            pose_ref_mask = np.ones((self._nRef,))
+            pose_ref_mask[len(episode.pose_ref_positions) :] = 0
+            self._pose_ref_mask = pose_ref_mask
+            return np.copy(self._pose_ref_mask)
+        else:
+            return None
+
+
+@registry.register_sensor
+class DeltaSensor(Sensor):
+    r"""Sensor that returns the odometer readings from the previous action.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the PoseEstimation sensor.
+
+    Attributes:
+        _nRef: number of pose references
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        super().__init__(config=config)
+
+        self.current_episode_id = None
+        self.prev_position = None
+        self.prev_rotation = None
+        self.start_position = None
+        self.start_rotation = None
+        self._enable_odometer_noise = self._sim._enable_odometer_noise
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "delta_sensor"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(low=-1000000.0, high=1000000.0, shape=(4,), dtype=np.float32,)
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self._enable_odometer_noise:
+            agent_position = self._sim._estimated_position
+            agent_rotation = self._sim._estimated_rotation
+        else:
+            agent_state = self._sim.get_agent_state()
+            agent_position = agent_state.position
+            agent_rotation = agent_state.rotation
+
+        if self.current_episode_id != episode_id:
+            # A new episode has started
+            self.current_episode_id = episode_id
+            delta = np.array([0.0, 0.0, 0.0, 0.0])
+            self.start_position = copy.deepcopy(agent_position)
+            self.start_rotation = copy.deepcopy(agent_rotation)
+        else:
+            current_position = agent_position
+            current_rotation = agent_rotation
+            # For the purposes of this sensor, forward is X and rightward is Y.
+            # The heading is measured positively from X to Y.
+            curr_x, curr_y = -current_position[2], current_position[0]
+            curr_heading = compute_heading_from_quaternion(current_rotation)
+            prev_x, prev_y = -self.prev_position[2], self.prev_position[0]
+            prev_heading = compute_heading_from_quaternion(self.prev_rotation)
+            dr = math.sqrt((curr_x - prev_x) ** 2 + (curr_y - prev_y) ** 2)
+            dphi = math.atan2(curr_y - prev_y, curr_x - prev_x)
+            dhead = curr_heading - prev_heading
+            # Convert these to the starting point's coordinate system.
+            start_heading = compute_heading_from_quaternion(self.start_rotation)
+            dphi = dphi - start_heading
+            delta = np.array([dr, dphi, dhead, 0.0])
+        self.prev_position = copy.deepcopy(agent_position)
+        self.prev_rotation = copy.deepcopy(agent_rotation)
+
+        return delta
+
+
+@registry.register_sensor
+class OracleActionSensor(Sensor):
+    r"""Sensor that returns the shortest path action to specific targets.
+
+    Args:
+        sim: reference to the simulator for calculating task observations.
+        config: config for the PoseEstimation sensor.
+
+    Attributes:
+        _nRef: number of pose references
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        super().__init__(config=config)
+
+        self.current_episode_id = None
+        self.target_positions = []
+        goal_radius = config.GOAL_RADIUS
+        self.goal_radius = goal_radius
+        self.follower = ShortestPathFollower(sim, goal_radius, False)
+        self.follower.mode = "geodesic_path"
+        self.oracle_type = config.ORACLE_TYPE
+        self.num_targets = config.NUM_TARGETS
+        if self.oracle_type == "object":
+            self.objects_covered_metric = ObjectsCoveredGeometric(sim, config)
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "oracle_action_sensor"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(low=0, high=10, shape=(1,), dtype=np.int32,)
+
+    def _sample_random_targets(self, episode):
+        """Samples a random set of navigable points from within the same
+        floor to navigate to.
+        """
+        num_targets = self.num_targets
+        self.target_positions = []
+        agent_y = self._sim.get_agent_state().position[1]
+        for i in range(num_targets):
+            point = self._sim.sample_navigable_point()
+            point_y = point[1]
+            # Sample points within the same floor
+            while abs(agent_y - point_y) > 0.5:
+                point = self._sim.sample_navigable_point()
+                point_y = point[1]
+            self.target_positions.append(point)
+
+    def _sample_pose_targets(self, episode):
+        """Sets the targets as the set of pose references in the environment
+        sampled for this episode. They are assumed to be on the same floor.
+        A greedy sampling of the pose references is done where the aim is to
+        reduce the distance travelled to cover all of them.
+        """
+        num_targets = self.num_targets
+        self.target_positions = []
+        pose_ref_positions = set([tuple(pos) for pos in episode.pose_ref_positions])
+        curr_position = episode.start_position
+        # Sample the pose references as targets in a greedy fashion.
+        while len(pose_ref_positions) > 0:
+            min_dist = 1000000000.0
+            min_dist_position = None
+            for position in pose_ref_positions:
+                dist = self._sim.geodesic_distance(curr_position, position)
+                if dist < min_dist:
+                    min_dist = dist
+                    min_dist_position = position
+            curr_position = min_dist_position
+            # None of the other points are reachable.
+            if min_dist_position is None:
+                break
+            self.target_positions.append(min_dist_position)
+            pose_ref_positions.remove(min_dist_position)
+        # Add more random targets to fill the quota of num_targets.
+        nRefValid = len(self.target_positions)
+        agent_y = self._sim.get_agent_state().position[1]
+        for i in range(nRefValid, num_targets):
+            point = self._sim.sample_navigable_point()
+            point_y = point[1]
+            # Sample points within the same floor.
+            while abs(agent_y - point_y) > 0.5:
+                point = self._sim.sample_navigable_point()
+                point_y = point[1]
+            self.target_positions.append(point)
+
+    def _sample_object_targets(self, episode):
+        """Sets the targets as the objects in the environment that can
+        be reached and are on the same floor. A greedy sampling of the
+        objects is done where the aim is to reduce the distance travelled
+        to cover all of them.
+        """
+        num_targets = self.num_targets
+        self.target_positions = []
+        self.target_objects = []
+        obj_id_pos = set(
+            [(obj["id"], tuple(obj["center"])) for obj in self._sim.object_annotations]
+        )
+        curr_position = episode.start_position
+        start_y = curr_position[1]
+        # Sample the objects as targets in a greedy fashion.
+        while len(obj_id_pos) > 0:
+            min_dist = 1000000000.0
+            min_dist_id_pos = None
+            for curr_id_pos in obj_id_pos:
+                curr_pos = curr_id_pos[1]
+                dist = self._sim.geodesic_distance(curr_position, curr_pos)
+                if dist < min_dist:
+                    min_dist = dist
+                    min_dist_id_pos = curr_id_pos
+            # None of the other points are reachable.
+            if min_dist_id_pos is None:
+                break
+            min_dist_position = min_dist_id_pos[1]
+            min_dist_id = min_dist_id_pos[0]
+            # Ignore objects that cannot be traversed to or are on a
+            # different floor.
+            if (
+                self._sim.is_navigable(min_dist_position)
+                and abs(min_dist_position[1] - start_y) <= 1.0
+            ):
+                self.target_positions.append(min_dist_position)
+                self.target_objects.append(min_dist_id)
+                curr_position = min_dist_position
+            obj_id_pos.remove(min_dist_id_pos)
+        # Add more random targets to fill the quota of num_targets.
+        nRefValid = len(self.target_positions)
+        for i in range(nRefValid, num_targets):
+            point = self._sim.sample_navigable_point()
+            while abs(point[1] - start_y) > 0.5:
+                point = self._sim.sample_navigable_point()
+            self.target_positions.append(point)
+            self.target_objects.append(None)
+
+    def _sample_targets(self, episode):
+        """Samples targets that the oracle navigates to using shortest-path
+        actions. The targets can be random, landmarks, or objects.
+        """
+        if self.oracle_type == "random":
+            self._sample_random_targets(episode)
+        elif self.oracle_type == "pose":
+            self._sample_pose_targets(episode)
+        elif self.oracle_type == "object":
+            self._sample_object_targets(episode)
+
+    def get_observation(self, observations, episode):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            self.current_episode_id = episode_id
+            self._sample_targets(episode)
+        if self.oracle_type == "object":
+            # Since the objects are generally dense in MP3D, multiple objects
+            # may be covered in the process of visiting one particular
+            # object. Make sure the oracle only navigates to objects that
+            # were not covered apriori.
+            self.objects_covered_metric.update_metric(episode, None)
+            current_target = self.target_positions[0]
+            current_obj_id = self.target_objects[0]
+            agent_position = self._sim.get_agent_state().position
+            d2target = self._sim.geodesic_distance(agent_position, current_target,)
+            # If the sampled target is already within reach or if it has been
+            # covered at some poin during the trajectory,
+            # resample a new target.
+            while (d2target <= self.goal_radius) or (
+                (current_obj_id is not None)
+                and current_obj_id in self.objects_covered_metric.seen_objects
+            ):
+                self.target_positions = self.target_positions[1:]
+                self.target_objects = self.target_objects[1:]
+                current_target = self.target_positions[0]
+                current_obj_id = self.target_objects[0]
+                d2target = self._sim.geodesic_distance(agent_position, current_target,)
+        else:
+            current_target = self.target_positions[0]
+            agent_position = self._sim.get_agent_state().position
+            d2target = self._sim.geodesic_distance(agent_position, current_target,)
+            # If the sampled target is already within reach,
+            # resample a new target.
+            while d2target <= self.goal_radius:
+                self.target_positions = self.target_positions[1:]
+                current_target = self.target_positions[0]
+                d2target = self._sim.geodesic_distance(agent_position, current_target,)
+        oracle_action = self.follower.get_next_action(current_target)
+        return np.array([oracle_action], dtype=np.int32)
+
+
+@registry.register_sensor
+class CollisionSensor(Sensor):
+    """Returns 1 if a collision occured in the previous action, otherwise
+    it returns 0.
+    """
+
+    def __init__(self, sim, config):
+        self._sim = sim
+        super().__init__(config=config)
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "collision_sensor"
+
+    def _get_sensor_type(self, *args: Any, **kwargs: Any):
+        return SensorTypes.PATH
+
+    def _get_observation_space(self, *args: Any, **kwargs: Any):
+        return spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32,)
+
+    def get_observation(self, observations, episode):
+        if self._sim.previous_step_collided:
+            return np.array([1.0])
+        else:
+            return np.array([0.0])
+
+
+@registry.register_measure
+class OPSR(Measure):
+    r"""OPSR (Oracle Pose Success Rate)
+    Measures if the agent has come close to the ground truth pose.
+    Note: Landmarks visited = OPSR * #landmarks.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._config = config
+        self._successes = None
+        self._geodesic_dist_thresh = config.GEODESIC_DIST_THRESH
+        self._angular_dist_thresh = config.ANGULAR_DIST_THRESH
+        self.current_episode_id = None
+        self._points_of_interest = None
+        self._enable_odometer_noise = self._sim._enable_odometer_noise
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "opsr"
+
+    def reset_metric(self, episode):
+        self._metric = 0
+        nRef = len(episode.pose_ref_positions)
+        self._successes = [0.0 for _ in range(nRef)]
+
+    def _euclidean_distance(self, position_a, position_b):
+        return np.linalg.norm(np.array(position_b) - np.array(position_a), ord=2)
+
+    def _compute_points_of_interest(self, episode):
+        ref_positions = episode.pose_ref_positions
+        ref_rotations = episode.pose_ref_rotations
+        points_of_interest = []
+        fwd_direction_vector = np.array([0, 0, -1])
+        for position, rotation in zip(ref_positions, ref_rotations):
+            position = np.array(position)
+            rotation = quat_from_coeffs(rotation)
+            # Compute depth of the central patch of the pose reference.
+            obs = self._sim.get_specific_sensor_observations_at(
+                position, rotation, "depth"
+            )
+            H, W = obs.shape[:2]
+            Hby2, Wby2 = H // 2, W // 2
+            # Raw depth in meters to the central patch, a.k.a. the
+            # point of interest.
+            center_depth = np.median(
+                obs[(Hby2 - 5) : (Hby2 + 6), (Wby2 - 5) : (Wby2 + 6)]
+            )
+            # Compute 3D coordinate of the point of interest.
+            heading_vector = quaternion_rotate_vector(rotation, fwd_direction_vector)
+            point_of_interest = center_depth * heading_vector + position
+            points_of_interest.append(point_of_interest)
+        self._points_of_interest = np.stack(points_of_interest, axis=0)
+
+    def update_metric(self, episode, action):
+        # To measure success rate, the agent's pose needs to close to the
+        # landmark's pose, and the agent must look at the same things (points
+        # of interest) that the landmark view is looking at, i.e., there must
+        # be no occlusions blocking the agent's view of the points of interest.
+        episode_id = (episode.episode_id, episode.scene_id)
+        if self.current_episode_id != episode_id:
+            # Compute the points of interest only at the episode starting.
+            self.current_episode_id = episode_id
+            self._compute_points_of_interest(episode)
+        if self._enable_odometer_noise:
+            current_position = self._sim._estimated_position.tolist()
+            current_rotation = self._sim._estimated_rotation
+        else:
+            agent_state = self._sim.get_agent_state()
+            current_position = agent_state.position.tolist()
+            current_rotation = agent_state.rotation
+
+        # Measure the agent's point of interest.
+        current_heading = compute_heading_from_quaternion(current_rotation)
+        current_rotation_list = quaternion_to_list(current_rotation)
+        curr_depth = self._sim.get_specific_sensor_observations_at(
+            current_position, current_rotation_list, "depth",
+        )
+        H, W = curr_depth.shape[:2]
+        Hby2, Wby2 = H // 2, W // 2
+        # Raw depth in meters to the central patch, a.k.a. the
+        # point of interest.
+        curr_center_depth = np.median(
+            curr_depth[(Hby2 - 5) : (Hby2 + 6), (Wby2 - 5) : (Wby2 + 6)]
+        )
+        # Verify that the agent is looking at the same point-of-interest as
+        # the pose reference (within some thresholds).
+        pose_ref_positions = episode.pose_ref_positions
+        pose_ref_rotations = np.array(episode.pose_ref_rotations)
+        for i in range(len(pose_ref_positions)):
+            # This might fail in the case of noisy odometry when the
+            # estimated position is invalid in the graph.
+            try:
+                distance = self._sim.geodesic_distance(
+                    current_position, pose_ref_positions[i]
+                )
+            except:
+                continue
+            # Criterion (1): The agent has to be close enough to
+            # the actual viewpoint.
+            if distance >= self._geodesic_dist_thresh:
+                continue
+            # Criterion (2): The viewing angle from must be within a threshold
+            # of the correct viewing angle to the reference point of interest.
+            point_of_interest = self._points_of_interest[i]
+            x_poi = -point_of_interest[2]  # -Z is forward --> X
+            y_poi = point_of_interest[0]  # X is rightward --> Y
+            x_curr = -current_position[2]
+            y_curr = current_position[0]
+            heading_curr2poi = math.atan2(y_poi - y_curr, x_poi - x_curr)
+            diff_angle = heading_curr2poi - current_heading
+            diff_angle = math.atan2(math.sin(diff_angle), math.cos(diff_angle))
+            diff_angle = math.degrees(abs(diff_angle))
+            if diff_angle >= self._angular_dist_thresh:
+                continue
+            # Criterion (3): The distances of the current point of interest
+            # and the reference point of interest from the agent's position
+            # must be similar. This accounts for occlusions.
+            dist2poi = math.sqrt((x_poi - x_curr) ** 2 + (y_poi - y_curr) ** 2)
+            occlusion_err = abs(dist2poi - curr_center_depth)
+            if occlusion_err >= 0.5:
+                continue
+            # If all the criteria were satisfied, this is a successful
+            # visitation to the ith reference.
+            self._successes[i] = 1
+
+        self._metric = np.sum(self._successes)
+
+
+@registry.register_measure
+class AreaCovered(Measure):
+    r"""Area covered metric in grid-cells.  Can only be used in
+    the occupancy environment.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._config = config
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "area_covered"
+
+    def reset_metric(self, episode):
+        self._metric = 0
+
+    def update_metric(self, episode, action):
+        self._metric = self._sim.occupancy_info["seen_area"]
+
+
+@registry.register_measure
+class IncAreaCovered(Measure):
+    r"""Increase in area covered (in grid cells) over the past action.
+    Can only be used in the occupancy environment.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._config = config
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "inc_area_covered"
+
+    def reset_metric(self, episode):
+        self._metric = 0
+
+    def update_metric(self, episode, action):
+        self._metric = self._sim.occupancy_info["inc_area"]
+
+
+@registry.register_measure
+class ObjectsCoveredGeometric(Measure):
+    r"""
+    Number of objects covered (estimated based on geometric knowledge rather than
+    rendering semantic images)
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._config = config
+        self.current_episode_id = None
+        self.seen_objects = set()
+        self.seen_categories = set()
+        self.IGNORE_CLASSES = [
+            "floor",
+            "wall",
+            "door",
+            "misc",
+            "ceiling",
+            "void",
+            "stairs",
+            "railing",
+            "column",
+            "beam",
+            "",
+            "board_panel",
+        ]
+        self.intrinsic_matrix = self._sim.occupancy_info["intrinsic_matrix"]
+        self.agent_height = self._sim.occupancy_info["agent_height"]
+        self.min_depth = float(self._sim.config.DEPTH_SENSOR.MIN_DEPTH)
+        self.max_depth = float(self._sim.config.DEPTH_SENSOR.MAX_DEPTH)
+        self._enable_odometer_noise = self._sim._enable_odometer_noise
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "objects_covered_geometric"
+
+    def reset_metric(self, episode):
+        self._metric = {
+            "small_objects_visited": 0.0,
+            "medium_objects_visited": 0.0,
+            "large_objects_visited": 0.0,
+            "categories_visited": 0.0,
+        }
+        self.seen_objects = set()
+        self.seen_categories = set()
+
+    def update_metric(self, episode, action):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if episode_id != self.current_episode_id:
+            self.current_episode_id = episode_id
+            self.reset_metric(episode)
+
+        objects = self._sim.object_annotations
+        if self._enable_odometer_noise:
+            agent_position = self._sim._estimated_position
+            agent_rotation = self._sim._estimated_rotation
+        else:
+            agent_state = self._sim.get_agent_state()
+            agent_position = agent_state.position
+            agent_rotation = agent_state.rotation
+        agent_position_list = agent_position.tolist()
+        agent_rotation_list = quaternion_to_list(agent_rotation)
+        curr_depth = self._sim.get_specific_sensor_observations_at(
+            agent_position_list, agent_rotation_list, "depth"
+        )
+        curr_depth = np.clip(
+            curr_depth, self.min_depth, self.max_depth
+        )  # Raw depth values
+        curr_depth = curr_depth[..., np.newaxis]
+        for obj in objects:
+            obj_name = obj["category_name"]
+            obj_dims = obj["sizes"]
+            obj_id = obj["id"]
+            obj_center = obj["center"]
+            if obj_id in self.seen_objects:
+                continue
+            looking_flag, obj_img_pos = self.is_looking_at_object(
+                agent_position, agent_rotation, obj_center, obj_name, curr_depth
+            )
+            if looking_flag:
+                size_class = self.get_object_size_class(sorted(obj_dims))
+                self.seen_objects.add(obj_id)
+                self.seen_categories.add(obj_name)
+                if size_class == 0:
+                    self._metric["small_objects_visited"] += 1.0
+                elif size_class == 1:
+                    self._metric["medium_objects_visited"] += 1.0
+                else:
+                    self._metric["large_objects_visited"] += 1.0
+
+        self._metric["categories_visited"] = float(len(self.seen_categories))
+
+    def convert_to_camera_coords(self, agent_position, agent_rotation, target_position):
+        T_cam2world = np.eye(4)
+        T_cam2world[:3, :3] = quaternion.as_rotation_matrix(agent_rotation)
+        T_cam2world[:3, 3] = agent_position
+        target_position = np.concatenate([target_position, np.array([1.0])], axis=0)
+        # The agent's position does not account for it's height
+        target_position[1] -= self.agent_height
+
+        T_cam2img = self.intrinsic_matrix
+        T_world2cam = np.linalg.inv(T_cam2world)
+        T_world2img = np.matmul(T_cam2img, T_world2cam)
+
+        target_in_img = np.matmul(T_world2img, target_position)
+        # target_in_img = [x * depth, y * depth, -depth, 1]
+        # where x is -1 to 1 from left to right and
+        # y is 1 to -1 from top to bottom
+        target_in_img = target_in_img[:2] / -target_in_img[2]
+
+        # Flip the y to make it go from -1 to 1 from top to bottom (similar to indexing)
+        target_in_img[1] = -target_in_img[1]
+
+        # Add 1 and divide by 2 to make x go from 0 to 1 from left to right
+        # and y go from 0 to 1 from top to bottom
+        target_in_img = (target_in_img + 1) / 2
+
+        return target_in_img
+
+    def is_looking_at_object(
+        self, agent_position, agent_rotation, object_position, obj_name, depth
+    ):
+        # Ignore non-object classes
+        if obj_name in self.IGNORE_CLASSES:
+            return False, None
+
+        # More than 3 meters away
+        dist_to_object = math.sqrt(
+            (agent_position[0] - object_position[0]) ** 2
+            + (agent_position[2] - object_position[2]) ** 2
+        )
+        if dist_to_object > 3.0:
+            return False, None
+
+        # Not on the same floor
+        if abs(agent_position[1] - object_position[1]) > 1.0:
+            return False, None
+
+        direction_vector = np.array([0, 0, -1])
+        heading_vector = quaternion_rotate_vector(
+            agent_rotation.inverse(), direction_vector
+        )
+        # This is positive from -Z to -X
+        heading_angle = cartesian_to_polar(-heading_vector[2], heading_vector[0])[1]
+
+        object_dir_vector = np.array(object_position) - np.array(agent_position)
+        # X is assumed to be -Z and Y is assumed to be -X, only then the angle is measured
+        # from -Z to -X
+        object_dir_angle = cartesian_to_polar(
+            -object_dir_vector[2], -object_dir_vector[0]
+        )[1]
+
+        diff_angle = heading_angle - object_dir_angle
+        diff_angle = abs(math.atan2(math.sin(diff_angle), math.cos(diff_angle)))
+
+        # Close, but not looking at it
+        if diff_angle > math.radians(60):
+            return False, None
+
+        obj_img_position = self.convert_to_camera_coords(
+            agent_position, agent_rotation, object_position,
+        )
+
+        # Out of the image range
+        if (obj_img_position[0] < 0.0 or obj_img_position[0] >= 1.0) or (
+            obj_img_position[1] < 0.0 or obj_img_position[1] >= 1.0
+        ):
+            return False, None
+
+        HEIGHT, WIDTH = depth.shape[:2]
+
+        obj_img_position[0] = obj_img_position[0] * WIDTH
+        obj_img_position[1] = obj_img_position[1] * HEIGHT
+        obj_img_position = (int(obj_img_position[0]), int(obj_img_position[1]))
+
+        # Obtain depth of central pixel patch in meters
+        obj_depth_patch = depth[
+            (obj_img_position[1] - 3) : (obj_img_position[1] + 4),
+            (obj_img_position[0] - 3) : (obj_img_position[0] + 4),
+            0,
+        ]
+
+        # Unable to sample a valid patch
+        if obj_depth_patch.shape[0] == 0 or obj_depth_patch.shape[1] == 0:
+            return False, None
+
+        obj_depth = obj_depth_patch.mean()
+
+        # Looking at it, but it is blocked
+        occlusion_error = abs(
+            obj_depth - dist_to_object * math.cos(object_dir_angle - heading_angle)
+        )
+        if occlusion_error > 0.3:
+            return False, None
+
+        return True, obj_img_position
+
+    def get_object_size_class(self, sizes):
+        value = (sizes[1] * sizes[2]) ** (1.0 / 2.0)
+        if value < 0.5:
+            return 0
+        elif value < 1.5:
+            return 1
+        else:
+            return 2
+
+
+@registry.register_measure
+class TopDownMapPose(TopDownMap):
+    r"""Top Down Map measure for Pose estimation. Displays the start position
+    and the pose references.
+    """
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "top_down_map_pose"
+
+    def draw_source_and_references(self, episode):
+        # mark source point
+        s_x, s_y = maps.to_grid(
+            episode.start_position[0],
+            episode.start_position[2],
+            self._coordinate_min,
+            self._coordinate_max,
+            self._map_resolution,
+        )
+        point_padding = 2 * int(np.ceil(self._map_resolution[0] / MAP_THICKNESS_SCALAR))
+        self._top_down_map[
+            s_x - point_padding : s_x + point_padding + 1,
+            s_y - point_padding : s_y + point_padding + 1,
+        ] = maps.MAP_SOURCE_POINT_INDICATOR
+
+        # Uncomment these if required.
+        # mark reference points
+        # nRef = len(episode.pose_ref_positions)
+        # for i in range(nRef):
+        #    t_x, t_y = maps.to_grid(
+        #        episode.pose_ref_positions[i][0],
+        #        episode.pose_ref_positions[i][2],
+        #        self._coordinate_min,
+        #        self._coordinate_max,
+        #        self._map_resolution,
+        #    )
+        #    self._top_down_map[
+        #        t_x - point_padding : t_x + point_padding + 1,
+        #        t_y - point_padding : t_y + point_padding + 1,
+        #    ] = maps.MAP_TARGET_POINT_INDICATOR
+
+    def reset_metric(self, episode):
+        self._step_count = 0
+        self._metric = None
+        self._top_down_map = self.get_original_map()
+        agent_position = self._sim.get_agent_state().position
+        a_x, a_y = maps.to_grid(
+            agent_position[0],
+            agent_position[2],
+            self._coordinate_min,
+            self._coordinate_max,
+            self._map_resolution,
+        )
+        self._previous_xy_location = (a_y, a_x)
+
+        self.update_fog_of_war_mask(np.array([a_x, a_y]))
+
+        # draw source and reference points last to avoid overlap.
+        if self._config.DRAW_SOURCE_AND_REFERENCES:
+            self.draw_source_and_references(episode)
+
+    def update_metric(self, episode, action):
+        super().update_metric(episode, action)
+        self._metric = topdown_to_image(self._metric)
+
+
+@registry.register_measure
+class NoveltyReward(Measure):
+    r"""Divides the set of valid locations into grid-cells. By conisdering each
+    grid-cell as a state, it assigns rewards based on the novelty of
+    the states visited.
+
+    novelty_reward(s) = 1/sqrt(N(s))
+
+    where N is the number of visitations to state s.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._config = config
+        self.current_episode_id = None
+        self._state_map = None
+        self.L_min = None
+        self.L_max = None
+        self._metric = 0.0
+        self.grid_size = config.GRID_SIZE
+        self._enable_odometer_noise = self._sim._enable_odometer_noise
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "novelty_reward"
+
+    def reset_metric(self, episode):
+        self._metric = 0.0
+        self.L_min = maps.COORDINATE_MIN
+        self.L_max = maps.COORDINATE_MAX
+        map_size = int((self.L_max - self.L_min) / self.grid_size) + 1
+        self._state_map = np.zeros((map_size, map_size))
+
+    def _convert_to_grid(self, position):
+        """position - (x, y, z) in real-world coordinates """
+        grid_x = (position[0] - self.L_min) / self.grid_size
+        grid_y = (position[2] - self.L_min) / self.grid_size
+        grid_x = int(grid_x)
+        grid_y = int(grid_y)
+        return (grid_x, grid_y)
+
+    def update_metric(self, episode, action):
+        episode_id = (episode.episode_id, episode.scene_id)
+        if episode_id != self.current_episode_id:
+            self.current_episode_id = episode_id
+            self.reset_metric(episode)
+        if self._enable_odometer_noise:
+            agent_position = self._sim._estimated_position
+        else:
+            agent_position = self._sim.get_agent_state().position
+        grid_x, grid_y = self._convert_to_grid(agent_position)
+        self._state_map[grid_y, grid_x] += 1.0
+        novelty_reward = 1 / math.sqrt(self._state_map[grid_y, grid_x])
+        self._metric = novelty_reward
+
+
+@registry.register_measure
+class CoverageNoveltyReward(Measure):
+    r"""
+    Assigns rewards based on the novelty of states covered.
+    """
+
+    def __init__(self, sim: Simulator, config: Config):
+        self._sim = sim
+        self._config = config
+
+        super().__init__()
+
+    def _get_uuid(self, *args: Any, **kwargs: Any):
+        return "coverage_novelty_reward"
+
+    def reset_metric(self, episode):
+        self._metric = 0
+
+    def update_metric(self, episode, action):
+        self._metric = self._sim.occupancy_info["seen_count_reward"]
+
+
+@registry.register_task(name="Pose-v0")
+class PoseEstimationTask(EmbodiedTask):
+    def __init__(
+        self, task_config: Config, sim: Simulator, dataset: Optional[Dataset] = None,
+    ) -> None:
+
+        task_measurements = []
+        for measurement_name in task_config.MEASUREMENTS:
+            measurement_cfg = getattr(task_config, measurement_name)
+            measure_type = registry.get_measure(measurement_cfg.TYPE)
+            assert measure_type is not None, "invalid measurement type {}".format(
+                measurement_cfg.TYPE
+            )
+            task_measurements.append(measure_type(sim, measurement_cfg))
+        self.measurements = Measurements(task_measurements)
+
+        task_sensors = []
+        for sensor_name in task_config.SENSORS:
+            sensor_cfg = getattr(task_config, sensor_name)
+            sensor_type = registry.get_sensor(sensor_cfg.TYPE)
+            assert sensor_type is not None, "invalid sensor type {}".format(
+                sensor_cfg.TYPE
+            )
+            task_sensors.append(sensor_type(sim, sensor_cfg))
+
+        self.sensor_suite = SensorSuite(task_sensors)
+        super().__init__(config=task_config, sim=sim, dataset=dataset)
+
+    def overwrite_sim_config(self, sim_config: Any, episode: Type[Episode]) -> Any:
+        return merge_sim_episode_config(sim_config, episode)
diff --git habitat/tasks/pose_estimation/shortest_path_follower.py habitat/tasks/pose_estimation/shortest_path_follower.py
new file mode 100644
index 0000000..68d0110
--- /dev/null
+++ habitat/tasks/pose_estimation/shortest_path_follower.py
@@ -0,0 +1,194 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Union
+
+import math
+import numpy as np
+
+import habitat_sim
+from habitat.core.simulator import SimulatorActions
+from habitat.sims.habitat_simulator.habitat_simulator import HabitatSim
+from habitat.utils.geometry_utils import (
+    angle_between_quaternions,
+    quaternion_from_two_vectors,
+)
+from habitat.tasks.utils import (
+    cartesian_to_polar,
+    quaternion_rotate_vector,
+    compute_heading_from_quaternion,
+)
+
+EPSILON = 1e-6
+
+
+def action_to_one_hot(action: int) -> np.array:
+    one_hot = np.zeros(len(SimulatorActions), dtype=np.float32)
+    one_hot[action] = 1
+    return one_hot
+
+
+class ShortestPathFollower:
+    r"""Utility class for extracting the action on the shortest path to the
+        goal.
+    Args:
+        sim: HabitatSim instance.
+        goal_radius: Distance between the agent and the goal for it to be
+            considered successful.
+        return_one_hot: If true, returns a one-hot encoding of the action
+            (useful for training ML agents). If false, returns the
+            SimulatorAction.
+    """
+
+    def __init__(
+        self, sim: HabitatSim, goal_radius: float, return_one_hot: bool = True
+    ):
+        assert (
+            getattr(sim, "geodesic_distance", None) is not None
+        ), "{} must have a method called geodesic_distance".format(type(sim).__name__)
+
+        self._sim = sim
+        self._max_delta = self._sim.config.FORWARD_STEP_SIZE - EPSILON
+        self._goal_radius = goal_radius
+        self._step_size = self._sim.config.FORWARD_STEP_SIZE
+
+        self._mode = (
+            "geodesic_path"
+            if getattr(sim, "get_straight_shortest_path_points", None) is not None
+            else "greedy"
+        )
+        self._return_one_hot = return_one_hot
+
+    def _get_return_value(self, action) -> Union[int, np.array]:
+        if self._return_one_hot:
+            return action_to_one_hot(action)
+        else:
+            return action
+
+    def get_next_action(self, goal_pos: np.array) -> Union[int, np.array]:
+        """Returns the next action along the shortest path.
+        """
+        # if (
+        #    np.linalg.norm(goal_pos - self._sim.get_agent_state().position)
+        #    <= self._goal_radius
+        # ):
+        #    return self._get_return_value(SimulatorActions.STOP)
+
+        max_grad_dir = self._est_max_grad_dir(goal_pos)
+        if max_grad_dir is None:
+            return self._get_return_value(SimulatorActions.MOVE_FORWARD)
+        return self._step_along_grad(max_grad_dir)
+
+    def _step_along_grad(self, grad_dir: np.quaternion) -> Union[int, np.array]:
+        current_state = self._sim.get_agent_state()
+        alpha = angle_between_quaternions(grad_dir, current_state.rotation)
+        if alpha <= np.deg2rad(self._sim.config.TURN_ANGLE) + EPSILON:
+            return self._get_return_value(SimulatorActions.MOVE_FORWARD)
+        else:
+            # These angles represent the rightward rotation from
+            # forward direction.
+            curr_dir = current_state.rotation
+            grad_angle = compute_heading_from_quaternion(grad_dir)
+            curr_angle = compute_heading_from_quaternion(curr_dir)
+            diff_angle = grad_angle - curr_angle
+            diff_angle = math.atan2(math.sin(diff_angle), math.cos(diff_angle))
+            if diff_angle > 0:
+                best_turn = SimulatorActions.TURN_RIGHT
+            else:
+                best_turn = SimulatorActions.TURN_LEFT
+            return self._get_return_value(best_turn)
+
+    def _reset_agent_state(self, state: habitat_sim.AgentState) -> None:
+        self._sim.set_agent_state(state.position, state.rotation, reset_sensors=False)
+
+    def _geo_dist(self, goal_pos: np.array) -> float:
+        return self._sim.geodesic_distance(
+            self._sim.get_agent_state().position, goal_pos
+        )
+
+    def _est_max_grad_dir(self, goal_pos: np.array) -> np.array:
+
+        current_state = self._sim.get_agent_state()
+        current_pos = current_state.position
+
+        if self.mode == "geodesic_path":
+            points = self._sim.get_straight_shortest_path_points(
+                self._sim.get_agent_state().position, goal_pos
+            )
+            # Add a little offset as things get weird if
+            # points[1] - points[0] is anti-parallel with forward
+            if len(points) < 2:
+                return None
+            max_grad_dir = quaternion_from_two_vectors(
+                self._sim.forward_vector,
+                points[1]
+                - points[0]
+                + EPSILON * np.cross(self._sim.up_vector, self._sim.forward_vector),
+            )
+            max_grad_dir.x = 0
+            max_grad_dir = np.normalized(max_grad_dir)
+        else:
+            current_rotation = self._sim.get_agent_state().rotation
+            current_dist = self._geo_dist(goal_pos)
+
+            best_geodesic_delta = -2 * self._max_delta
+            best_rotation = current_rotation
+            for _ in range(0, 360, self._sim.config.TURN_ANGLE):
+                sim_action = SimulatorActions.MOVE_FORWARD
+                self._sim.step(sim_action)
+                new_delta = current_dist - self._geo_dist(goal_pos)
+
+                if new_delta > best_geodesic_delta:
+                    best_rotation = self._sim.get_agent_state().rotation
+                    best_geodesic_delta = new_delta
+
+                # If the best delta is within (1 - cos(TURN_ANGLE))% of the
+                # best delta (the step size), then we almost certainly have
+                # found the max grad dir and should just exit
+                if np.isclose(
+                    best_geodesic_delta,
+                    self._max_delta,
+                    rtol=1 - np.cos(np.deg2rad(self._sim.config.TURN_ANGLE)),
+                ):
+                    break
+
+                self._sim.set_agent_state(
+                    current_pos,
+                    self._sim.get_agent_state().rotation,
+                    reset_sensors=False,
+                )
+
+                sim_action = SimulatorActions.TURN_LEFT
+                self._sim.step(sim_action)
+
+            self._reset_agent_state(current_state)
+
+            max_grad_dir = best_rotation
+
+        return max_grad_dir
+
+    @property
+    def mode(self):
+        return self._mode
+
+    @mode.setter
+    def mode(self, new_mode: str):
+        r"""Sets the mode for how the greedy follower determines the best next
+            step.
+        Args:
+            new_mode: geodesic_path indicates using the simulator's shortest
+                path algorithm to find points on the map to navigate between.
+                greedy indicates trying to move forward at all possible
+                orientations and selecting the one which reduces the geodesic
+                distance the most.
+        """
+        assert new_mode in {"geodesic_path", "greedy"}
+        if new_mode == "geodesic_path":
+            assert (
+                getattr(self._sim, "get_straight_shortest_path_points", None)
+                is not None
+            )
+        self._mode = new_mode
diff --git habitat/tasks/registration.py habitat/tasks/registration.py
index 90400ce..f1fc9c6 100644
--- habitat/tasks/registration.py
+++ habitat/tasks/registration.py
@@ -13,8 +13,6 @@ from habitat.tasks.nav.nav_task import NavigationTask
 def make_task(id_task, **kwargs):
     logger.info("initializing task {}".format(id_task))
     _task = registry.get_task(id_task)
-    assert _task is not None, "Could not find task with name {}".format(
-        id_task
-    )
+    assert _task is not None, "Could not find task with name {}".format(id_task)
 
     return _task(**kwargs)
diff --git habitat/tasks/utils.py habitat/tasks/utils.py
index ceb3660..8012fff 100644
--- habitat/tasks/utils.py
+++ habitat/tasks/utils.py
@@ -4,8 +4,22 @@
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+import pdb
+import math
 import numpy as np
 import quaternion  # noqa # pylint: disable=unused-import
+import scipy.stats as stats
+
+from typing import Tuple
+
+
+def quaternion_from_coeff(coeffs: np.ndarray) -> np.quaternion:
+    r"""Creates a quaternions from coeffs in [x, y, z, w] format
+    """
+    quat = np.quaternion(0, 0, 0, 0)
+    quat.real = coeffs[3]
+    quat.imag = coeffs[0:3]
+    return quat
 
 
 def quaternion_to_rotation(q_r, q_i, q_j, q_k):
@@ -55,3 +69,116 @@ def cartesian_to_polar(x, y):
     rho = np.sqrt(x ** 2 + y ** 2)
     phi = np.arctan2(y, x)
     return rho, phi
+
+
+def compute_heading_from_quaternion(rotation: np.quaternion) -> float:
+    r"""Converts np.quaternion to a heading angle scalar.
+
+    Args:
+        rotation - represents a counter-clockwise rotation about Y-axis.
+    Returns:
+        Heading angle with clockwise rotation from -Z to X being +ve.
+    """
+    direction_vector = np.array([0, 0, -1])  # Forward vector
+    heading_vector = quaternion_rotate_vector(rotation.inverse(), direction_vector)
+    # Flip sign to compute clockwise rotation
+    phi = -cartesian_to_polar(-heading_vector[2], heading_vector[0])[1]
+
+    return phi
+
+
+def compute_quaternion_from_heading(heading: float) -> np.quaternion:
+    r"""Converts a heading angle scalar to np.quaternion.
+
+    Args:
+        heading - represents a clockwise rotation angle in radians
+                  from -Z to X.
+    Returns:
+        A quaternion that represents a counter-clockwise rotation about Y-axis.
+    """
+    # Real part of quaternion.
+    q0 = math.cos(-heading / 2)
+    # Imaginary part of quaternion.
+    q = (0, math.sin(-heading / 2), 0)
+    return np.quaternion(q0, *q)
+
+
+def compute_egocentric_delta(
+    position1: np.array,
+    rotation1: np.quaternion,
+    position2: np.array,
+    rotation2: np.quaternion,
+) -> np.array:
+    r"""Computes the relative change in pose from (position1, rotation1) to
+    (position2, rotation2).
+    Conventions for position, rotation follow habitat simulator conventions,
+    i.e., -Z is forward, X is rightward, Y is upward and
+    the rotation is a counter-clockwise rotation about Y.
+
+    Args:
+        position1 - (x, y, z) position.
+        position2 - (x, y, z) position.
+        rotation1 - represents a counter-clockwise rotation about Y-axis.
+        rotation2 - represents a counter-clockwise rotation about Y-axis.
+    Returns:
+        Relative change from (position1, rotation1) to (position2, rotation2)
+        in polar coordinates (r, phi, theta)
+    """
+    x1, y1, z1 = position1
+    x2, y2, z2 = position2
+    # Clockwise rotations from -Z to X.
+    theta_1 = compute_heading_from_quaternion(rotation1)
+    theta_2 = compute_heading_from_quaternion(rotation2)
+    # Compute relative delta in polar coordinates.
+    # In this 2D coordinate system, X' is forward and Y' is rightward.
+    D_rho = math.sqrt((x2 - x1) ** 2 + (z2 - z1) ** 2)
+    D_phi = math.atan2(x2 - x1, -z2 + z1) - theta_1
+    D_theta = theta_2 - theta_1
+    return np.array((D_rho, D_phi, D_theta))
+
+
+def truncated_normal_noise(eta, width):
+    """Generates truncated normal noise scalar.
+
+    Args:
+        eta - standard deviation of gaussian.
+        width - maximum absolute width on either sides of the mean.
+    Returns:
+        Sampled noise scalar from truncated gaussian with mean=0, sigma=eta,
+        and width=width.
+    """
+    mu = 0
+    sigma = eta
+    lower = mu - width
+    upper = mu + width
+    X = stats.truncnorm((lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)
+    return X.rvs()
+
+
+def compute_updated_pose(
+    position: np.array, rotation: np.quaternion, delta_rpt: np.array, delta_y: float,
+) -> Tuple[np.array, np.quaternion]:
+    r"""Given initial position, rotation and an egocentric delta,
+    compute the updated pose by linear transformations.
+
+    Args:
+        position - (x, y, z) position.
+        rotation - represents a counter-clockwise rotation about Y-axis.
+        delta_rpt - egocentric delta in polar coordinates.
+        delta_y - change in Y position.
+    Returns:
+        New position, rotation with deltas applied to old position, rotation.
+    """
+    x, y, z = position
+    theta = compute_heading_from_quaternion(rotation)
+    D_rho, D_phi, D_theta = delta_rpt
+
+    x_new = x + D_rho * math.sin(theta + D_phi)
+    y_new = y + delta_y
+    z_new = z - D_rho * math.cos(theta + D_phi)
+    theta_new = theta + D_theta
+
+    position_new = np.array([x_new, y_new, z_new])
+    rotation_new = compute_quaternion_from_heading(theta_new)
+
+    return (position_new, rotation_new)
diff --git habitat/utils/visualizations/fog_of_war.py habitat/utils/visualizations/fog_of_war.py
index 388acb0..932c10f 100644
--- habitat/utils/visualizations/fog_of_war.py
+++ habitat/utils/visualizations/fog_of_war.py
@@ -100,12 +100,7 @@ def draw_fog_of_war_line(top_down_map, fog_of_war_mask, pt1, pt2):
 
 @numba.jit(nopython=True)
 def _draw_loop(
-    top_down_map,
-    fog_of_war_mask,
-    current_point,
-    current_angle,
-    max_line_len,
-    angles,
+    top_down_map, fog_of_war_mask, current_point, current_angle, max_line_len, angles,
 ):
     for angle in angles:
         draw_fog_of_war_line(
@@ -114,9 +109,7 @@ def _draw_loop(
             current_point,
             current_point
             + max_line_len
-            * np.array(
-                [np.cos(current_angle + angle), np.sin(current_angle + angle)]
-            ),
+            * np.array([np.cos(current_angle + angle), np.sin(current_angle + angle)]),
         )
 
 
@@ -147,9 +140,7 @@ def reveal_fog_of_war(
     fov = np.deg2rad(fov)
 
     # Set the angle step to a value such that delta_angle * max_line_len = 1
-    angles = np.arange(
-        -fov / 2, fov / 2, step=1.0 / max_line_len, dtype=np.float32
-    )
+    angles = np.arange(-fov / 2, fov / 2, step=1.0 / max_line_len, dtype=np.float32)
 
     fog_of_war_mask = current_fog_of_war_mask.copy()
     _draw_loop(
diff --git habitat/utils/visualizations/maps.py habitat/utils/visualizations/maps.py
index 714fd4e..158736b 100644
--- habitat/utils/visualizations/maps.py
+++ habitat/utils/visualizations/maps.py
@@ -17,10 +17,7 @@ from habitat.utils.visualizations import utils
 
 AGENT_SPRITE = imageio.imread(
     os.path.join(
-        os.path.dirname(__file__),
-        "assets",
-        "maps_topdown_agent_sprite",
-        "100x100.png",
+        os.path.dirname(__file__), "assets", "maps_topdown_agent_sprite", "100x100.png",
     )
 )
 AGENT_SPRITE = np.ascontiguousarray(np.flipud(AGENT_SPRITE))
@@ -39,11 +36,13 @@ TOP_DOWN_MAP_COLORS[10:] = cv2.applyColorMap(
     np.arange(246, dtype=np.uint8), cv2.COLORMAP_JET
 ).squeeze(1)[:, ::-1]
 TOP_DOWN_MAP_COLORS[MAP_INVALID_POINT] = [255, 255, 255]
-TOP_DOWN_MAP_COLORS[MAP_VALID_POINT] = [150, 150, 150]
+TOP_DOWN_MAP_COLORS[MAP_VALID_POINT] = [50, 170, 50]
+# TOP_DOWN_MAP_COLORS[MAP_VALID_POINT] = [150, 150, 150]
 TOP_DOWN_MAP_COLORS[MAP_BORDER_INDICATOR] = [50, 50, 50]
 TOP_DOWN_MAP_COLORS[MAP_SOURCE_POINT_INDICATOR] = [0, 0, 200]
 TOP_DOWN_MAP_COLORS[MAP_TARGET_POINT_INDICATOR] = [200, 0, 0]
-TOP_DOWN_MAP_COLORS[MAP_SHORTEST_PATH_COLOR] = [0, 200, 0]
+# TOP_DOWN_MAP_COLORS[MAP_SHORTEST_PATH_COLOR] = [0, 200, 0]
+TOP_DOWN_MAP_COLORS[MAP_SHORTEST_PATH_COLOR] = [75, 0, 130]
 
 
 def draw_agent(
@@ -70,13 +69,9 @@ def draw_agent(
     # the agent sprite size should stay the same.
     initial_agent_size = AGENT_SPRITE.shape[0]
     new_size = rotated_agent.shape[0]
-    agent_size_px = max(
-        1, int(agent_radius_px * 2 * new_size / initial_agent_size)
-    )
+    agent_size_px = max(1, int(agent_radius_px * 2 * new_size / initial_agent_size))
     resized_agent = cv2.resize(
-        rotated_agent,
-        (agent_size_px, agent_size_px),
-        interpolation=cv2.INTER_LINEAR,
+        rotated_agent, (agent_size_px, agent_size_px), interpolation=cv2.INTER_LINEAR,
     )
     utils.paste_overlapping_image(image, resized_agent, agent_center_coord)
     return image
@@ -135,9 +130,7 @@ def pointnav_draw_target_birdseye_view(
     )
     movement_scale = 1.0 / goal_distance_padding
     half_res = resolution_px // 2
-    im_position = np.full(
-        (resolution_px, resolution_px, 3), 255, dtype=np.uint8
-    )
+    im_position = np.full((resolution_px, resolution_px, 3), 255, dtype=np.uint8)
 
     # Draw bands:
     for scale, color in zip(target_band_radii, target_band_colors):
@@ -198,6 +191,24 @@ def to_grid(
     return grid_x, grid_y
 
 
+def to_grid_v2(
+    realworld_x: float,
+    realworld_y: float,
+    x_min: float,
+    y_min: float,
+    map_scale: float,
+) -> Tuple[int, int]:
+    r"""Return gridworld index of realworld coordinates assuming top-left
+    corner is the origin. This differs from to_grid() based on the map extent
+    specification. x_min, y_min are the minimum x and y coordinates from the
+    environment. map_scale is the real-world length that corresponds to one
+    grid-cell in the map.
+    """
+    grid_x = int((realworld_x - x_min) / map_scale)
+    grid_y = int((realworld_y - y_min) / map_scale)
+    return grid_x, grid_y
+
+
 def from_grid(
     grid_x: int,
     grid_y: int,
@@ -219,6 +230,21 @@ def from_grid(
     return realworld_x, realworld_y
 
 
+def from_grid_v2(
+    grid_x: int, grid_y: int, x_min: float, y_min: float, map_scale: float,
+) -> Tuple[float, float]:
+    r"""Inverse of to_grid_v2 function. Return real world coordinate from
+    gridworld assuming top-left corner is the origin. This differs from
+    from_grid() based on the map extent specification.
+    x_min, y_min are the minimum x and y coordinates from the environment.
+    map_scale is the real-world length that corresponds to one grid-cell
+    in the map.
+    """
+    realworld_x = x_min + grid_x * map_scale
+    realworld_y = y_min + grid_y * map_scale
+    return realworld_x, realworld_y
+
+
 def _outline_border(top_down_map):
     left_right_block_nav = (top_down_map[:, :-1] == 1) & (
         top_down_map[:, :-1] != top_down_map[:, 1:]
@@ -300,12 +326,8 @@ def get_topdown_map(
             realworld_x, realworld_y = from_grid(
                 ii, jj, COORDINATE_MIN, COORDINATE_MAX, map_resolution
             )
-            valid_point = sim.is_navigable(
-                [realworld_x, start_height, realworld_y]
-            )
-            top_down_map[ii, jj] = (
-                MAP_VALID_POINT if valid_point else MAP_INVALID_POINT
-            )
+            valid_point = sim.is_navigable([realworld_x, start_height, realworld_y])
+            top_down_map[ii, jj] = MAP_VALID_POINT if valid_point else MAP_INVALID_POINT
 
     # Draw border if necessary
     if draw_border:
@@ -321,13 +343,70 @@ def get_topdown_map(
             min(range_y[-1] + border_padding + 1, top_down_map.shape[1]),
         )
 
-        _outline_border(
-            top_down_map[range_x[0] : range_x[1], range_y[0] : range_y[1]]
-        )
+        _outline_border(top_down_map[range_x[0] : range_x[1], range_y[0] : range_y[1]])
+    return top_down_map
+
+
+def get_topdown_map_v2(
+    sim: Simulator,
+    map_extents: Tuple[int, int, int, int],
+    map_scale: float,
+    num_samples: int = 20000,
+) -> np.ndarray:
+    r"""Return a top-down occupancy map for a sim. Note, this only returns
+    valid values for whatever floor the agent is currently on. This differs
+    from get_topdown_map() based on the map size specification.
+
+    Args:
+        sim: The simulator.
+        map_resolution: The resolution of map which will be computed and
+            returned.
+        num_samples: The number of random navigable points which will be
+            initially
+            sampled. For large environments it may need to be increased.
+        draw_border: Whether to outline the border of the occupied spaces.
+
+    Returns:
+        Image containing 0 if occupied, 1 if unoccupied, and 2 if border (if
+        the flag is set).
+    """
+
+    x_min, x_max, y_min, y_max = map_extents
+    map_resolution = (
+        int((x_max - x_min) / map_scale),
+        int((y_max - y_min) / map_scale),
+    )
+    top_down_map = np.zeros(map_resolution, dtype=np.uint8)
+
+    start_height = sim.get_agent_state().position[1]
+
+    # Use sampling to find the extrema points that might be navigable.
+    range_x = (map_resolution[0], 0)
+    range_y = (map_resolution[1], 0)
+    for _ in range(num_samples):
+        point = sim.sample_navigable_point()
+        # Check if on same level as original
+        if np.abs(start_height - point[1]) > 0.5:
+            continue
+        g_x, g_y = to_grid_v2(point[0], point[2], x_min, y_min, map_scale)
+        range_x = (min(range_x[0], g_x), max(range_x[1], g_x))
+        range_y = (min(range_y[0], g_y), max(range_y[1], g_y))
+
+    range_x = (max(0, range_x[0]), min(map_resolution[0], range_x[1]))
+    range_y = (max(0, range_y[0]), min(map_resolution[1], range_y[1]))
+
+    # Search over grid for valid points.
+    for ii in range(range_x[0], range_x[1]):
+        for jj in range(range_y[0], range_y[1]):
+            realworld_x, realworld_y = from_grid_v2(ii, jj, x_min, y_min, map_scale)
+            valid_point = sim.is_navigable([realworld_x, start_height, realworld_y])
+            top_down_map[ii, jj] = MAP_VALID_POINT if valid_point else MAP_INVALID_POINT
+
     return top_down_map
 
 
-FOG_OF_WAR_COLOR_DESAT = np.array([[0.7], [1.0]])
+FOG_OF_WAR_COLOR_DESAT = np.array([[0.2], [1.0]])
+# FOG_OF_WAR_COLOR_DESAT = np.array([[0.4], [1.0]])
 
 
 def colorize_topdown_map(
@@ -336,7 +415,7 @@ def colorize_topdown_map(
     r"""Convert the top down map to RGB based on the indicator values.
         Args:
             top_down_map: A non-colored version of the map.
-            fog_of_war_mask: A mask used to determine which parts of the 
+            fog_of_war_mask: A mask used to determine which parts of the
                 top_down_map are visible
                 Non-visible parts will be desaturated
         Returns:
@@ -345,21 +424,19 @@ def colorize_topdown_map(
     _map = TOP_DOWN_MAP_COLORS[top_down_map]
 
     if fog_of_war_mask is not None:
-        # Only desaturate things that are valid points as only valid points get revealed
+        # Only desaturate things that are valid points as only valid points
+        # get revealed.
         desat_mask = top_down_map != MAP_INVALID_POINT
 
-        _map[desat_mask] = (
-            _map * FOG_OF_WAR_COLOR_DESAT[fog_of_war_mask]
-        ).astype(np.uint8)[desat_mask]
+        _map[desat_mask] = (_map * FOG_OF_WAR_COLOR_DESAT[fog_of_war_mask]).astype(
+            np.uint8
+        )[desat_mask]
 
     return _map
 
 
 def draw_path(
-    top_down_map: np.ndarray,
-    path_points: List[Tuple],
-    color: int,
-    thickness: int = 2,
+    top_down_map: np.ndarray, path_points: List[Tuple], color: int, thickness: int = 2,
 ) -> None:
     r"""Draw path on top_down_map (in place) with specified color.
         Args:
diff --git habitat/utils/visualizations/utils.py habitat/utils/visualizations/utils.py
index 48ffc02..0a25cf2 100644
--- habitat/utils/visualizations/utils.py
+++ habitat/utils/visualizations/utils.py
@@ -56,14 +56,10 @@ def paste_overlapping_image(
 
     background_patch = background[
         (location[0] - foreground_size[0] // 2 + min_pad[0]) : (
-            location[0]
-            + (foreground_size[0] - foreground_size[0] // 2)
-            - max_pad[0]
+            location[0] + (foreground_size[0] - foreground_size[0] // 2) - max_pad[0]
         ),
         (location[1] - foreground_size[1] // 2 + min_pad[1]) : (
-            location[1]
-            + (foreground_size[1] - foreground_size[1] // 2)
-            - max_pad[1]
+            location[1] + (foreground_size[1] - foreground_size[1] // 2) - max_pad[1]
         ),
     ]
     foreground = foreground[
@@ -120,10 +116,7 @@ def images_to_video(
         os.makedirs(output_dir)
     video_name = video_name.replace(" ", "_").replace("\n", "_") + ".mp4"
     writer = imageio.get_writer(
-        os.path.join(output_dir, video_name),
-        fps=fps,
-        quality=quality,
-        **kwargs
+        os.path.join(output_dir, video_name), fps=fps, quality=quality, **kwargs
     )
     for im in tqdm.tqdm(images):
         writer.append_data(im)
@@ -200,3 +193,49 @@ def observations_to_image(observation: Dict, info: Dict) -> np.ndarray:
         )
         frame = np.concatenate((egocentric_view, top_down_map), axis=1)
     return frame
+
+
+def topdown_to_image(topdown_info: np.ndarray) -> np.ndarray:
+    r"""Generate image of the topdown map.
+    """
+    top_down_map = topdown_info["map"]
+    fog_of_war_mask = topdown_info["fog_of_war_mask"]
+    top_down_map = maps.colorize_topdown_map(top_down_map, fog_of_war_mask)
+    map_agent_pos = topdown_info["agent_map_coord"]
+
+    # Add zero padding
+    min_map_size = 200
+    if top_down_map.shape[0] != top_down_map.shape[1]:
+        H = top_down_map.shape[0]
+        W = top_down_map.shape[1]
+        if H > W:
+            pad_value = (H - W) // 2
+            padding = ((0, 0), (pad_value, pad_value), (0, 0))
+            map_agent_pos = (map_agent_pos[0], map_agent_pos[1] + pad_value)
+        else:
+            pad_value = (W - H) // 2
+            padding = ((pad_value, pad_value), (0, 0), (0, 0))
+            map_agent_pos = (map_agent_pos[0] + pad_value, map_agent_pos[1])
+        top_down_map = np.pad(
+            top_down_map, padding, mode="constant", constant_values=255
+        )
+
+    if top_down_map.shape[0] < min_map_size:
+        H, W = top_down_map.shape[:2]
+        top_down_map = cv2.resize(top_down_map, (min_map_size, min_map_size))
+        map_agent_pos = (
+            int(map_agent_pos[0] * min_map_size // H),
+            int(map_agent_pos[1] * min_map_size // W),
+        )
+    top_down_map = maps.draw_agent(
+        image=top_down_map,
+        agent_center_coord=map_agent_pos,
+        agent_rotation=topdown_info["agent_angle"],
+        agent_radius_px=top_down_map.shape[0] // 16,
+    )
+    # if top_down_map.shape[0] < min_map_size:
+    #    pad_value = (min_map_size - top_down_map.shape[0]) // 2
+    #    padding = ((pad_value, pad_value), (pad_value, pad_value), (0, 0))
+    #    top_down_map = np.pad(top_down_map, padding, mode='constant', constant_values=255)
+
+    return top_down_map
diff --git habitat_baselines/agents/ppo_agents.py habitat_baselines/agents/ppo_agents.py
index 919298f..32f1669 100644
--- habitat_baselines/agents/ppo_agents.py
+++ habitat_baselines/agents/ppo_agents.py
@@ -129,9 +129,7 @@ class PPOAgent(Agent):
 def main():
     parser = argparse.ArgumentParser()
     parser.add_argument(
-        "--input-type",
-        default="blind",
-        choices=["blind", "rgb", "depth", "rgbd"],
+        "--input-type", default="blind", choices=["blind", "rgb", "depth", "rgbd"],
     )
     parser.add_argument("--model-path", default="", type=str)
     parser.add_argument(
diff --git habitat_baselines/agents/simple_agents.py habitat_baselines/agents/simple_agents.py
index 83568ab..2344e10 100644
--- habitat_baselines/agents/simple_agents.py
+++ habitat_baselines/agents/simple_agents.py
@@ -84,9 +84,7 @@ class GoalFollower(RandomAgent):
         return angle
 
     def turn_towards_goal(self, angle_to_goal):
-        if angle_to_goal > pi or (
-            (angle_to_goal < 0) and (angle_to_goal > -pi)
-        ):
+        if angle_to_goal > pi or ((angle_to_goal < 0) and (angle_to_goal > -pi)):
             action = SimulatorActions.TURN_RIGHT
         else:
             action = SimulatorActions.TURN_LEFT
diff --git habitat_baselines/agents/slam_agents.py habitat_baselines/agents/slam_agents.py
index 59e2b73..4887513 100644
--- habitat_baselines/agents/slam_agents.py
+++ habitat_baselines/agents/slam_agents.py
@@ -52,9 +52,7 @@ def download(url, filename):
                 downloaded += len(data)
                 f.write(data)
                 done = int(50 * downloaded / total)
-                sys.stdout.write(
-                    "\r[{}{}]".format("█" * done, "." * (50 - done))
-                )
+                sys.stdout.write("\r[{}{}]".format("█" * done, "." * (50 - done)))
                 sys.stdout.flush()
     sys.stdout.write("\n")
 
@@ -70,15 +68,9 @@ def make_good_config_for_orbslam2(config):
     config.SIMULATOR.RGB_SENSOR.HEIGHT = 256
     config.SIMULATOR.DEPTH_SENSOR.WIDTH = 256
     config.SIMULATOR.DEPTH_SENSOR.HEIGHT = 256
-    config.TRAINER.ORBSLAM2.CAMERA_HEIGHT = config.SIMULATOR.DEPTH_SENSOR.POSITION[
-        1
-    ]
-    config.TRAINER.ORBSLAM2.H_OBSTACLE_MIN = (
-        0.3 * config.TRAINER.ORBSLAM2.CAMERA_HEIGHT
-    )
-    config.TRAINER.ORBSLAM2.H_OBSTACLE_MAX = (
-        1.0 * config.TRAINER.ORBSLAM2.CAMERA_HEIGHT
-    )
+    config.TRAINER.ORBSLAM2.CAMERA_HEIGHT = config.SIMULATOR.DEPTH_SENSOR.POSITION[1]
+    config.TRAINER.ORBSLAM2.H_OBSTACLE_MIN = 0.3 * config.TRAINER.ORBSLAM2.CAMERA_HEIGHT
+    config.TRAINER.ORBSLAM2.H_OBSTACLE_MAX = 1.0 * config.TRAINER.ORBSLAM2.CAMERA_HEIGHT
     config.TRAINER.ORBSLAM2.MIN_PTS_IN_OBSTACLE = (
         config.SIMULATOR.DEPTH_SENSOR.WIDTH / 2.0
     )
@@ -218,9 +210,7 @@ class ORBSLAM2Agent(RandomAgent):
         self.planned_waypoints = []
         self.map2DObstacles = self.init_map2d()
         n, ch, height, width = self.map2DObstacles.size()
-        self.coordinatesGrid = generate_2dgrid(height, width, False).to(
-            self.device
-        )
+        self.coordinatesGrid = generate_2dgrid(height, width, False).to(self.device)
         self.pose6D = self.init_pose6d()
         self.action_history = []
         self.pose6D_history = []
@@ -254,9 +244,7 @@ class ORBSLAM2Agent(RandomAgent):
         if self.tracking_is_OK:
             trajectory_history = np.array(self.slam.get_trajectory_points())
             self.pose6D = homogenize_p(
-                torch.from_numpy(trajectory_history[-1])[1:]
-                .view(3, 4)
-                .to(self.device)
+                torch.from_numpy(trajectory_history[-1])[1:].view(3, 4).to(self.device)
             ).view(1, 4, 4)
             self.trajectory_history = trajectory_history
             if len(self.position_history) > 1:
@@ -289,9 +277,7 @@ class ORBSLAM2Agent(RandomAgent):
 
     def init_map2d(self):
         return (
-            torch.zeros(
-                1, 1, self.map_size_in_cells(), self.map_size_in_cells()
-            )
+            torch.zeros(1, 1, self.map_size_in_cells(), self.map_size_in_cells())
             .float()
             .to(self.device)
         )
@@ -342,10 +328,7 @@ class ORBSLAM2Agent(RandomAgent):
         # Act
         if self.waypointPose6D is None:
             self.waypointPose6D = self.get_valid_waypoint_pose6d()
-        if (
-            self.is_waypoint_reached(self.waypointPose6D)
-            or not self.tracking_is_OK
-        ):
+        if self.is_waypoint_reached(self.waypointPose6D) or not self.tracking_is_OK:
             self.waypointPose6D = self.get_valid_waypoint_pose6d()
             self.waypoint_id += 1
         action = self.decide_what_to_do()
@@ -375,9 +358,7 @@ class ORBSLAM2Agent(RandomAgent):
         angle = get_direction(
             self.pose6D.squeeze(), self.waypointPose6D.squeeze(), 0, 0
         )
-        dist = get_distance(
-            self.pose6D.squeeze(), self.waypointPose6D.squeeze()
-        )
+        dist = get_distance(self.pose6D.squeeze(), self.waypointPose6D.squeeze())
         return torch.cat(
             [
                 dist.view(1, 1),
@@ -409,10 +390,7 @@ class ORBSLAM2Agent(RandomAgent):
             self.map_size_meters,
         )
         self.estimatedGoalPos6D = planned_path2tps(
-            [self.estimatedGoalPos2D],
-            self.map_cell_size,
-            self.map_size_meters,
-            1.0,
+            [self.estimatedGoalPos2D], self.map_cell_size, self.map_size_meters, 1.0,
         ).to(self.device)[0]
         return
 
@@ -453,9 +431,7 @@ class ORBSLAM2Agent(RandomAgent):
         self.waypointPose6D = None
         current_pos = self.get_position_on_map()
         start_map = torch.zeros_like(self.map2DObstacles).to(self.device)
-        start_map[
-            0, 0, current_pos[0, 0].long(), current_pos[0, 1].long()
-        ] = 1.0
+        start_map[0, 0, current_pos[0, 0].long(), current_pos[0, 1].long()] = 1.0
         goal_map = torch.zeros_like(self.map2DObstacles).to(self.device)
         goal_map[
             0,
@@ -464,9 +440,9 @@ class ORBSLAM2Agent(RandomAgent):
             self.estimatedGoalPos2D[0, 1].long(),
         ] = 1.0
         path, cost = self.planner(
-            self.rawmap2_planner_ready(
-                self.map2DObstacles, start_map, goal_map
-            ).to(self.device),
+            self.rawmap2_planner_ready(self.map2DObstacles, start_map, goal_map).to(
+                self.device
+            ),
             self.coordinatesGrid.to(self.device),
             goal_map.to(self.device),
             start_map.to(self.device),
@@ -566,8 +542,12 @@ class ORBSLAM2MonodepthAgent(ORBSLAM2Agent):
         self.checkpoint = monocheckpoint
         if not os.path.isfile(self.checkpoint):
             mp3d_url = "http://cmp.felk.cvut.cz/~mishkdmy/navigation/mp3d_ft_monodepth_resnet50.pth"
-            suncg_me_url = "http://cmp.felk.cvut.cz/~mishkdmy/navigation/suncg_me_resnet.pth"
-            suncg_mf_url = "http://cmp.felk.cvut.cz/~mishkdmy/navigation/suncg_mf_resnet.pth"
+            suncg_me_url = (
+                "http://cmp.felk.cvut.cz/~mishkdmy/navigation/suncg_me_resnet.pth"
+            )
+            suncg_mf_url = (
+                "http://cmp.felk.cvut.cz/~mishkdmy/navigation/suncg_mf_resnet.pth"
+            )
             url = mp3d_url
             print("No monodepth checkpoint found. Downloading...", url)
             download(url, self.checkpoint)
@@ -578,9 +558,7 @@ class ORBSLAM2MonodepthAgent(ORBSLAM2Agent):
     def rgb_d_from_observation(self, habitat_observation):
         rgb = habitat_observation["rgb"]
         depth = ResizePIL2(
-            self.monodepth.compute_depth(
-                PIL.Image.fromarray(rgb).resize((320, 320))
-            ),
+            self.monodepth.compute_depth(PIL.Image.fromarray(rgb).resize((320, 320))),
             256,
         )  # /1.75
         depth[depth > 3.0] = 0
@@ -595,9 +573,7 @@ def main():
         default="orbslam2-rgbd",
         choices=["blind", "orbslam2-rgbd", "orbslam2-rgb-monod"],
     )
-    parser.add_argument(
-        "--task-config", type=str, default="tasks/pointnav_rgbd.yaml"
-    )
+    parser.add_argument("--task-config", type=str, default="tasks/pointnav_rgbd.yaml")
     args = parser.parse_args()
 
     config = get_config()
diff --git habitat_baselines/common/baseline_registry.py habitat_baselines/common/baseline_registry.py
index fe54971..ce78b6e 100644
--- habitat_baselines/common/baseline_registry.py
+++ habitat_baselines/common/baseline_registry.py
@@ -35,9 +35,7 @@ class BaselineRegistry(Registry):
         """
         from habitat_baselines.common.base_trainer import BaseTrainer
 
-        return cls._register_impl(
-            "trainer", to_register, name, assert_type=BaseTrainer
-        )
+        return cls._register_impl("trainer", to_register, name, assert_type=BaseTrainer)
 
     @classmethod
     def get_trainer(cls, name):
diff --git habitat_baselines/common/env_utils.py habitat_baselines/common/env_utils.py
index 89d3d4c..a469115 100644
--- habitat_baselines/common/env_utils.py
+++ habitat_baselines/common/env_utils.py
@@ -4,6 +4,7 @@
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+import pdb
 import random
 from typing import Type
 
@@ -26,9 +27,7 @@ def make_env_fn(
     Returns:
         env object created according to specification.
     """
-    dataset = make_dataset(
-        task_config.DATASET.TYPE, config=task_config.DATASET
-    )
+    dataset = make_dataset(task_config.DATASET.TYPE, config=task_config.DATASET)
     env = env_class(
         config_env=task_config, config_baseline=rl_env_config, dataset=dataset
     )
@@ -36,7 +35,7 @@ def make_env_fn(
     return env
 
 
-def construct_envs(config: Config, env_class: Type) -> VectorEnv:
+def construct_envs(config: Config, env_class: Type, devices=None) -> VectorEnv:
     r"""
     Create VectorEnv object with specified config and env class type.
     To allow better performance, dataset are split into small ones for
@@ -57,12 +56,23 @@ def construct_envs(config: Config, env_class: Type) -> VectorEnv:
     dataset = make_dataset(task_config.DATASET.TYPE)
     scenes = dataset.get_scenes_to_load(task_config.DATASET)
 
+    if devices is None:
+        devices = [task_config.SIMULATOR.HABITAT_SIM_V0.GPU_DEVICE_ID]
+
     if len(scenes) > 0:
         random.shuffle(scenes)
 
+        if len(scenes) < trainer_config.num_processes:
+            n_repeats = trainer_config.num_processes // len(scenes)
+            if n_repeats * len(scenes) < trainer_config.num_processes:
+                n_repeats += 1
+            new_scenes = []
+            for scene in scenes:
+                new_scenes += [scene] * n_repeats
+            scenes = new_scenes
+
         assert len(scenes) >= trainer_config.num_processes, (
-            "reduce the number of processes as there "
-            "aren't enough number of scenes"
+            "reduce the number of processes as there " "aren't enough number of scenes"
         )
 
     scene_splits = [[] for _ in range(trainer_config.num_processes)]
@@ -78,9 +88,7 @@ def construct_envs(config: Config, env_class: Type) -> VectorEnv:
         if len(scenes) > 0:
             env_config.DATASET.CONTENT_SCENES = scene_splits[i]
 
-        env_config.SIMULATOR.HABITAT_SIM_V0.GPU_DEVICE_ID = (
-            trainer_config.sim_gpu_id
-        )
+        env_config.SIMULATOR.HABITAT_SIM_V0.GPU_DEVICE_ID = devices[i % len(devices)]
 
         agent_sensors = trainer_config.sensors.strip().split(",")
         env_config.SIMULATOR.AGENT_0.SENSORS = agent_sensors
diff --git habitat_baselines/common/environments.py habitat_baselines/common/environments.py
index e542ae6..189c7d4 100644
--- habitat_baselines/common/environments.py
+++ habitat_baselines/common/environments.py
@@ -60,9 +60,7 @@ class NavRLEnv(habitat.RLEnv):
     def _distance_target(self):
         current_position = self._env.sim.get_agent_state().position.tolist()
         target_position = self._env.current_episode.goals[0].position
-        distance = self._env.sim.geodesic_distance(
-            current_position, target_position
-        )
+        distance = self._env.sim.geodesic_distance(current_position, target_position)
         return distance
 
     def _episode_success(self):
@@ -81,3 +79,164 @@ class NavRLEnv(habitat.RLEnv):
 
     def get_info(self, observations):
         return self.habitat_env.get_metrics()
+
+
+@baseline_registry.register_env(name="PoseRLEnv")
+class PoseRLEnv(habitat.RLEnv):
+    def __init__(self, config_env, config_baseline, dataset):
+        self._config_env = config_env.TASK
+        self._config_baseline = config_baseline
+        self._previous_action = None
+        self._episode_distance_covered = None
+        super().__init__(config_env, dataset)
+
+    def reset(self):
+        self._previous_action = None
+
+        observations = super().reset()
+
+        return observations
+
+    def step(self, action):
+        self._previous_action = action
+        obs, reward, done, info = super().step(action)
+        if not done:
+            # Optimization: do not return these keys after the 1st step.
+            for key in [
+                "pose_estimation_rgb",
+                "pose_estimation_depth",
+                "pose_estimation_reg",
+                "pose_estimation_mask",
+            ]:
+                _ = obs.pop(key, None)
+
+        return obs, reward, done, info
+
+    def get_reward_range(self):
+        return (-1.0, +1.0)
+
+    def get_reward(self, observations):
+        # All rewards for this task are computed external to the environment.
+        reward = 0.0
+        return reward
+
+    def _episode_success(self):
+        return False
+
+    def get_done(self, observations):
+        done = False
+        if self._env.episode_over or self._episode_success():
+            done = True
+        return done
+
+    def get_info(self, observations):
+        metrics = self.habitat_env.get_metrics()
+        environment_statistics = {
+            "episode_id": self.habitat_env.current_episode.episode_id,
+            "scene_id": self.habitat_env.current_episode.scene_id,
+        }
+        metrics["environment_statistics"] = environment_statistics
+        return metrics
+
+
+@baseline_registry.register_env(name="ExpNavRLEnv")
+class ExpNavRLEnv(habitat.RLEnv):
+    def __init__(self, config_env, config_baseline, dataset):
+        self._config_env = config_env.TASK
+        self._config_baseline = config_baseline
+        self._previous_target_distance = None
+        self._previous_action = None
+        self._episode_distance_covered = None
+        self.T_exp = config_env.ENVIRONMENT.T_EXP
+        self.T_nav = config_env.ENVIRONMENT.T_NAV
+        assert self.T_exp + self.T_nav == config_env.ENVIRONMENT.MAX_EPISODE_STEPS
+        super().__init__(config_env, dataset)
+
+    def reset(self):
+        self._previous_action = None
+
+        observations = super().reset()
+
+        self._previous_target_distance = self.habitat_env.current_episode.info[
+            "geodesic_distance"
+        ]
+        return observations
+
+    def step(self, action):
+        self._previous_action = action
+
+        observations, reward, done, info = super().step(action)
+        if self._env._elapsed_steps == self.T_exp:
+            observations = self._respawn_agent()
+            info["finished_exploration"] = True
+        else:
+            info["finished_exploration"] = False
+
+        return observations, reward, done, info
+
+    def _respawn_agent(self):
+        position = self.habitat_env.current_episode.start_nav_position
+        rotation = self.habitat_env.current_episode.start_nav_rotation
+        observations = self.habitat_env._sim.get_observations_at(
+            position, rotation, keep_agent_at_new_pose=True
+        )
+
+        observations.update(
+            self.habitat_env.task.sensor_suite.get_observations(
+                observations=observations, episode=self.current_episode
+            )
+        )
+
+        return observations
+
+    def get_reward_range(self):
+        return (
+            self._config_baseline.SLACK_REWARD - 1.0,
+            self._config_baseline.SUCCESS_REWARD + 1.0,
+        )
+
+    def get_reward(self, observations):
+        if self._env._elapsed_steps >= self.T_exp:
+            reward = self._config_baseline.SLACK_REWARD
+
+            current_target_distance = self._distance_target()
+            reward += self._previous_target_distance - current_target_distance
+            self._previous_target_distance = current_target_distance
+
+            if self._episode_success():
+                reward += self._config_baseline.SUCCESS_REWARD
+        else:
+            return 0.0
+
+        return reward
+
+    def _distance_target(self):
+        current_position = self._env.sim.get_agent_state().position.tolist()
+        target_position = self._env.current_episode.goals[0].position
+        distance = self._env.sim.geodesic_distance(current_position, target_position)
+        return distance
+
+    def _episode_success(self):
+        if (
+            self._previous_action == SimulatorActions.STOP
+            and self._distance_target() < self._config_env.SUCCESS_DISTANCE
+        ):
+            return True
+        return False
+
+    def get_done(self, observations):
+        done = False
+        if self._env._elapsed_steps >= self.T_exp and (
+            self._env.episode_over or self._episode_success()
+        ):
+            done = True
+        return done
+
+    def get_info(self, observations):
+        metrics = self.habitat_env.get_metrics()
+        environment_statistics = {
+            "episode_id": self.habitat_env.current_episode.episode_id,
+            "scene_id": self.habitat_env.current_episode.scene_id,
+        }
+        metrics["environment_statistics"] = environment_statistics
+        return metrics
diff --git habitat_baselines/common/rollout_storage.py habitat_baselines/common/rollout_storage.py
index 908e50b..b3d9836 100644
--- habitat_baselines/common/rollout_storage.py
+++ habitat_baselines/common/rollout_storage.py
@@ -26,9 +26,7 @@ class RolloutStorage:
 
         for sensor in observation_space.spaces:
             self.observations[sensor] = torch.zeros(
-                num_steps + 1,
-                num_envs,
-                *observation_space.spaces[sensor].shape,
+                num_steps + 1, num_envs, *observation_space.spaces[sensor].shape,
             )
 
         self.recurrent_hidden_states = torch.zeros(
@@ -77,12 +75,8 @@ class RolloutStorage:
         masks,
     ):
         for sensor in observations:
-            self.observations[sensor][self.step + 1].copy_(
-                observations[sensor]
-            )
-        self.recurrent_hidden_states[self.step + 1].copy_(
-            recurrent_hidden_states
-        )
+            self.observations[sensor][self.step + 1].copy_(observations[sensor])
+        self.recurrent_hidden_states[self.step + 1].copy_(recurrent_hidden_states)
         self.actions[self.step].copy_(actions)
         self.action_log_probs[self.step].copy_(action_log_probs)
         self.value_preds[self.step].copy_(value_preds)
@@ -154,9 +148,7 @@ class RolloutStorage:
                 value_preds_batch.append(self.value_preds[:-1, ind])
                 return_batch.append(self.returns[:-1, ind])
                 masks_batch.append(self.masks[:-1, ind])
-                old_action_log_probs_batch.append(
-                    self.action_log_probs[:, ind]
-                )
+                old_action_log_probs_batch.append(self.action_log_probs[:, ind])
 
                 adv_targ.append(advantages[:, ind])
 
@@ -164,17 +156,13 @@ class RolloutStorage:
 
             # These are all tensors of size (T, N, -1)
             for sensor in observations_batch:
-                observations_batch[sensor] = torch.stack(
-                    observations_batch[sensor], 1
-                )
+                observations_batch[sensor] = torch.stack(observations_batch[sensor], 1)
 
             actions_batch = torch.stack(actions_batch, 1)
             value_preds_batch = torch.stack(value_preds_batch, 1)
             return_batch = torch.stack(return_batch, 1)
             masks_batch = torch.stack(masks_batch, 1)
-            old_action_log_probs_batch = torch.stack(
-                old_action_log_probs_batch, 1
-            )
+            old_action_log_probs_batch = torch.stack(old_action_log_probs_batch, 1)
             adv_targ = torch.stack(adv_targ, 1)
 
             # States is just a (N, -1) tensor
diff --git habitat_baselines/common/tensorboard_utils.py habitat_baselines/common/tensorboard_utils.py
index ec5799d..2c320a9 100644
--- habitat_baselines/common/tensorboard_utils.py
+++ habitat_baselines/common/tensorboard_utils.py
@@ -53,9 +53,7 @@ class TensorboardWriter(SummaryWriter):
             None.
         """
         # initial shape of np.ndarray list: N * (H, W, 3)
-        frame_tensors = [
-            torch.from_numpy(np_arr).unsqueeze(0) for np_arr in images
-        ]
+        frame_tensors = [torch.from_numpy(np_arr).unsqueeze(0) for np_arr in images]
         video_tensor = torch.cat(tuple(frame_tensors))
         video_tensor = video_tensor.permute(0, 3, 1, 2).unsqueeze(0)
         # final shape of video tensor: (1, n, 3, H, W)
diff --git habitat_baselines/common/utils.py habitat_baselines/common/utils.py
index 54fb6a3..6c073a0 100644
--- habitat_baselines/common/utils.py
+++ habitat_baselines/common/utils.py
@@ -103,9 +103,7 @@ def batch_obs(observations: List[Dict]) -> Dict:
             batch[sensor].append(obs[sensor])
 
     for sensor in batch:
-        batch[sensor] = torch.tensor(
-            np.array(batch[sensor]), dtype=torch.float
-        )
+        batch[sensor] = torch.tensor(np.array(batch[sensor]), dtype=torch.float)
     return batch
 
 
@@ -124,9 +122,7 @@ def poll_checkpoint_folder(
         else return None.
     """
     assert os.path.isdir(checkpoint_folder), "invalid checkpoint folder path"
-    models_paths = list(
-        filter(os.path.isfile, glob.glob(checkpoint_folder + "/*"))
-    )
+    models_paths = list(filter(os.path.isfile, glob.glob(checkpoint_folder + "/*")))
     models_paths.sort(key=os.path.getmtime)
     ind = previous_ckpt_ind + 1
     if ind < len(models_paths):
diff --git habitat_baselines/config/default.py habitat_baselines/config/default.py
index 1de2f7c..8150a87 100644
--- habitat_baselines/config/default.py
+++ habitat_baselines/config/default.py
@@ -71,17 +71,13 @@ _C.TRAINER.RL.PPO.eval_ckpt_path_or_dir = "data/checkpoints"
 # ORBSLAM2 BASELINE
 # -----------------------------------------------------------------------------
 _C.TRAINER.ORBSLAM2 = CN()
-_C.TRAINER.ORBSLAM2.SLAM_VOCAB_PATH = (
-    "habitat_baselines/slambased/data/ORBvoc.txt"
-)
+_C.TRAINER.ORBSLAM2.SLAM_VOCAB_PATH = "habitat_baselines/slambased/data/ORBvoc.txt"
 _C.TRAINER.ORBSLAM2.SLAM_SETTINGS_PATH = (
     "habitat_baselines/slambased/data/mp3d3_small1k.yaml"
 )
 _C.TRAINER.ORBSLAM2.MAP_CELL_SIZE = 0.1
 _C.TRAINER.ORBSLAM2.MAP_SIZE = 40
-_C.TRAINER.ORBSLAM2.CAMERA_HEIGHT = get_task_config().SIMULATOR.DEPTH_SENSOR.POSITION[
-    1
-]
+_C.TRAINER.ORBSLAM2.CAMERA_HEIGHT = get_task_config().SIMULATOR.DEPTH_SENSOR.POSITION[1]
 _C.TRAINER.ORBSLAM2.BETA = 100
 _C.TRAINER.ORBSLAM2.H_OBSTACLE_MIN = 0.3 * _C.TRAINER.ORBSLAM2.CAMERA_HEIGHT
 _C.TRAINER.ORBSLAM2.H_OBSTACLE_MAX = 1.0 * _C.TRAINER.ORBSLAM2.CAMERA_HEIGHT
@@ -97,14 +93,11 @@ _C.TRAINER.ORBSLAM2.NEXT_WAYPOINT_TH = 0.5
 _C.TRAINER.ORBSLAM2.NUM_ACTIONS = 3
 _C.TRAINER.ORBSLAM2.DIST_TO_STOP = 0.05
 _C.TRAINER.ORBSLAM2.PLANNER_MAX_STEPS = 500
-_C.TRAINER.ORBSLAM2.DEPTH_DENORM = (
-    get_task_config().SIMULATOR.DEPTH_SENSOR.MAX_DEPTH
-)
+_C.TRAINER.ORBSLAM2.DEPTH_DENORM = get_task_config().SIMULATOR.DEPTH_SENSOR.MAX_DEPTH
 
 
 def get_config(
-    config_paths: Optional[Union[List[str], str]] = None,
-    opts: Optional[list] = None,
+    config_paths: Optional[Union[List[str], str]] = None, opts: Optional[list] = None,
 ) -> CN:
     r"""Create a unified config with default values overwritten by values from
     `config_paths` and overwritten by options from `opts`.
diff --git habitat_baselines/config/default_exp_nav.py habitat_baselines/config/default_exp_nav.py
new file mode 100644
index 0000000..95a5d10
--- /dev/null
+++ habitat_baselines/config/default_exp_nav.py
@@ -0,0 +1,128 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List, Optional, Union
+
+import numpy as np
+
+from habitat import get_config_exp_nav as get_task_config
+from habitat.config import Config as CN
+
+DEFAULT_CONFIG_DIR = "configs/"
+CONFIG_FILE_SEPARATOR = ","
+# -----------------------------------------------------------------------------
+# Config definition
+# -----------------------------------------------------------------------------
+_C = CN()
+_C.BASE_TASK_CONFIG_PATH = "configs/tasks/pointnav.yaml"
+_C.TASK_CONFIG = CN()  # task_config will be stored as a config node
+_C.CMD_TRAILING_OPTS = ""  # store command line options"
+# -----------------------------------------------------------------------------
+# TRAINER ALGORITHMS
+# -----------------------------------------------------------------------------
+_C.TRAINER = CN()
+_C.TRAINER.TRAINER_NAME = "ppo"
+# -----------------------------------------------------------------------------
+# REINFORCEMENT LEARNING (RL)
+# -----------------------------------------------------------------------------
+_C.TRAINER.RL = CN()
+_C.TRAINER.RL.SUCCESS_REWARD = 10.0
+_C.TRAINER.RL.SLACK_REWARD = -0.01
+# -----------------------------------------------------------------------------
+# PROXIMAL POLICY OPTIMIZATION (PPO)
+# -----------------------------------------------------------------------------
+# TODO move general options out of PPO
+_C.TRAINER.RL.PPO = CN()
+_C.TRAINER.RL.PPO.clip_param = 0.2
+_C.TRAINER.RL.PPO.ppo_epoch = 4
+_C.TRAINER.RL.PPO.num_mini_batch = 16
+_C.TRAINER.RL.PPO.value_loss_coef = 0.5
+_C.TRAINER.RL.PPO.entropy_coef = 0.01
+_C.TRAINER.RL.PPO.lr = 7e-4
+_C.TRAINER.RL.PPO.eps = 1e-5
+_C.TRAINER.RL.PPO.max_grad_norm = 0.5
+_C.TRAINER.RL.PPO.num_steps = 5
+_C.TRAINER.RL.PPO.hidden_size = 512
+_C.TRAINER.RL.PPO.num_processes = 16
+_C.TRAINER.RL.PPO.use_gae = True
+_C.TRAINER.RL.PPO.use_linear_lr_decay = False
+_C.TRAINER.RL.PPO.use_linear_clip_decay = False
+_C.TRAINER.RL.PPO.gamma = 0.99
+_C.TRAINER.RL.PPO.tau = 0.95
+_C.TRAINER.RL.PPO.log_file = "train.log"
+_C.TRAINER.RL.PPO.reward_window_size = 50
+_C.TRAINER.RL.PPO.log_interval = 50
+_C.TRAINER.RL.PPO.checkpoint_interval = 50
+_C.TRAINER.RL.PPO.checkpoint_folder = "data/checkpoints"
+_C.TRAINER.RL.PPO.sim_gpu_id = 0
+_C.TRAINER.RL.PPO.pth_gpu_id = 0
+_C.TRAINER.RL.PPO.num_updates = 10000
+_C.TRAINER.RL.PPO.sensors = "RGB_SENSOR,DEPTH_SENSOR"
+_C.TRAINER.RL.PPO.task_config = "configs/tasks/pointnav.yaml"
+_C.TRAINER.RL.PPO.tensorboard_dir = "tb"
+_C.TRAINER.RL.PPO.count_test_episodes = 2
+_C.TRAINER.RL.PPO.video_option = "disk,tensorboard"
+_C.TRAINER.RL.PPO.video_dir = "video_dir"
+_C.TRAINER.RL.PPO.eval_ckpt_path_or_dir = "data/checkpoints"
+# -----------------------------------------------------------------------------
+# ORBSLAM2 BASELINE
+# -----------------------------------------------------------------------------
+_C.TRAINER.ORBSLAM2 = CN()
+_C.TRAINER.ORBSLAM2.SLAM_VOCAB_PATH = "habitat_baselines/slambased/data/ORBvoc.txt"
+_C.TRAINER.ORBSLAM2.SLAM_SETTINGS_PATH = (
+    "habitat_baselines/slambased/data/mp3d3_small1k.yaml"
+)
+_C.TRAINER.ORBSLAM2.MAP_CELL_SIZE = 0.1
+_C.TRAINER.ORBSLAM2.MAP_SIZE = 40
+_C.TRAINER.ORBSLAM2.CAMERA_HEIGHT = get_task_config().SIMULATOR.DEPTH_SENSOR.POSITION[1]
+_C.TRAINER.ORBSLAM2.BETA = 100
+_C.TRAINER.ORBSLAM2.H_OBSTACLE_MIN = 0.3 * _C.TRAINER.ORBSLAM2.CAMERA_HEIGHT
+_C.TRAINER.ORBSLAM2.H_OBSTACLE_MAX = 1.0 * _C.TRAINER.ORBSLAM2.CAMERA_HEIGHT
+_C.TRAINER.ORBSLAM2.D_OBSTACLE_MIN = 0.1
+_C.TRAINER.ORBSLAM2.D_OBSTACLE_MAX = 4.0
+_C.TRAINER.ORBSLAM2.PREPROCESS_MAP = True
+_C.TRAINER.ORBSLAM2.MIN_PTS_IN_OBSTACLE = (
+    get_task_config().SIMULATOR.DEPTH_SENSOR.WIDTH / 2.0
+)
+_C.TRAINER.ORBSLAM2.ANGLE_TH = float(np.deg2rad(15))
+_C.TRAINER.ORBSLAM2.DIST_REACHED_TH = 0.15
+_C.TRAINER.ORBSLAM2.NEXT_WAYPOINT_TH = 0.5
+_C.TRAINER.ORBSLAM2.NUM_ACTIONS = 3
+_C.TRAINER.ORBSLAM2.DIST_TO_STOP = 0.05
+_C.TRAINER.ORBSLAM2.PLANNER_MAX_STEPS = 500
+_C.TRAINER.ORBSLAM2.DEPTH_DENORM = get_task_config().SIMULATOR.DEPTH_SENSOR.MAX_DEPTH
+
+
+def get_config_exp_nav(
+    config_paths: Optional[Union[List[str], str]] = None, opts: Optional[list] = None,
+) -> CN:
+    r"""Create a unified config with default values overwritten by values from
+    `config_paths` and overwritten by options from `opts`.
+    Args:
+        config_paths: List of config paths or string that contains comma
+        separated list of config paths.
+        opts: Config options (keys, values) in a list (e.g., passed from
+        command line into the config. For example, `opts = ['FOO.BAR',
+        0.5]`. Argument can be used for parameter sweeping or quick tests.
+    """
+    config = _C.clone()
+    if config_paths:
+        if isinstance(config_paths, str):
+            if CONFIG_FILE_SEPARATOR in config_paths:
+                config_paths = config_paths.split(CONFIG_FILE_SEPARATOR)
+            else:
+                config_paths = [config_paths]
+
+        for config_path in config_paths:
+            config.merge_from_file(config_path)
+
+    config.TASK_CONFIG = get_task_config(config.BASE_TASK_CONFIG_PATH)
+    config.CMD_TRAILING_OPTS = opts
+    if opts:
+        config.merge_from_list(opts)
+
+    config.freeze()
+    return config
diff --git habitat_baselines/config/default_pose.py habitat_baselines/config/default_pose.py
new file mode 100644
index 0000000..b4693c6
--- /dev/null
+++ habitat_baselines/config/default_pose.py
@@ -0,0 +1,128 @@
+#!/usr/bin/env python3
+
+# Copyright (c) Facebook, Inc. and its affiliates.
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import List, Optional, Union
+
+import numpy as np
+
+from habitat import get_config_pose as get_task_config
+from habitat.config import Config as CN
+
+DEFAULT_CONFIG_DIR = "configs/"
+CONFIG_FILE_SEPARATOR = ","
+# -----------------------------------------------------------------------------
+# Config definition
+# -----------------------------------------------------------------------------
+_C = CN()
+_C.BASE_TASK_CONFIG_PATH = "configs/tasks/pointnav.yaml"
+_C.TASK_CONFIG = CN()  # task_config will be stored as a config node
+_C.CMD_TRAILING_OPTS = ""  # store command line options"
+# -----------------------------------------------------------------------------
+# TRAINER ALGORITHMS
+# -----------------------------------------------------------------------------
+_C.TRAINER = CN()
+_C.TRAINER.TRAINER_NAME = "ppo"
+# -----------------------------------------------------------------------------
+# REINFORCEMENT LEARNING (RL)
+# -----------------------------------------------------------------------------
+_C.TRAINER.RL = CN()
+_C.TRAINER.RL.SUCCESS_REWARD = 10.0
+_C.TRAINER.RL.SLACK_REWARD = -0.01
+# -----------------------------------------------------------------------------
+# PROXIMAL POLICY OPTIMIZATION (PPO)
+# -----------------------------------------------------------------------------
+# TODO move general options out of PPO
+_C.TRAINER.RL.PPO = CN()
+_C.TRAINER.RL.PPO.clip_param = 0.2
+_C.TRAINER.RL.PPO.ppo_epoch = 4
+_C.TRAINER.RL.PPO.num_mini_batch = 16
+_C.TRAINER.RL.PPO.value_loss_coef = 0.5
+_C.TRAINER.RL.PPO.entropy_coef = 0.01
+_C.TRAINER.RL.PPO.lr = 7e-4
+_C.TRAINER.RL.PPO.eps = 1e-5
+_C.TRAINER.RL.PPO.max_grad_norm = 0.5
+_C.TRAINER.RL.PPO.num_steps = 5
+_C.TRAINER.RL.PPO.hidden_size = 512
+_C.TRAINER.RL.PPO.num_processes = 16
+_C.TRAINER.RL.PPO.use_gae = True
+_C.TRAINER.RL.PPO.use_linear_lr_decay = False
+_C.TRAINER.RL.PPO.use_linear_clip_decay = False
+_C.TRAINER.RL.PPO.gamma = 0.99
+_C.TRAINER.RL.PPO.tau = 0.95
+_C.TRAINER.RL.PPO.log_file = "train.log"
+_C.TRAINER.RL.PPO.reward_window_size = 50
+_C.TRAINER.RL.PPO.log_interval = 50
+_C.TRAINER.RL.PPO.checkpoint_interval = 50
+_C.TRAINER.RL.PPO.checkpoint_folder = "data/checkpoints"
+_C.TRAINER.RL.PPO.sim_gpu_id = 0
+_C.TRAINER.RL.PPO.pth_gpu_id = 0
+_C.TRAINER.RL.PPO.num_updates = 10000
+_C.TRAINER.RL.PPO.sensors = "RGB_SENSOR,DEPTH_SENSOR"
+_C.TRAINER.RL.PPO.task_config = "configs/tasks/pointnav.yaml"
+_C.TRAINER.RL.PPO.tensorboard_dir = "tb"
+_C.TRAINER.RL.PPO.count_test_episodes = 2
+_C.TRAINER.RL.PPO.video_option = "disk,tensorboard"
+_C.TRAINER.RL.PPO.video_dir = "video_dir"
+_C.TRAINER.RL.PPO.eval_ckpt_path_or_dir = "data/checkpoints"
+# -----------------------------------------------------------------------------
+# ORBSLAM2 BASELINE
+# -----------------------------------------------------------------------------
+_C.TRAINER.ORBSLAM2 = CN()
+_C.TRAINER.ORBSLAM2.SLAM_VOCAB_PATH = "habitat_baselines/slambased/data/ORBvoc.txt"
+_C.TRAINER.ORBSLAM2.SLAM_SETTINGS_PATH = (
+    "habitat_baselines/slambased/data/mp3d3_small1k.yaml"
+)
+_C.TRAINER.ORBSLAM2.MAP_CELL_SIZE = 0.1
+_C.TRAINER.ORBSLAM2.MAP_SIZE = 40
+_C.TRAINER.ORBSLAM2.CAMERA_HEIGHT = get_task_config().SIMULATOR.DEPTH_SENSOR.POSITION[1]
+_C.TRAINER.ORBSLAM2.BETA = 100
+_C.TRAINER.ORBSLAM2.H_OBSTACLE_MIN = 0.3 * _C.TRAINER.ORBSLAM2.CAMERA_HEIGHT
+_C.TRAINER.ORBSLAM2.H_OBSTACLE_MAX = 1.0 * _C.TRAINER.ORBSLAM2.CAMERA_HEIGHT
+_C.TRAINER.ORBSLAM2.D_OBSTACLE_MIN = 0.1
+_C.TRAINER.ORBSLAM2.D_OBSTACLE_MAX = 4.0
+_C.TRAINER.ORBSLAM2.PREPROCESS_MAP = True
+_C.TRAINER.ORBSLAM2.MIN_PTS_IN_OBSTACLE = (
+    get_task_config().SIMULATOR.DEPTH_SENSOR.WIDTH / 2.0
+)
+_C.TRAINER.ORBSLAM2.ANGLE_TH = float(np.deg2rad(15))
+_C.TRAINER.ORBSLAM2.DIST_REACHED_TH = 0.15
+_C.TRAINER.ORBSLAM2.NEXT_WAYPOINT_TH = 0.5
+_C.TRAINER.ORBSLAM2.NUM_ACTIONS = 3
+_C.TRAINER.ORBSLAM2.DIST_TO_STOP = 0.05
+_C.TRAINER.ORBSLAM2.PLANNER_MAX_STEPS = 500
+_C.TRAINER.ORBSLAM2.DEPTH_DENORM = get_task_config().SIMULATOR.DEPTH_SENSOR.MAX_DEPTH
+
+
+def get_config_pose(
+    config_paths: Optional[Union[List[str], str]] = None, opts: Optional[list] = None,
+) -> CN:
+    r"""Create a unified config with default values overwritten by values from
+    `config_paths` and overwritten by options from `opts`.
+    Args:
+        config_paths: List of config paths or string that contains comma
+        separated list of config paths.
+        opts: Config options (keys, values) in a list (e.g., passed from
+        command line into the config. For example, `opts = ['FOO.BAR',
+        0.5]`. Argument can be used for parameter sweeping or quick tests.
+    """
+    config = _C.clone()
+    if config_paths:
+        if isinstance(config_paths, str):
+            if CONFIG_FILE_SEPARATOR in config_paths:
+                config_paths = config_paths.split(CONFIG_FILE_SEPARATOR)
+            else:
+                config_paths = [config_paths]
+
+        for config_path in config_paths:
+            config.merge_from_file(config_path)
+
+    config.TASK_CONFIG = get_task_config(config.BASE_TASK_CONFIG_PATH)
+    config.CMD_TRAILING_OPTS = opts
+    if opts:
+        config.merge_from_list(opts)
+
+    config.freeze()
+    return config
diff --git habitat_baselines/config/pointnav/ppo.yaml habitat_baselines/config/pointnav/ppo.yaml
index 305a2ee..eab4784 100644
--- habitat_baselines/config/pointnav/ppo.yaml
+++ habitat_baselines/config/pointnav/ppo.yaml
@@ -1,4 +1,4 @@
-BASE_TASK_CONFIG_PATH: "configs/tasks/pointnav.yaml"
+BASE_TASK_CONFIG_PATH: "configs/tasks/pointnav_gibson.yaml"
 TRAINER:
   TRAINER_NAME: "ppo"
   RL:
@@ -11,7 +11,7 @@ TRAINER:
       sim_gpu_id: 0
       checkpoint_interval: 50
       checkpoint_folder: "data/checkpoints"
-      task_config: "configs/tasks/pointnav.yaml"
+      task_config: "configs/tasks/pointnav_gibson.yaml"
       num_updates: 10000
       # eval specific:
       count_test_episodes: 2
diff --git habitat_baselines/rl/ppo/policy.py habitat_baselines/rl/ppo/policy.py
index 0be21e5..5e34add 100644
--- habitat_baselines/rl/ppo/policy.py
+++ habitat_baselines/rl/ppo/policy.py
@@ -13,11 +13,7 @@ from habitat_baselines.common.utils import CategoricalNet, Flatten
 
 class Policy(nn.Module):
     def __init__(
-        self,
-        observation_space,
-        action_space,
-        goal_sensor_uuid,
-        hidden_size=512,
+        self, observation_space, action_space, goal_sensor_uuid, hidden_size=512,
     ):
         super().__init__()
         self.dim_actions = action_space.n
@@ -74,9 +70,7 @@ class Net(nn.Module):
     def __init__(self, observation_space, hidden_size, goal_sensor_uuid):
         super().__init__()
         self.goal_sensor_uuid = goal_sensor_uuid
-        self._n_input_goal = observation_space.spaces[
-            self.goal_sensor_uuid
-        ].shape[0]
+        self._n_input_goal = observation_space.spaces[self.goal_sensor_uuid].shape[0]
         self._hidden_size = hidden_size
 
         self.cnn = self._init_perception_model(observation_space)
@@ -84,9 +78,7 @@ class Net(nn.Module):
         if self.is_blind:
             self.rnn = nn.GRU(self._n_input_goal, self._hidden_size)
         else:
-            self.rnn = nn.GRU(
-                self.output_size + self._n_input_goal, self._hidden_size
-            )
+            self.rnn = nn.GRU(self.output_size + self._n_input_goal, self._hidden_size)
 
         self.critic_linear = nn.Linear(self._hidden_size, 1)
 
@@ -159,9 +151,7 @@ class Net(nn.Module):
                 nn.ReLU(),
             )
 
-    def _conv_output_dim(
-        self, dimension, padding, dilation, kernel_size, stride
-    ):
+    def _conv_output_dim(self, dimension, padding, dilation, kernel_size, stride):
         r"""Calculates the output height and width based on the input
         height and width to the convolution layer.
 
@@ -195,9 +185,7 @@ class Net(nn.Module):
     def layer_init(self):
         for layer in self.cnn:
             if isinstance(layer, (nn.Conv2d, nn.Linear)):
-                nn.init.orthogonal_(
-                    layer.weight, nn.init.calculate_gain("relu")
-                )
+                nn.init.orthogonal_(layer.weight, nn.init.calculate_gain("relu"))
                 nn.init.constant_(layer.bias, val=0)
 
         for name, param in self.rnn.named_parameters():
@@ -227,9 +215,7 @@ class Net(nn.Module):
 
             # steps in sequence which have zero for any agent. Assume t=0 has
             # a zero in it.
-            has_zeros = (
-                (masks[1:] == 0.0).any(dim=-1).nonzero().squeeze().cpu()
-            )
+            has_zeros = (masks[1:] == 0.0).any(dim=-1).nonzero().squeeze().cpu()
 
             # +1 to correct the masks[1:]
             if has_zeros.dim() == 0:
diff --git habitat_baselines/rl/ppo/ppo.py habitat_baselines/rl/ppo/ppo.py
index 78f113f..7b0247e 100644
--- habitat_baselines/rl/ppo/ppo.py
+++ habitat_baselines/rl/ppo/ppo.py
@@ -47,9 +47,7 @@ class PPO(nn.Module):
 
     def update(self, rollouts):
         advantages = rollouts.returns[:-1] - rollouts.value_preds[:-1]
-        advantages = (advantages - advantages.mean()) / (
-            advantages.std() + EPS_PPO
-        )
+        advantages = (advantages - advantages.mean()) / (advantages.std() + EPS_PPO)
 
         value_loss_epoch = 0
         action_loss_epoch = 0
@@ -85,14 +83,10 @@ class PPO(nn.Module):
                     actions_batch,
                 )
 
-                ratio = torch.exp(
-                    action_log_probs - old_action_log_probs_batch
-                )
+                ratio = torch.exp(action_log_probs - old_action_log_probs_batch)
                 surr1 = ratio * adv_targ
                 surr2 = (
-                    torch.clamp(
-                        ratio, 1.0 - self.clip_param, 1.0 + self.clip_param
-                    )
+                    torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param)
                     * adv_targ
                 )
                 action_loss = -torch.min(surr1, surr2).mean()
@@ -102,12 +96,9 @@ class PPO(nn.Module):
                         values - value_preds_batch
                     ).clamp(-self.clip_param, self.clip_param)
                     value_losses = (values - return_batch).pow(2)
-                    value_losses_clipped = (
-                        value_pred_clipped - return_batch
-                    ).pow(2)
+                    value_losses_clipped = (value_pred_clipped - return_batch).pow(2)
                     value_loss = (
-                        0.5
-                        * torch.max(value_losses, value_losses_clipped).mean()
+                        0.5 * torch.max(value_losses, value_losses_clipped).mean()
                     )
                 else:
                     value_loss = 0.5 * (return_batch - values).pow(2).mean()
diff --git habitat_baselines/rl/ppo/ppo_trainer.py habitat_baselines/rl/ppo/ppo_trainer.py
index 31d4b47..6597818 100644
--- habitat_baselines/rl/ppo/ppo_trainer.py
+++ habitat_baselines/rl/ppo/ppo_trainer.py
@@ -96,9 +96,7 @@ class PPOTrainer(BaseRLTrainer):
         }
         torch.save(
             checkpoint,
-            os.path.join(
-                self.config.TRAINER.RL.PPO.checkpoint_folder, file_name
-            ),
+            os.path.join(self.config.TRAINER.RL.PPO.checkpoint_folder, file_name),
         )
 
     def load_checkpoint(self, checkpoint_path: str, *args, **kwargs) -> Dict:
@@ -165,18 +163,13 @@ class PPOTrainer(BaseRLTrainer):
 
         with (
             get_tensorboard_writer(
-                log_dir=ppo_cfg.tensorboard_dir,
-                purge_step=count_steps,
-                flush_secs=30,
+                log_dir=ppo_cfg.tensorboard_dir, purge_step=count_steps, flush_secs=30,
             )
         ) as writer:
             for update in range(ppo_cfg.num_updates):
                 if ppo_cfg.use_linear_lr_decay:
                     update_linear_schedule(
-                        self.agent.optimizer,
-                        update,
-                        ppo_cfg.num_updates,
-                        ppo_cfg.lr,
+                        self.agent.optimizer, update, ppo_cfg.num_updates, ppo_cfg.lr,
                     )
 
                 if ppo_cfg.use_linear_clip_decay:
@@ -189,8 +182,7 @@ class PPOTrainer(BaseRLTrainer):
                     # sample actions
                     with torch.no_grad():
                         step_observation = {
-                            k: v[step]
-                            for k, v in rollouts.observations.items()
+                            k: v[step] for k, v in rollouts.observations.items()
                         }
 
                         (
@@ -220,8 +212,7 @@ class PPOTrainer(BaseRLTrainer):
                     rewards = rewards.unsqueeze(1)
 
                     masks = torch.tensor(
-                        [[0.0] if done else [1.0] for done in dones],
-                        dtype=torch.float,
+                        [[0.0] if done else [1.0] for done in dones], dtype=torch.float,
                     )
 
                     current_episode_reward += rewards
@@ -260,23 +251,18 @@ class PPOTrainer(BaseRLTrainer):
                     next_value, ppo_cfg.use_gae, ppo_cfg.gamma, ppo_cfg.tau
                 )
 
-                value_loss, action_loss, dist_entropy = self.agent.update(
-                    rollouts
-                )
+                value_loss, action_loss, dist_entropy = self.agent.update(rollouts)
 
                 rollouts.after_update()
                 pth_time += time.time() - t_update_model
 
                 losses = [value_loss, action_loss]
                 stats = zip(
-                    ["count", "reward"],
-                    [window_episode_counts, window_episode_reward],
+                    ["count", "reward"], [window_episode_counts, window_episode_reward],
                 )
                 deltas = {
                     k: (
-                        (v[-1] - v[0]).sum().item()
-                        if len(v) > 1
-                        else v[0].sum().item()
+                        (v[-1] - v[0]).sum().item() if len(v) > 1 else v[0].sum().item()
                     )
                     for k, v in stats
                 }
@@ -302,9 +288,7 @@ class PPOTrainer(BaseRLTrainer):
 
                     logger.info(
                         "update: {}\tenv-time: {:.3f}s\tpth-time: {:.3f}s\t"
-                        "frames: {}".format(
-                            update, env_time, pth_time, count_steps
-                        )
+                        "frames: {}".format(update, env_time, pth_time, count_steps)
                     )
 
                     window_rewards = (
@@ -377,10 +361,7 @@ class PPOTrainer(BaseRLTrainer):
                     )
 
     def _eval_checkpoint(
-        self,
-        checkpoint_path: str,
-        writer: TensorboardWriter,
-        cur_ckpt_idx: int = 0,
+        self, checkpoint_path: str, writer: TensorboardWriter, cur_ckpt_idx: int = 0,
     ) -> None:
         r"""
         Evaluates a single checkpoint
@@ -392,9 +373,7 @@ class PPOTrainer(BaseRLTrainer):
         Returns:
             None
         """
-        ckpt_dict = self.load_checkpoint(
-            checkpoint_path, map_location=self.device
-        )
+        ckpt_dict = self.load_checkpoint(checkpoint_path, map_location=self.device)
 
         ckpt_config = ckpt_dict["config"]
         config = self.config.clone()
@@ -430,27 +409,20 @@ class PPOTrainer(BaseRLTrainer):
         for sensor in batch:
             batch[sensor] = batch[sensor].to(self.device)
 
-        current_episode_reward = torch.zeros(
-            self.envs.num_envs, 1, device=self.device
-        )
+        current_episode_reward = torch.zeros(self.envs.num_envs, 1, device=self.device)
 
         test_recurrent_hidden_states = torch.zeros(
             ppo_cfg.num_processes, ppo_cfg.hidden_size, device=self.device
         )
-        not_done_masks = torch.zeros(
-            ppo_cfg.num_processes, 1, device=self.device
-        )
+        not_done_masks = torch.zeros(ppo_cfg.num_processes, 1, device=self.device)
         stats_episodes = dict()  # dict of dicts that stores stats per episode
 
-        rgb_frames = [
-            []
-        ] * ppo_cfg.num_processes  # type: List[List[np.ndarray]]
+        rgb_frames = [[]] * ppo_cfg.num_processes  # type: List[List[np.ndarray]]
         if self.video_option:
             os.makedirs(ppo_cfg.video_dir, exist_ok=True)
 
         while (
-            len(stats_episodes) < ppo_cfg.count_test_episodes
-            and self.envs.num_envs > 0
+            len(stats_episodes) < ppo_cfg.count_test_episodes and self.envs.num_envs > 0
         ):
             current_episodes = self.envs.current_episodes()
 
@@ -464,9 +436,7 @@ class PPOTrainer(BaseRLTrainer):
 
             outputs = self.envs.step([a[0].item() for a in actions])
 
-            observations, rewards, dones, infos = [
-                list(x) for x in zip(*outputs)
-            ]
+            observations, rewards, dones, infos = [list(x) for x in zip(*outputs)]
             batch = batch_obs(observations)
             for sensor in batch:
                 batch[sensor] = batch[sensor].to(self.device)
@@ -500,10 +470,7 @@ class PPOTrainer(BaseRLTrainer):
                     current_episode_reward[i] = 0
                     # use scene_id + episode_id as unique id for storing stats
                     stats_episodes[
-                        (
-                            current_episodes[i].scene_id,
-                            current_episodes[i].episode_id,
-                        )
+                        (current_episodes[i].scene_id, current_episodes[i].episode_id,)
                     ] = episode_stats
                     if self.video_option:
                         generate_video(
@@ -529,9 +496,7 @@ class PPOTrainer(BaseRLTrainer):
                     self.envs.pause_at(idx)
 
                 # indexing along the batch dimensions
-                test_recurrent_hidden_states = test_recurrent_hidden_states[
-                    state_index
-                ]
+                test_recurrent_hidden_states = test_recurrent_hidden_states[state_index]
                 not_done_masks = not_done_masks[state_index]
                 current_episode_reward = current_episode_reward[state_index]
 
@@ -552,24 +517,14 @@ class PPOTrainer(BaseRLTrainer):
         episode_spl_mean = aggregated_stats["spl"] / num_episodes
         episode_success_mean = aggregated_stats["success"] / num_episodes
 
-        logger.info(
-            "Average episode reward: {:.6f}".format(episode_reward_mean)
-        )
-        logger.info(
-            "Average episode success: {:.6f}".format(episode_success_mean)
-        )
+        logger.info("Average episode reward: {:.6f}".format(episode_reward_mean))
+        logger.info("Average episode success: {:.6f}".format(episode_success_mean))
         logger.info("Average episode SPL: {:.6f}".format(episode_spl_mean))
 
         writer.add_scalars(
-            "eval_reward",
-            {"average reward": episode_reward_mean},
-            cur_ckpt_idx,
-        )
-        writer.add_scalars(
-            "eval_SPL", {"average SPL": episode_spl_mean}, cur_ckpt_idx
+            "eval_reward", {"average reward": episode_reward_mean}, cur_ckpt_idx,
         )
+        writer.add_scalars("eval_SPL", {"average SPL": episode_spl_mean}, cur_ckpt_idx)
         writer.add_scalars(
-            "eval_success",
-            {"average success": episode_success_mean},
-            cur_ckpt_idx,
+            "eval_success", {"average success": episode_success_mean}, cur_ckpt_idx,
         )
diff --git habitat_baselines/slambased/data/mp3d3_small1k.yaml habitat_baselines/slambased/data/mp3d3_small1k.yaml
deleted file mode 100644
index c814c08..0000000
--- habitat_baselines/slambased/data/mp3d3_small1k.yaml
+++ /dev/null
@@ -1,69 +0,0 @@
-%YAML:1.0
-
-#--------------------------------------------------------------------------------------------
-# Camera Parameters. Adjust them!
-#--------------------------------------------------------------------------------------------
-
-# Camera calibration and distortion parameters (OpenCV) 
-#For resolution 256x256, FOV 90 deg
-Camera.fx: 128.0 
-Camera.fy: 128.0
-Camera.cx: 127.0
-Camera.cy: 127.0
-
-
-Camera.k1: 0.0
-Camera.k2: 0.0
-Camera.p1: 0.0
-Camera.p2: 0.0
-
-# Camera frames per second 
-Camera.fps: 30.0
-
-# IR projector baseline times fx (aprox.)
-Camera.bf: 50.0
-
-# Color order of the images (0: BGR, 1: RGB. It is ignored if images are grayscale)
-Camera.RGB: 1
-
-# Close/Far threshold. Baseline times.
-#ThDepth: 40.0
-ThDepth: 70.0
-
-# Deptmap values factor
-DepthMapFactor: 1.0
-
-#--------------------------------------------------------------------------------------------
-# ORB Parameters
-#--------------------------------------------------------------------------------------------
-
-# ORB Extractor: Number of features per image
-ORBextractor.nFeatures: 1000
-
-# ORB Extractor: Scale factor between levels in the scale pyramid 	
-ORBextractor.scaleFactor: 1.2
-
-# ORB Extractor: Number of levels in the scale pyramid	
-ORBextractor.nLevels: 8
-
-# ORB Extractor: Fast threshold
-# Image is divided in a grid. At each cell FAST are extracted imposing a minimum response.
-# Firstly we impose iniThFAST. If no corners are detected we impose a lower value minThFAST
-# You can lower these values if your images have low contrast			
-ORBextractor.iniThFAST: 5
-ORBextractor.minThFAST: 1
-
-#--------------------------------------------------------------------------------------------
-# Viewer Parameters
-#--------------------------------------------------------------------------------------------
-Viewer.KeyFrameSize: 0.1
-Viewer.KeyFrameLineWidth: 1
-Viewer.GraphLineWidth: 1
-Viewer.PointSize:2
-Viewer.CameraSize: 0.15
-Viewer.CameraLineWidth: 2
-Viewer.ViewpointX: 0
-Viewer.ViewpointY: -10
-Viewer.ViewpointZ: -0.1
-Viewer.ViewpointF: 2000
-
diff --git habitat_baselines/slambased/data/slam-based-agent.png habitat_baselines/slambased/data/slam-based-agent.png
deleted file mode 100644
index 3aafc63..0000000
Binary files habitat_baselines/slambased/data/slam-based-agent.png and /dev/null differ
diff --git habitat_baselines/slambased/mappers.py habitat_baselines/slambased/mappers.py
index c3823b0..4189bff 100644
--- habitat_baselines/slambased/mappers.py
+++ habitat_baselines/slambased/mappers.py
@@ -41,16 +41,12 @@ def pcl_to_obstacles(pts3d, map_size=40, cell_size=0.2, min_pts=10):
     """
     device = pts3d.device
     map_size_in_cells = get_map_size_in_cells(map_size, cell_size) - 1
-    init_map = torch.zeros(
-        (map_size_in_cells, map_size_in_cells), device=device
-    )
+    init_map = torch.zeros((map_size_in_cells, map_size_in_cells), device=device)
     if len(pts3d) <= 1:
         return init_map
     num_pts, dim = pts3d.size()
     pts2d = torch.cat([pts3d[:, 2:3], pts3d[:, 0:1]], dim=1)
-    data_idxs = torch.round(
-        project2d_pcl_into_worldmap(pts2d, map_size, cell_size)
-    )
+    data_idxs = torch.round(project2d_pcl_into_worldmap(pts2d, map_size, cell_size))
     if len(data_idxs) > min_pts:
         u, counts = np.unique(
             data_idxs.detach().cpu().numpy(), axis=0, return_counts=True
@@ -107,8 +103,7 @@ class DirectDepthMapper(nn.Module):
         survived_points = local_3d_pcl[idxs]
         if len(survived_points) < 20:
             map_size_in_cells = (
-                get_map_size_in_cells(self.map_size_meters, self.map_cell_size)
-                - 1
+                get_map_size_in_cells(self.map_size_meters, self.map_cell_size) - 1
             )
             init_map = torch.zeros(
                 (map_size_in_cells, map_size_in_cells), device=self.device
diff --git habitat_baselines/slambased/monodepth.py habitat_baselines/slambased/monodepth.py
index 182b4a5..a4d6190 100644
--- habitat_baselines/slambased/monodepth.py
+++ habitat_baselines/slambased/monodepth.py
@@ -55,12 +55,7 @@ model_urls = {
 def conv3x3(in_planes, out_planes, stride=1):
     "3x3 convolution with padding"
     return nn.Conv2d(
-        in_planes,
-        out_planes,
-        kernel_size=3,
-        stride=stride,
-        padding=1,
-        bias=False,
+        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False,
     )
 
 
@@ -140,9 +135,7 @@ class ResNet(nn.Module):
     def __init__(self, block, layers, num_classes=1000):
         self.inplanes = 64
         super(ResNet, self).__init__()
-        self.conv1 = nn.Conv2d(
-            3, 64, kernel_size=7, stride=2, padding=3, bias=False
-        )
+        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
         self.bn1 = nn.BatchNorm2d(64)
         self.relu = nn.ReLU(inplace=True)
         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
@@ -231,9 +224,7 @@ def resnet50(pretrained=False, **kwargs):
     model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)
     if pretrained:
         model.load_state_dict(
-            model_zoo.load_url(
-                model_urls["resnet50"], "pretrained_model/encoder"
-            )
+            model_zoo.load_url(model_urls["resnet50"], "pretrained_model/encoder")
         )
     return model
 
@@ -361,36 +352,28 @@ class D(nn.Module):
     def __init__(self, num_features=2048):
         super(D, self).__init__()
         self.conv = nn.Conv2d(
-            num_features,
-            num_features // 2,
-            kernel_size=1,
-            stride=1,
-            bias=False,
+            num_features, num_features // 2, kernel_size=1, stride=1, bias=False,
         )
         num_features = num_features // 2
         self.bn = nn.BatchNorm2d(num_features)
 
         self.up1 = _UpProjection(
-            num_input_features=num_features,
-            num_output_features=num_features // 2,
+            num_input_features=num_features, num_output_features=num_features // 2,
         )
         num_features = num_features // 2
 
         self.up2 = _UpProjection(
-            num_input_features=num_features,
-            num_output_features=num_features // 2,
+            num_input_features=num_features, num_output_features=num_features // 2,
         )
         num_features = num_features // 2
 
         self.up3 = _UpProjection(
-            num_input_features=num_features,
-            num_output_features=num_features // 2,
+            num_input_features=num_features, num_output_features=num_features // 2,
         )
         num_features = num_features // 2
 
         self.up4 = _UpProjection(
-            num_input_features=num_features,
-            num_output_features=num_features // 2,
+            num_input_features=num_features, num_output_features=num_features // 2,
         )
         num_features = num_features // 2
 
@@ -426,12 +409,7 @@ class MFF(nn.Module):
         )
 
         self.conv = nn.Conv2d(
-            num_features,
-            num_features,
-            kernel_size=5,
-            stride=1,
-            padding=2,
-            bias=False,
+            num_features, num_features, kernel_size=5, stride=1, padding=2, bias=False,
         )
         self.bn = nn.BatchNorm2d(num_features)
 
@@ -454,22 +432,12 @@ class R(nn.Module):
 
         num_features = 64 + block_channel[3] // 32
         self.conv0 = nn.Conv2d(
-            num_features,
-            num_features,
-            kernel_size=5,
-            stride=1,
-            padding=2,
-            bias=False,
+            num_features, num_features, kernel_size=5, stride=1, padding=2, bias=False,
         )
         self.bn0 = nn.BatchNorm2d(num_features)
 
         self.conv1 = nn.Conv2d(
-            num_features,
-            num_features,
-            kernel_size=5,
-            stride=1,
-            padding=2,
-            bias=False,
+            num_features, num_features, kernel_size=5, stride=1, padding=2, bias=False,
         )
         self.bn1 = nn.BatchNorm2d(num_features)
 
@@ -561,9 +529,7 @@ class ToTensor(object):
             return img.float().div(255)
 
         if accimage is not None and isinstance(pic, accimage.Image):
-            nppic = np.zeros(
-                [pic.channels, pic.height, pic.width], dtype=np.float32
-            )
+            nppic = np.zeros([pic.channels, pic.height, pic.width], dtype=np.float32)
             pic.copyto(nppic)
             return torch.from_numpy(nppic)
 
@@ -573,9 +539,7 @@ class ToTensor(object):
         elif pic.mode == "I;16":
             img = torch.from_numpy(np.array(pic, np.int16, copy=False))
         else:
-            img = torch.ByteTensor(
-                torch.ByteStorage.from_buffer(pic.tobytes())
-            )
+            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))
         # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK
         if pic.mode == "YCbCr":
             nchannel = 3
@@ -614,30 +578,22 @@ def define_model(is_resnet, is_densenet, is_senet):
     if is_resnet:
         original_model = resnet50(pretrained=False)
         Encoder = E_resnet(original_model)
-        model1 = model(
-            Encoder, num_features=2048, block_channel=[256, 512, 1024, 2048]
-        )
+        model1 = model(Encoder, num_features=2048, block_channel=[256, 512, 1024, 2048])
     if is_densenet:
         original_model = dendensenet161(pretrained=False)
         Encoder = E_densenet(original_model)
-        model1 = model(
-            Encoder, num_features=2208, block_channel=[192, 384, 1056, 2208]
-        )
+        model1 = model(Encoder, num_features=2208, block_channel=[192, 384, 1056, 2208])
     if is_senet:
         original_model = senet154(pretrained=False)
         Encoder = E_senet(original_model)
-        model1 = model(
-            Encoder, num_features=2048, block_channel=[256, 512, 1024, 2048]
-        )
+        model1 = model(Encoder, num_features=2048, block_channel=[256, 512, 1024, 2048])
 
     return model1
 
 
 class MonoDepthEstimator:
     def __init__(self, checkpoint="./pretrained_model/model_resnet"):
-        self.model = define_model(
-            is_resnet=True, is_densenet=False, is_senet=False
-        )
+        self.model = define_model(is_resnet=True, is_densenet=False, is_senet=False)
         self.model = torch.nn.DataParallel(self.model).cuda()
         cpt = torch.load(checkpoint)
         if "state_dict" in cpt.keys():
diff --git habitat_baselines/slambased/path_planners.py habitat_baselines/slambased/path_planners.py
index 52b4d46..a2c4acd 100644
--- habitat_baselines/slambased/path_planners.py
+++ habitat_baselines/slambased/path_planners.py
@@ -45,9 +45,9 @@ class SoftArgMin(nn.Module):
         if coords2d is None:
             coords2d = generate_2dgrid(x.size(2), x.size(3), False)
         coords2d_flat = coords2d.view(2, -1)
-        return (bx_sm.expand_as(coords2d_flat) * coords2d_flat).sum(
+        return (bx_sm.expand_as(coords2d_flat) * coords2d_flat).sum(dim=1) / bx_sm.sum(
             dim=1
-        ) / bx_sm.sum(dim=1)
+        )
 
 
 class HardArgMin(nn.Module):
@@ -90,9 +90,7 @@ class DifferentiableStarPlanner(nn.Module):
             init_neights_to_channels(3)
         )
         self.neights2channels.to(device)
-        self.preprocessNet = nn.Conv2d(
-            1, 1, kernel_size=(3, 3), padding=1, bias=False
-        )
+        self.preprocessNet = nn.Conv2d(1, 1, kernel_size=(3, 3), padding=1, bias=False)
         self.preprocessNet.weight.data = torch.from_numpy(
             np.array(
                 [
@@ -142,9 +140,9 @@ class DifferentiableStarPlanner(nn.Module):
         return obstacle_map
 
     def coords2grid(self, node_coords, h, w):
-        grid = node_coords.squeeze() - torch.FloatTensor(
-            (h / 2.0, w / 2.0)
-        ).to(self.device)
+        grid = node_coords.squeeze() - torch.FloatTensor((h / 2.0, w / 2.0)).to(
+            self.device
+        )
         grid = grid / torch.FloatTensor((h / 2.0, w / 2.0)).to(self.device)
         return grid.view(1, 1, 1, 2).flip(3)
 
@@ -156,8 +154,7 @@ class DifferentiableStarPlanner(nn.Module):
 
     def init_g_map(self):
         return torch.clamp(
-            self.inf
-            * (torch.ones_like(self.start_map) - self.start_map.clone()),
+            self.inf * (torch.ones_like(self.start_map) - self.start_map.clone()),
             min=0,
             max=self.inf,
         )
@@ -186,13 +183,9 @@ class DifferentiableStarPlanner(nn.Module):
         self.conv_time = 0
         self.close_time = 0
 
-        self.obstacles = self.preprocess_obstacle_map(
-            obstacles.to(self.device)
-        )
+        self.obstacles = self.preprocess_obstacle_map(obstacles.to(self.device))
         self.start_map = start_map.to(self.device)
-        self.been_there = torch.zeros_like(self.start_map).to(
-            torch.device("cpu")
-        )
+        self.been_there = torch.zeros_like(self.start_map).to(torch.device("cpu"))
         self.coords = coords.to(self.device)
         self.goal_map = goal_map.to(self.device)
         self.been_there = torch.zeros_like(self.goal_map).to(self.device)
@@ -254,9 +247,9 @@ class DifferentiableStarPlanner(nn.Module):
                 )
                 self.g_map[:, :, ymin:ymax, xmin:xmax] = torch.min(
                     self.g_map[:, :, ymin:ymax, xmin:xmax].clone(),
-                    (n2c + c_map[:, :, ymin:ymax, xmin:xmax]).min(
-                        dim=1, keepdim=True
-                    )[0],
+                    (n2c + c_map[:, :, ymin:ymax, xmin:xmax]).min(dim=1, keepdim=True)[
+                        0
+                    ],
                 )
                 self.close_list_map[:, :, ymin:ymax, xmin:xmax] = torch.max(
                     self.close_list_map[:, :, ymin:ymax, xmin:xmax],
@@ -284,9 +277,7 @@ class DifferentiableStarPlanner(nn.Module):
                         + c_map
                     ).min(dim=1, keepdim=True)[0],
                 )
-                self.close_list_map = torch.max(
-                    self.close_list_map, self.open_list_map
-                )
+                self.close_list_map = torch.max(self.close_list_map, self.open_list_map)
                 self.open_list_map = F.relu(
                     F.max_pool2d(self.open_list_map, 3, stride=1, padding=1)
                     - self.close_list_map
@@ -296,9 +287,9 @@ class DifferentiableStarPlanner(nn.Module):
             if step >= self.max_steps:
                 stopped_by_max_iter = True
                 break
-            not_done = (
-                self.close_list_map.view(-1)[goal_idx].item() < 1.0
-            ) or (self.g_map.view(-1)[goal_idx].item() >= 0.1 * self.inf)
+            not_done = (self.close_list_map.view(-1)[goal_idx].item() < 1.0) or (
+                self.g_map.view(-1)[goal_idx].item() >= 0.1 * self.inf
+            )
             rad += 1
         if not stopped_by_max_iter:
             for i in range(additional_steps):
@@ -312,9 +303,7 @@ class DifferentiableStarPlanner(nn.Module):
                         + c_map
                     ).min(dim=1, keepdim=True)[0],
                 )
-                self.close_list_map = torch.max(
-                    self.close_list_map, self.open_list_map
-                )
+                self.close_list_map = torch.max(self.close_list_map, self.open_list_map)
                 self.open_list_map = F.relu(
                     F.max_pool2d(self.open_list_map, 3, stride=1, padding=1)
                     - self.close_list_map
@@ -331,34 +320,21 @@ class DifferentiableStarPlanner(nn.Module):
         w = coords.size(3)
         obstacles_pd = F.pad(self.obstacles, (1, 1, 1, 1), "replicate")
         if non_obstacle_cost_map is None:
-            learned_bias = torch.ones_like(self.obstacles).to(
-                obstacles_pd.device
-            )
+            learned_bias = torch.ones_like(self.obstacles).to(obstacles_pd.device)
         else:
             learned_bias = non_obstacle_cost_map.to(obstacles_pd.device)
         left_diff_sq = (
-            self.gx_to_left(
-                F.pad(coords[:, 1:2, :, :], (1, 1, 0, 0), "replicate")
-            )
-            ** 2
+            self.gx_to_left(F.pad(coords[:, 1:2, :, :], (1, 1, 0, 0), "replicate")) ** 2
         )
         right_diff_sq = (
-            self.gx_to_right(
-                F.pad(coords[:, 1:2, :, :], (1, 1, 0, 0), "replicate")
-            )
+            self.gx_to_right(F.pad(coords[:, 1:2, :, :], (1, 1, 0, 0), "replicate"))
             ** 2
         )
         up_diff_sq = (
-            self.gy_to_up(
-                F.pad(coords[:, 0:1, :, :], (0, 0, 1, 1), "replicate")
-            )
-            ** 2
+            self.gy_to_up(F.pad(coords[:, 0:1, :, :], (0, 0, 1, 1), "replicate")) ** 2
         )
         down_diff_sq = (
-            self.gy_to_down(
-                F.pad(coords[:, 0:1, :, :], (0, 0, 1, 1), "replicate")
-            )
-            ** 2
+            self.gy_to_down(F.pad(coords[:, 0:1, :, :], (0, 0, 1, 1), "replicate")) ** 2
         )
         out = torch.cat(
             [
@@ -418,9 +394,7 @@ class DifferentiableStarPlanner(nn.Module):
             ],
             dim=1,
         )
-        return out + torch.clamp(
-            learned_bias.expand_as(out), min=0, max=self.ob_cost
-        )
+        return out + torch.clamp(learned_bias.expand_as(out), min=0, max=self.ob_cost)
 
     def propagate_traversal(self, node_coords, close, g, coords):
         ymin, ymax, xmin, xmax = self.safe_roi_2d(
@@ -430,37 +404,24 @@ class DifferentiableStarPlanner(nn.Module):
             node_coords[1] + 2,
         )
         mask = close[:, :, ymin:ymax, xmin:xmax] > 0
-        mask[
-            :, :, f2ind(node_coords, 0) - ymin, f2ind(node_coords, 1) - xmin
-        ] = 0
+        mask[:, :, f2ind(node_coords, 0) - ymin, f2ind(node_coords, 1) - xmin] = 0
         mask = mask > 0
         current_g_cost = g[:, :, ymin:ymax, xmin:xmax][mask].clone()
         if len(current_g_cost.view(-1)) == 0:
             # we are kind surrounded by obstacles,
             # but still need to output something
-            mask = torch.relu(
-                1.0 - self.been_there[:, :, ymin:ymax, xmin:xmax]
-            )
-            mask[
-                :,
-                :,
-                f2ind(node_coords, 0) - ymin,
-                f2ind(node_coords, 1) - xmin,
-            ] = 0
+            mask = torch.relu(1.0 - self.been_there[:, :, ymin:ymax, xmin:xmax])
+            mask[:, :, f2ind(node_coords, 0) - ymin, f2ind(node_coords, 1) - xmin,] = 0
             mask = mask > 0
             current_g_cost = g[:, :, ymin:ymax, xmin:xmax][mask].clone()
         if len(current_g_cost.view(-1)) > 1:
             current_g_cost = current_g_cost - torch.min(current_g_cost).item()
             current_g_cost = current_g_cost + 0.41 * torch.randperm(
-                len(current_g_cost),
-                dtype=torch.float32,
-                device=torch.device("cpu"),
+                len(current_g_cost), dtype=torch.float32, device=torch.device("cpu"),
             ) / (len(current_g_cost))
         #
         coords_roi = coords[:, :, ymin:ymax, xmin:xmax]
-        out = self.argmin(
-            current_g_cost, coords_roi[mask.expand_as(coords_roi)]
-        )
+        out = self.argmin(current_g_cost, coords_roi[mask.expand_as(coords_roi)])
         return out
 
     def get_clean_costmap_and_goodmask(self):
@@ -481,9 +442,7 @@ class DifferentiableStarPlanner(nn.Module):
         node_coords = goal_coords.cpu()
         out_path.append(node_coords)
         self.been_there = 0 * self.been_there.cpu()
-        self.been_there[
-            :, :, f2ind(node_coords, 0), f2ind(node_coords, 1)
-        ] = 1.0
+        self.been_there[:, :, f2ind(node_coords, 0), f2ind(node_coords, 1)] = 1.0
         self.close_list_map = self.close_list_map.cpu()
         self.g_map = self.g_map.cpu()
         self.coords = self.coords.cpu()
@@ -492,9 +451,7 @@ class DifferentiableStarPlanner(nn.Module):
             node_coords = self.propagate_traversal(
                 node_coords, self.close_list_map, self.g_map, self.coords
             )
-            self.been_there[
-                :, :, f2ind(node_coords, 0), f2ind(node_coords, 1)
-            ] = 1.0
+            self.been_there[:, :, f2ind(node_coords, 0), f2ind(node_coords, 1)] = 1.0
             if torch.norm(node_coords - out_path[-1], 2).item() < 0.3:
                 y = node_coords.flatten()[0].long()
                 x = node_coords.flatten()[1].long()
diff --git habitat_baselines/slambased/reprojection.py habitat_baselines/slambased/reprojection.py
index d9d9e33..fec876b 100644
--- habitat_baselines/slambased/reprojection.py
+++ habitat_baselines/slambased/reprojection.py
@@ -45,9 +45,7 @@ def get_direction(p_init, p_fin, ang_th=0.2, pos_th=0.1):
     else:
         needed_angle = torch.atan2(pos_diff[1], pos_diff[0])
         current_angle = torch.atan2(p_init[2, 0], p_init[0, 0])
-    to_rotate = angle_to_pi_2_minus_pi_2(
-        -np.pi / 2.0 + needed_angle - current_angle
-    )
+    to_rotate = angle_to_pi_2_minus_pi_2(-np.pi / 2.0 + needed_angle - current_angle)
     if torch.abs(to_rotate).item() < ang_th:
         return 0
     return to_rotate
@@ -58,18 +56,13 @@ def reproject_local_to_global(xyz_local, p):
     num, dim = xyz_local.size()
     if dim == 3:
         xyz = torch.cat(
-            [
-                xyz_local,
-                torch.ones((num, 1), dtype=torch.float32, device=device),
-            ],
+            [xyz_local, torch.ones((num, 1), dtype=torch.float32, device=device),],
             dim=1,
         )
     elif dim == 4:
         xyz = xyz_local
     else:
-        raise ValueError(
-            "3d point cloud dim is neighter 3, or 4 (homogenious)"
-        )
+        raise ValueError("3d point cloud dim is neighter 3, or 4 (homogenious)")
     # print(xyz.shape, P.shape)
     xyz_global = torch.mm(p.squeeze(), xyz.t())
     return xyz_global.t()
@@ -112,8 +105,7 @@ def normalize_zx_ori(p):
     out = torch.cat(
         [
             torch.cat(
-                [p[:, :3, :3] / norms.expand(p.size(0), 3, 3), p[:, 3:, :3]],
-                dim=1,
+                [p[:, :3, :3] / norms.expand(p.size(0), 3, 3), p[:, 3:, :3]], dim=1,
             ),
             p[:, :, 3:],
         ],
@@ -169,9 +161,7 @@ def planned_path2tps(path, cell_size, map_size, agent_h, add_rot=False):
             [0, 0, 0, 1],
         ]
     )
-    planned_tps = torch.bmm(
-        p.inverse().unsqueeze(0).expand(num_pts, 4, 4), planned_tps
-    )
+    planned_tps = torch.bmm(p.inverse().unsqueeze(0).expand(num_pts, 4, 4), planned_tps)
     if add_rot:
         return add_rot_wps(planned_tps)
     return planned_tps
@@ -184,11 +174,7 @@ def habitat_goalpos_to_tp(ro_phi, p_curr):
     """
     device = ro_phi.device
     offset = torch.tensor(
-        [
-            -ro_phi[0] * torch.sin(ro_phi[1]),
-            0,
-            ro_phi[0] * torch.cos(ro_phi[1]),
-        ]
+        [-ro_phi[0] * torch.sin(ro_phi[1]), 0, ro_phi[0] * torch.cos(ro_phi[1]),]
     ).to(device)
     if p_curr.size(1) == 3:
         p_curr = homogenize_p(p_curr)
@@ -197,9 +183,7 @@ def habitat_goalpos_to_tp(ro_phi, p_curr):
         torch.cat(
             [
                 offset
-                * torch.tensor(
-                    [1.0, 1.0, 1.0], dtype=torch.float32, device=device
-                ),
+                * torch.tensor([1.0, 1.0, 1.0], dtype=torch.float32, device=device),
                 torch.tensor([1.0], device=device),
             ]
         ).reshape(4, 1),
@@ -215,9 +199,7 @@ def habitat_goalpos_to_mapgoal_pos(offset, p_curr, cell_size, map_size):
     goal_tp = habitat_goalpos_to_tp(offset, p_curr)
     goal_tp1 = torch.eye(4).to(device)
     goal_tp1[:, 3:] = goal_tp
-    projected_p = project_tps_into_worldmap(
-        goal_tp1.view(1, 4, 4), cell_size, map_size
-    )
+    projected_p = project_tps_into_worldmap(goal_tp1.view(1, 4, 4), cell_size, map_size)
     return projected_p
 
 
@@ -247,8 +229,7 @@ def project_tps_into_worldmap(tps, cell_size, map_size, do_floor=True):
     device = tps.device
     topdown_p = torch.tensor([[1.0, 0, 0, 0], [0, 0, 1.0, 0]]).to(device)
     world_coords = torch.bmm(
-        topdown_p.view(1, 2, 4).expand(tps.size(0), 2, 4),
-        tps[:, :, 3:].view(-1, 4, 1),
+        topdown_p.view(1, 2, 4).expand(tps.size(0), 2, 4), tps[:, :, 3:].view(-1, 4, 1),
     )
     shift = int(floor(get_map_size_in_cells(map_size, cell_size) / 2.0))
     topdown2index = torch.tensor(
@@ -258,8 +239,7 @@ def project_tps_into_worldmap(tps, cell_size, map_size, do_floor=True):
         [world_coords, torch.ones((len(world_coords), 1, 1)).to(device)], dim=1
     )
     world_coords = torch.bmm(
-        topdown2index.unsqueeze(0).expand(world_coords_h.size(0), 3, 3),
-        world_coords_h,
+        topdown2index.unsqueeze(0).expand(world_coords_h.size(0), 3, 3), world_coords_h,
     )[:, :2, 0]
     if do_floor:
         return (
diff --git habitat_baselines/slambased/utils.py habitat_baselines/slambased/utils.py
index 8f181d1..2cc0b75 100644
--- habitat_baselines/slambased/utils.py
+++ habitat_baselines/slambased/utils.py
@@ -18,9 +18,7 @@ def generate_2dgrid(h, w, centered=False):
     else:
         x = torch.linspace(0, w - 1, w)
         y = torch.linspace(0, h - 1, h)
-    grid2d = torch.stack(
-        [y.repeat(w, 1).t().contiguous().view(-1), x.repeat(h)], 1
-    )
+    grid2d = torch.stack([y.repeat(w, 1).t().contiguous().view(-1), x.repeat(h)], 1)
     return grid2d.view(1, h, w, 2).permute(0, 3, 1, 2)
 
 
@@ -40,9 +38,7 @@ def resize_pil(np_img, size=128):
 def find_map_size(h, w):
     map_size_in_meters = int(0.1 * 3 * max(h, w))
     if map_size_in_meters % 10 != 0:
-        map_size_in_meters = map_size_in_meters + (
-            10 - (map_size_in_meters % 10)
-        )
+        map_size_in_meters = map_size_in_meters + (10 - (map_size_in_meters % 10))
     return map_size_in_meters
 
 
diff --git test/data/habitat-sim_trajectory_data.json test/data/habitat-sim_trajectory_data.json
deleted file mode 100644
index 10d8d65..0000000
--- test/data/habitat-sim_trajectory_data.json
+++ /dev/null
@@ -1 +0,0 @@
-{"positions": [[2.9085702896118164, 0.07244700193405151, 0.13949552178382874], [2.692063808441162, 0.17669875919818878, 0.014495700597763062], [2.475557327270508, 0.17669875919818878, -0.11050412058830261], [2.2590508460998535, 0.17669875919818878, -0.12059974670410156], [2.042544364929199, 0.17669875919818878, -0.12059974670410156], [1.826037883758545, 0.17669875919818878, -0.12059974670410156], [1.6095314025878906, 0.17669875919818878, -0.12059974670410156], [1.3930249214172363, 0.17669875919818878, -0.12059974670410156], [1.176518440246582, 0.17669875919818878, -0.12059974670410156], [1.176518440246582, 0.17669875919818878, -0.12059974670410156], [0.941595196723938, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156], [0.8741309642791748, 0.17669875919818878, -0.12059974670410156]], "rotations": [[0.0, 0.5000002980232239, 0.0, 0.8660252094268799], [0.0, 0.5000003576278687, 0.0, 0.8660252094268799], [0.0, 0.5000002980232239, 0.0, 0.8660252094268799], [0.0, 0.5000002980232239, 0.0, 0.8660252094268799], [0.0, 0.5000002980232239, 0.0, 0.8660252094268799], [0.0, 0.5000002980232239, 0.0, 0.8660252094268799], [0.0, 0.5000002980232239, 0.0, 0.8660252094268799], [0.0, 0.5000002980232239, 0.0, 0.8660252094268799], [0.0, 0.5000002980232239, 0.0, 0.8660252094268799], [0.0, 0.573576807975769, 0.0, 0.8191518187522888], [0.0, 0.5735767483711243, 0.0, 0.8191518187522888], [0.0, 0.573576807975769, 0.0, 0.8191518187522888], [0.0, 0.5735767483711243, 0.0, 0.8191518187522888], [0.0, 0.573576807975769, 0.0, 0.8191518187522888], [0.0, 0.5735767483711243, 0.0, 0.8191518187522888], [0.0, 0.573576807975769, 0.0, 0.8191518187522888], [0.0, 0.5735767483711243, 0.0, 0.8191518187522888], [0.0, 0.573576807975769, 0.0, 0.8191518187522888], [0.0, 0.5735767483711243, 0.0, 0.8191518187522888], [0.0, 0.573576807975769, 0.0, 0.8191518187522888], [0.0, 0.5735767483711243, 0.0, 0.8191518187522888], [0.0, 0.573576807975769, 0.0, 0.8191518187522888], [0.0, 0.5735767483711243, 0.0, 0.8191518187522888], [0.0, 0.573576807975769, 0.0, 0.8191518187522888], [0.0, 0.5735767483711243, 0.0, 0.8191518187522888], [0.0, 0.573576807975769, 0.0, 0.8191518187522888], [0.0, 0.5735767483711243, 0.0, 0.8191518187522888], [0.0, 0.573576807975769, 0.0, 0.8191518187522888], [0.0, 0.6427879929542542, 0.0, 0.7660441398620605], [0.0, 0.6427879929542542, 0.0, 0.7660441398620605], [0.0, 0.6427879929542542, 0.0, 0.7660441398620605], [0.0, 0.7071071863174438, 0.0, 0.707106351852417], [0.0, 0.7071071863174438, 0.0, 0.707106351852417], [0.0, 0.7071071863174438, 0.0, 0.707106351852417], [0.0, 0.7071071863174438, 0.0, 0.707106351852417], [0.0, 0.7071071863174438, 0.0, 0.707106351852417], [0.0, 0.7071071863174438, 0.0, 0.707106351852417], [0.0, 0.7071071863174438, 0.0, 0.707106351852417], [0.0, 0.7071071863174438, 0.0, 0.707106351852417], [0.0, 0.7071071863174438, 0.0, 0.707106351852417], [0.0, 0.7071071863174438, 0.0, 0.707106351852417], [0.0, 0.7071071863174438, 0.0, 0.707106351852417], [0.0, 0.7071071863174438, 0.0, 0.707106351852417], [0.0, 0.7071071863174438, 0.0, 0.707106351852417], [0.0, 0.7071071863174438, 0.0, 0.707106351852417], [0.0, 0.7660449147224426, 0.0, 0.6427870988845825], [0.0, 0.7660449743270874, 0.0, 0.6427870392799377], [0.0, 0.7660449743270874, 0.0, 0.642786979675293], [0.0, 0.8191525340080261, 0.0, 0.5735757350921631], [0.0, 0.8191525340080261, 0.0, 0.5735757350921631], [0.0, 0.8191525340080261, 0.0, 0.5735757350921631], [0.0, 0.8191525340080261, 0.0, 0.5735757350921631], [0.0, 0.8660258650779724, 0.0, 0.4999992549419403], [0.0, 0.8660258650779724, 0.0, 0.4999992549419403], [0.0, 0.8660258650779724, 0.0, 0.4999992549419403], [0.0, 0.8660258650779724, 0.0, 0.4999992549419403], [0.0, 0.8660258650779724, 0.0, 0.4999992549419403], [0.0, 0.8660258650779724, 0.0, 0.4999992549419403]], "actions": ["MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "TURN_LEFT", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "TURN_LEFT", "MOVE_FORWARD", "MOVE_FORWARD", "TURN_LEFT", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "TURN_LEFT", "MOVE_FORWARD", "MOVE_FORWARD", "TURN_LEFT", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "TURN_LEFT", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "MOVE_FORWARD", "STOP"], "distances_to_obstacles": [null, 0.2564087510108948, 0.010195869952440262, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
diff --git test/test_config.py test/test_config.py
index a113e82..0c072f6 100644
--- test/test_config.py
+++ test/test_config.py
@@ -25,8 +25,7 @@ def test_merged_configs():
 def test_overwrite_options():
     for steps_limit in range(MAX_TEST_STEPS_LIMIT):
         config = get_config(
-            config_paths=CFG_TEST,
-            opts=["ENVIRONMENT.MAX_EPISODE_STEPS", steps_limit],
+            config_paths=CFG_TEST, opts=["ENVIRONMENT.MAX_EPISODE_STEPS", steps_limit],
         )
         assert (
             config.ENVIRONMENT.MAX_EPISODE_STEPS == steps_limit
diff --git test/test_habitat_env.py test/test_habitat_env.py
index 61851e5..b575803 100644
--- test/test_habitat_env.py
+++ test/test_habitat_env.py
@@ -56,9 +56,7 @@ def _load_test_data():
             pytest.skip("Please download Habitat test data to data folder.")
 
         datasets.append(
-            habitat.make_dataset(
-                id_dataset=config.DATASET.TYPE, config=config.DATASET
-            )
+            habitat.make_dataset(id_dataset=config.DATASET.TYPE, config=config.DATASET)
         )
 
         config.defrost()
@@ -80,9 +78,7 @@ def _vec_env_test_fn(configs, datasets, multiprocessing_start_method):
     )
     envs.reset()
     non_stop_actions = [
-        act
-        for act in range(envs.action_spaces[0].n)
-        if act != SimulatorActions.STOP
+        act for act in range(envs.action_spaces[0].n) if act != SimulatorActions.STOP
     ]
 
     for _ in range(2 * configs[0].ENVIRONMENT.MAX_EPISODE_STEPS):
@@ -134,9 +130,7 @@ def test_threaded_vectorized_env():
     envs = habitat.ThreadedVectorEnv(env_fn_args=env_fn_args)
     envs.reset()
     non_stop_actions = [
-        act
-        for act in range(envs.action_spaces[0].n)
-        if act != SimulatorActions.STOP
+        act for act in range(envs.action_spaces[0].n) if act != SimulatorActions.STOP
     ]
 
     for i in range(2 * configs[0].ENVIRONMENT.MAX_EPISODE_STEPS):
@@ -157,35 +151,27 @@ def test_env():
             scene_id=config.SIMULATOR.SCENE,
             start_position=[-3.0133917, 0.04623024, 7.3064547],
             start_rotation=[0, 0.163276, 0, 0.98658],
-            goals=[
-                NavigationGoal(position=[-3.0133917, 0.04623024, 7.3064547])
-            ],
+            goals=[NavigationGoal(position=[-3.0133917, 0.04623024, 7.3064547])],
             info={"geodesic_distance": 0.001},
         )
     ]
     env.reset()
 
     non_stop_actions = [
-        act
-        for act in range(env.action_space.n)
-        if act != SimulatorActions.STOP
+        act for act in range(env.action_space.n) if act != SimulatorActions.STOP
     ]
     for _ in range(config.ENVIRONMENT.MAX_EPISODE_STEPS):
         act = np.random.choice(non_stop_actions)
         env.step(act)
 
     # check for steps limit on environment
-    assert env.episode_over is True, (
-        "episode should be over after " "max_episode_steps"
-    )
+    assert env.episode_over is True, "episode should be over after " "max_episode_steps"
 
     env.reset()
 
     env.step(SimulatorActions.STOP)
     # check for STOP action
-    assert env.episode_over is True, (
-        "episode should be over after STOP " "action"
-    )
+    assert env.episode_over is True, "episode should be over after STOP " "action"
 
     env.close()
 
@@ -210,9 +196,7 @@ def test_rl_vectorized_envs():
     envs = habitat.VectorEnv(make_env_fn=make_rl_env, env_fn_args=env_fn_args)
     envs.reset()
     non_stop_actions = [
-        act
-        for act in range(envs.action_spaces[0].n)
-        if act != SimulatorActions.STOP
+        act for act in range(envs.action_spaces[0].n) if act != SimulatorActions.STOP
     ]
 
     for i in range(2 * configs[0].ENVIRONMENT.MAX_EPISODE_STEPS):
@@ -251,9 +235,7 @@ def test_rl_env():
             scene_id=config.SIMULATOR.SCENE,
             start_position=[-3.0133917, 0.04623024, 7.3064547],
             start_rotation=[0, 0.163276, 0, 0.98658],
-            goals=[
-                NavigationGoal(position=[-3.0133917, 0.04623024, 7.3064547])
-            ],
+            goals=[NavigationGoal(position=[-3.0133917, 0.04623024, 7.3064547])],
             info={"geodesic_distance": 0.001},
         )
     ]
@@ -262,14 +244,10 @@ def test_rl_env():
     observation = env.reset()
 
     non_stop_actions = [
-        act
-        for act in range(env.action_space.n)
-        if act != SimulatorActions.STOP
+        act for act in range(env.action_space.n) if act != SimulatorActions.STOP
     ]
     for _ in range(config.ENVIRONMENT.MAX_EPISODE_STEPS):
-        observation, reward, done, info = env.step(
-            np.random.choice(non_stop_actions)
-        )
+        observation, reward, done, info = env.step(np.random.choice(non_stop_actions))
 
     # check for steps limit on environment
     assert done is True, "episodes should be over after max_episode_steps"
diff --git test/test_mp3d_eqa.py test/test_mp3d_eqa.py
index eef9cda..7d16e1c 100644
--- test/test_mp3d_eqa.py
+++ test/test_mp3d_eqa.py
@@ -70,9 +70,7 @@ def get_minos_for_sim_eqa_config():
 def check_json_serializaiton(dataset: habitat.Dataset):
     start_time = time.time()
     json_str = str(dataset.to_json())
-    logger.info(
-        "JSON conversion finished. {} sec".format((time.time() - start_time))
-    )
+    logger.info("JSON conversion finished. {} sec".format((time.time() - start_time)))
     decoded_dataset = dataset.__class__()
     decoded_dataset.from_json(json_str)
     assert len(decoded_dataset.episodes) > 0
@@ -85,12 +83,8 @@ def check_json_serializaiton(dataset: habitat.Dataset):
 
 def test_mp3d_eqa_dataset():
     dataset_config = get_config(CFG_TEST).DATASET
-    if not mp3d_dataset.Matterport3dDatasetV1.check_config_paths_exist(
-        dataset_config
-    ):
-        pytest.skip(
-            "Please download Matterport3D EQA dataset to " "data folder."
-        )
+    if not mp3d_dataset.Matterport3dDatasetV1.check_config_paths_exist(dataset_config):
+        pytest.skip("Please download Matterport3D EQA dataset to " "data folder.")
 
     dataset = mp3d_dataset.Matterport3dDatasetV1(config=dataset_config)
     assert dataset
@@ -106,9 +100,7 @@ def test_mp3d_eqa_sim():
     if not mp3d_dataset.Matterport3dDatasetV1.check_config_paths_exist(
         eqa_config.DATASET
     ):
-        pytest.skip(
-            "Please download Matterport3D EQA dataset to " "data folder."
-        )
+        pytest.skip("Please download Matterport3D EQA dataset to " "data folder.")
 
     dataset = make_dataset(
         id_dataset=eqa_config.DATASET.TYPE, config=eqa_config.DATASET
@@ -145,9 +137,7 @@ def test_mp3d_eqa_sim_correspondence():
     if not mp3d_dataset.Matterport3dDatasetV1.check_config_paths_exist(
         eqa_config.DATASET
     ):
-        pytest.skip(
-            "Please download Matterport3D EQA dataset to " "data folder."
-        )
+        pytest.skip("Please download Matterport3D EQA dataset to " "data folder.")
 
     dataset = make_dataset(
         id_dataset=eqa_config.DATASET.TYPE, config=eqa_config.DATASET
@@ -164,9 +154,7 @@ def test_mp3d_eqa_sim_correspondence():
     while cycles_n > 0:
         env.reset()
         episode = env.current_episode
-        assert (
-            len(episode.goals) == 1
-        ), "Episode has no goals or more than one."
+        assert len(episode.goals) == 1, "Episode has no goals or more than one."
         assert (
             len(episode.shortest_paths) == 1
         ), "Episode has no shortest paths or more than one."
diff --git test/test_pointnav_dataset.py test/test_pointnav_dataset.py
index c655fc0..99339f1 100644
--- test/test_pointnav_dataset.py
+++ test/test_pointnav_dataset.py
@@ -32,9 +32,7 @@ NUM_EPISODES = 10
 def check_json_serializaiton(dataset: habitat.Dataset):
     start_time = time.time()
     json_str = str(dataset.to_json())
-    logger.info(
-        "JSON conversion finished. {} sec".format((time.time() - start_time))
-    )
+    logger.info("JSON conversion finished. {} sec".format((time.time() - start_time)))
     decoded_dataset = dataset.__class__()
     decoded_dataset.from_json(json_str)
     assert len(decoded_dataset.episodes) > 0
@@ -55,9 +53,7 @@ def test_single_pointnav_dataset():
     ), "Expected dataset doesn't expect separate episode file per scene."
     dataset = PointNavDatasetV1(config=dataset_config)
     assert len(dataset.episodes) > 0, "The dataset shouldn't be empty."
-    assert (
-        len(dataset.scene_ids) == 2
-    ), "The test dataset scenes number is wrong."
+    assert len(dataset.scene_ids) == 2, "The test dataset scenes number is wrong."
     check_json_serializaiton(dataset)
 
 
@@ -66,14 +62,10 @@ def test_multiple_files_scene_path():
     if not PointNavDatasetV1.check_config_paths_exist(dataset_config):
         pytest.skip("Test skipped as dataset files are missing.")
     scenes = PointNavDatasetV1.get_scenes_to_load(config=dataset_config)
-    assert (
-        len(scenes) > 0
-    ), "Expected dataset contains separate episode file per scene."
+    assert len(scenes) > 0, "Expected dataset contains separate episode file per scene."
     dataset_config.defrost()
     dataset_config.CONTENT_SCENES = scenes[:PARTIAL_LOAD_SCENES]
-    dataset_config.SCENES_DIR = os.path.join(
-        os.getcwd(), DEFAULT_SCENE_PATH_PREFIX
-    )
+    dataset_config.SCENES_DIR = os.path.join(os.getcwd(), DEFAULT_SCENE_PATH_PREFIX)
     dataset_config.freeze()
     partial_dataset = make_dataset(
         id_dataset=dataset_config.TYPE, config=dataset_config
@@ -94,9 +86,7 @@ def test_multiple_files_pointnav_dataset():
     if not PointNavDatasetV1.check_config_paths_exist(dataset_config):
         pytest.skip("Test skipped as dataset files are missing.")
     scenes = PointNavDatasetV1.get_scenes_to_load(config=dataset_config)
-    assert (
-        len(scenes) > 0
-    ), "Expected dataset contains separate episode file per scene."
+    assert len(scenes) > 0, "Expected dataset contains separate episode file per scene."
     dataset_config.defrost()
     dataset_config.CONTENT_SCENES = scenes[:PARTIAL_LOAD_SCENES]
     dataset_config.freeze()
diff --git test/test_sensors.py test/test_sensors.py
index 9372dac..2acf3ae 100644
--- test/test_sensors.py
+++ test/test_sensors.py
@@ -184,9 +184,7 @@ def test_static_pointgoal_sensor():
     )
 
     non_stop_actions = [
-        act
-        for act in range(env.action_space.n)
-        if act != SimulatorActions.STOP
+        act for act in range(env.action_space.n) if act != SimulatorActions.STOP
     ]
     env.reset()
     for _ in range(100):
@@ -229,9 +227,7 @@ def test_get_observations_at():
         ]
     )
     non_stop_actions = [
-        act
-        for act in range(env.action_space.n)
-        if act != SimulatorActions.STOP
+        act for act in range(env.action_space.n) if act != SimulatorActions.STOP
     ]
 
     obs = env.reset()
@@ -248,9 +244,7 @@ def test_get_observations_at():
             ):
                 assert not np.allclose(val, obs[key])
         obs_at_point = env.sim.get_observations_at(
-            start_state.position,
-            start_state.rotation,
-            keep_agent_at_new_pose=False,
+            start_state.position, start_state.rotation, keep_agent_at_new_pose=False,
         )
         for key, val in obs_at_point.items():
             assert np.allclose(val, obs[key])
diff --git test/test_trajectory_sim.py test/test_trajectory_sim.py
index 285b8e1..349745e 100644
--- test/test_trajectory_sim.py
+++ test/test_trajectory_sim.py
@@ -40,18 +40,14 @@ def test_sim_trajectory():
             state = sim.get_agent_state()
             assert (
                 np.allclose(
-                    np.array(
-                        test_trajectory["positions"][i], dtype=np.float32
-                    ),
+                    np.array(test_trajectory["positions"][i], dtype=np.float32),
                     state.position,
                 )
                 is True
             ), "mismatch in position " "at step {}".format(i)
             assert (
                 np.allclose(
-                    np.array(
-                        test_trajectory["rotations"][i], dtype=np.float32
-                    ),
+                    np.array(test_trajectory["rotations"][i], dtype=np.float32),
                     np.array([*state.rotation.imag, state.rotation.real]),
                 )
                 is True
@@ -61,9 +57,7 @@ def test_sim_trajectory():
             dist_to_obs = sim.distance_to_closest_obstacle(
                 state.position, max_search_radius
             )
-            assert np.isclose(
-                dist_to_obs, test_trajectory["distances_to_obstacles"][i]
-            )
+            assert np.isclose(dist_to_obs, test_trajectory["distances_to_obstacles"][i])
 
         assert sim.action_space.contains(action)
 
